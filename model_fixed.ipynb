{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from distutils.command.config import config\n",
    "import pickle5 as pickle\n",
    "\n",
    "# Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import os\n",
    "from os.path import exists\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import f1_score, multilabel_confusion_matrix, accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Tokenization\n",
    "from tokenizers import  Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import glob\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/svetlana.maslenkova/LSTM/pickles/pid_hid_dignoses.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2687410/2435019705.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mPKL_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/svetlana.maslenkova/LSTM/pickles/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPKL_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'pid_hid_dignoses.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdata_diagnoses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/svetlana.maslenkova/LSTM/pickles/pid_hid_dignoses.pkl'"
     ]
    }
   ],
   "source": [
    "PKL_PATH = '/home/svetlana.maslenkova/LSTM/pickles/'\n",
    "\n",
    "with open(PKL_PATH + 'pid_hid_dignoses.pkl', 'rb') as handle:\n",
    "    data_diagnoses = pickle.load(handle)\n",
    "\n",
    "data_diagnoses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>admittime</th>\n",
       "      <th>dischtime</th>\n",
       "      <th>deathtime</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>admission_location</th>\n",
       "      <th>discharge_location</th>\n",
       "      <th>insurance</th>\n",
       "      <th>language</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>edregtime</th>\n",
       "      <th>edouttime</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14679932</td>\n",
       "      <td>21038362</td>\n",
       "      <td>2139-09-26 14:16:00</td>\n",
       "      <td>2139-09-28 11:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Other</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15585972</td>\n",
       "      <td>24941086</td>\n",
       "      <td>2123-10-07 23:56:00</td>\n",
       "      <td>2123-10-12 11:22:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Other</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id           admittime            dischtime deathtime  \\\n",
       "0    14679932  21038362 2139-09-26 14:16:00  2139-09-28 11:30:00       NaN   \n",
       "1    15585972  24941086 2123-10-07 23:56:00  2123-10-12 11:22:00       NaN   \n",
       "\n",
       "  admission_type admission_location discharge_location insurance language  \\\n",
       "0       ELECTIVE                NaN               HOME     Other  ENGLISH   \n",
       "1       ELECTIVE                NaN               HOME     Other  ENGLISH   \n",
       "\n",
       "  marital_status ethnicity edregtime edouttime  hospital_expire_flag  \n",
       "0         SINGLE   UNKNOWN       NaN       NaN                     0  \n",
       "1            NaN     WHITE       NaN       NaN                     0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = '/home/svetlana.maslenkova/data/'\n",
    "\n",
    "data_admissions = pd.read_csv(DATA_PATH+'mimic-iv-1.0/core/admissions.csv')\n",
    "\n",
    "data_admissions.columns = data_admissions.columns.str.lower()\n",
    "data_admissions['hadm_id'] = data_admissions['hadm_id'].astype(int)\n",
    "format_ = '%Y-%m-%d %H:%M:%S'\n",
    "data_admissions['admittime'] = pd.to_datetime(data_admissions['admittime'], format=format_)\n",
    "\n",
    "data_admissions.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>seq_num</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>icd_version</th>\n",
       "      <th>admittime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15734973</td>\n",
       "      <td>20475282</td>\n",
       "      <td>3</td>\n",
       "      <td>2825</td>\n",
       "      <td>9</td>\n",
       "      <td>2112-02-02 07:53:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15734973</td>\n",
       "      <td>20475282</td>\n",
       "      <td>2</td>\n",
       "      <td>V0251</td>\n",
       "      <td>9</td>\n",
       "      <td>2112-02-02 07:53:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15734973</td>\n",
       "      <td>20475282</td>\n",
       "      <td>5</td>\n",
       "      <td>V270</td>\n",
       "      <td>9</td>\n",
       "      <td>2112-02-02 07:53:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15734973</td>\n",
       "      <td>20475282</td>\n",
       "      <td>1</td>\n",
       "      <td>64891</td>\n",
       "      <td>9</td>\n",
       "      <td>2112-02-02 07:53:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15734973</td>\n",
       "      <td>20475282</td>\n",
       "      <td>4</td>\n",
       "      <td>66481</td>\n",
       "      <td>9</td>\n",
       "      <td>2112-02-02 07:53:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id  seq_num icd_code  icd_version           admittime\n",
       "0    15734973  20475282        3     2825            9 2112-02-02 07:53:00\n",
       "1    15734973  20475282        2    V0251            9 2112-02-02 07:53:00\n",
       "2    15734973  20475282        5     V270            9 2112-02-02 07:53:00\n",
       "3    15734973  20475282        1    64891            9 2112-02-02 07:53:00\n",
       "4    15734973  20475282        4    66481            9 2112-02-02 07:53:00"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_diagnoses = pd.read_csv(DATA_PATH+'mimic-iv-1.0/hosp/diagnoses_icd.csv')\n",
    "\n",
    "data_diagnoses.columns = data_diagnoses.columns.str.lower()\n",
    "\n",
    "data_diagnoses = data_diagnoses.merge(data_admissions[['hadm_id', 'admittime']])\n",
    "\n",
    "data_diagnoses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>demographics_in_visit</th>\n",
       "      <th>lab_tests_in_visit</th>\n",
       "      <th>medications_in_visit</th>\n",
       "      <th>vitals_in_visit</th>\n",
       "      <th>days_in_visit</th>\n",
       "      <th>aki_status_in_visit</th>\n",
       "      <th>days</th>\n",
       "      <th>admittime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17223646</td>\n",
       "      <td>20023461</td>\n",
       "      <td>[WHITE F 69.0, WHITE F 69.0, WHITE F 69.0, WHI...</td>\n",
       "      <td>[Hematology Blood hematocrit {37.7} %; Hematol...</td>\n",
       "      <td>[Sodium Chloride 0.9%  Flush {3} mL ; Heparin ...</td>\n",
       "      <td>[temp {97.7} heartrate {68.0} resprate {18.0} ...</td>\n",
       "      <td>[WHITE F 69.0$temp {97.7} heartrate {68.0} res...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, nan...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>2152-10-31 20:11:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18371764</td>\n",
       "      <td>20046098</td>\n",
       "      <td>[WHITE M 56.0, WHITE M 56.0, WHITE M 56.0, WHI...</td>\n",
       "      <td>[Hematology Blood hematocrit {40.1} %; Hematol...</td>\n",
       "      <td>[Sodium Chloride 0.9%  Flush {3} mL ; D5 1/2NS...</td>\n",
       "      <td>[temp {100.3} heartrate {103.0} resprate {18.0...</td>\n",
       "      <td>[WHITE M 56.0$temp {100.3} heartrate {103.0} r...</td>\n",
       "      <td>[0, 0, nan, nan]</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>2162-09-15 13:43:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17416292</td>\n",
       "      <td>20051972</td>\n",
       "      <td>[UNKNOWN F 56.0, UNKNOWN F 56.0, UNKNOWN F 56....</td>\n",
       "      <td>[Hematology Blood hematocrit {27.5} %; Hematol...</td>\n",
       "      <td>[ARIPiprazole {10} mg ; Docusate Sodium {100} ...</td>\n",
       "      <td>[temp {} heartrate {68.0} resprate {22.0} o2sa...</td>\n",
       "      <td>[UNKNOWN F 56.0$temp {} heartrate {68.0} respr...</td>\n",
       "      <td>[0, 0, 0, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8]</td>\n",
       "      <td>2168-04-06 03:17:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15109929</td>\n",
       "      <td>20058401</td>\n",
       "      <td>[WHITE M 77.0, WHITE M 77.0, WHITE M 77.0, WHI...</td>\n",
       "      <td>[Hematology Blood hematocrit {24.0} %; Hematol...</td>\n",
       "      <td>[nan, Sodium Chloride 0.9%  Flush {3} mL ; Ace...</td>\n",
       "      <td>[temp {97.9} heartrate {64.0} resprate {18.0} ...</td>\n",
       "      <td>[WHITE M 77.0$temp {97.9} heartrate {64.0} res...</td>\n",
       "      <td>[0, 0, 0, 0, 0, nan, nan]</td>\n",
       "      <td>[-1, 0, 1, 2, 3, 4, 5]</td>\n",
       "      <td>2157-11-11 01:29:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14687005</td>\n",
       "      <td>20064641</td>\n",
       "      <td>[HISPANIC/LATINO M 48.0, HISPANIC/LATINO M 48....</td>\n",
       "      <td>[Hematology Blood hematocrit {27.2} %; Hematol...</td>\n",
       "      <td>[nan, nan, nan, Influenza Vaccine Quadrivalent...</td>\n",
       "      <td>[temp {98.8} heartrate {91.0} resprate {18.0} ...</td>\n",
       "      <td>[HISPANIC/LATINO M 48.0$temp {98.8} heartrate ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, nan...</td>\n",
       "      <td>[-3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>2159-04-05 10:13:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id                              demographics_in_visit  \\\n",
       "0    17223646  20023461  [WHITE F 69.0, WHITE F 69.0, WHITE F 69.0, WHI...   \n",
       "1    18371764  20046098  [WHITE M 56.0, WHITE M 56.0, WHITE M 56.0, WHI...   \n",
       "2    17416292  20051972  [UNKNOWN F 56.0, UNKNOWN F 56.0, UNKNOWN F 56....   \n",
       "3    15109929  20058401  [WHITE M 77.0, WHITE M 77.0, WHITE M 77.0, WHI...   \n",
       "4    14687005  20064641  [HISPANIC/LATINO M 48.0, HISPANIC/LATINO M 48....   \n",
       "\n",
       "                                  lab_tests_in_visit  \\\n",
       "0  [Hematology Blood hematocrit {37.7} %; Hematol...   \n",
       "1  [Hematology Blood hematocrit {40.1} %; Hematol...   \n",
       "2  [Hematology Blood hematocrit {27.5} %; Hematol...   \n",
       "3  [Hematology Blood hematocrit {24.0} %; Hematol...   \n",
       "4  [Hematology Blood hematocrit {27.2} %; Hematol...   \n",
       "\n",
       "                                medications_in_visit  \\\n",
       "0  [Sodium Chloride 0.9%  Flush {3} mL ; Heparin ...   \n",
       "1  [Sodium Chloride 0.9%  Flush {3} mL ; D5 1/2NS...   \n",
       "2  [ARIPiprazole {10} mg ; Docusate Sodium {100} ...   \n",
       "3  [nan, Sodium Chloride 0.9%  Flush {3} mL ; Ace...   \n",
       "4  [nan, nan, nan, Influenza Vaccine Quadrivalent...   \n",
       "\n",
       "                                     vitals_in_visit  \\\n",
       "0  [temp {97.7} heartrate {68.0} resprate {18.0} ...   \n",
       "1  [temp {100.3} heartrate {103.0} resprate {18.0...   \n",
       "2  [temp {} heartrate {68.0} resprate {22.0} o2sa...   \n",
       "3  [temp {97.9} heartrate {64.0} resprate {18.0} ...   \n",
       "4  [temp {98.8} heartrate {91.0} resprate {18.0} ...   \n",
       "\n",
       "                                       days_in_visit  \\\n",
       "0  [WHITE F 69.0$temp {97.7} heartrate {68.0} res...   \n",
       "1  [WHITE M 56.0$temp {100.3} heartrate {103.0} r...   \n",
       "2  [UNKNOWN F 56.0$temp {} heartrate {68.0} respr...   \n",
       "3  [WHITE M 77.0$temp {97.9} heartrate {64.0} res...   \n",
       "4  [HISPANIC/LATINO M 48.0$temp {98.8} heartrate ...   \n",
       "\n",
       "                                 aki_status_in_visit  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, nan...   \n",
       "1                                   [0, 0, nan, nan]   \n",
       "2                        [0, 0, 0, 1, 1, 1, 1, 1, 1]   \n",
       "3                          [0, 0, 0, 0, 0, nan, nan]   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, nan...   \n",
       "\n",
       "                                                days           admittime  \n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,... 2152-10-31 20:11:00  \n",
       "1                                       [0, 1, 2, 3] 2162-09-15 13:43:00  \n",
       "2                        [0, 1, 2, 3, 4, 5, 6, 7, 8] 2168-04-06 03:17:00  \n",
       "3                             [-1, 0, 1, 2, 3, 4, 5] 2157-11-11 01:29:00  \n",
       "4  [-3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,... 2159-04-05 10:13:00  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res = pid_train_df.merge(data_admissions[['hadm_id', 'admittime']])\n",
    "# res = pid_val_df.merge(data_admissions[['hadm_id', 'admittime']])\n",
    "res = pid_test_df.merge(data_admissions[['hadm_id', 'admittime']])\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = 13131863\n",
    "admittime = res[res.subject_id==patient].admittime.values[0]\n",
    "data_diagnoses[(data_diagnoses.subject_id==patient) & (data_diagnoses.admittime < admittime)].sort_values('admittime', ascending=False).icd_code.tolist()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 827 patients\n",
      "0 patients\n",
      "100 patients\n",
      "200 patients\n",
      "300 patients\n",
      "400 patients\n",
      "500 patients\n",
      "600 patients\n",
      "700 patients\n",
      "800 patients\n"
     ]
    }
   ],
   "source": [
    "# patient_previous_diag_dict = {}\n",
    "i=0\n",
    "print(f'Total {len(res.subject_id.unique())} patients')\n",
    "for patient in res.subject_id.unique():\n",
    "    if i%100==0:\n",
    "        print(f'{i} patients')\n",
    "    admittime = res[res.subject_id==patient].admittime.values[0]\n",
    "    prev_diags = data_diagnoses[(data_diagnoses.subject_id==patient) & (data_diagnoses.admittime < admittime) ].sort_values('admittime', ascending=False).icd_code.tolist()[:30]\n",
    "    patient_previous_diag_dict[patient] = prev_diags\n",
    "    i += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10504"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(patient_previous_diag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prev_diagnoses = pd.DataFrame.from_dict(patient_previous_diag_dict, orient='index').reset_index().rename(columns={'index':'hadm_id'}).fillna('PAD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prev_diagnoses['prev_diags'] = df_prev_diagnoses.drop(labels=['hadm_id'], axis=1).apply(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>prev_diags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16679562</td>\n",
       "      <td>F17210 E785 J111 K219 F1020 C61 N189 J441 N400...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10189736</td>\n",
       "      <td>K921 I7103 J9691 J90 I959 D62 Z87891 K269 D696...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19669999</td>\n",
       "      <td>42833 V4581 4280 73313 41400 2724 5770 57450 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12377638</td>\n",
       "      <td>PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18423382</td>\n",
       "      <td>3051 V1581 42789 44422 99674 6826 4019 44022 V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10499</th>\n",
       "      <td>18203312</td>\n",
       "      <td>PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10500</th>\n",
       "      <td>16688191</td>\n",
       "      <td>PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10501</th>\n",
       "      <td>14644623</td>\n",
       "      <td>PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10502</th>\n",
       "      <td>18011979</td>\n",
       "      <td>PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10503</th>\n",
       "      <td>17020543</td>\n",
       "      <td>PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10504 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        hadm_id                                         prev_diags\n",
       "0      16679562  F17210 E785 J111 K219 F1020 C61 N189 J441 N400...\n",
       "1      10189736  K921 I7103 J9691 J90 I959 D62 Z87891 K269 D696...\n",
       "2      19669999  42833 V4581 4280 73313 41400 2724 5770 57450 4...\n",
       "3      12377638  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...\n",
       "4      18423382  3051 V1581 42789 44422 99674 6826 4019 44022 V...\n",
       "...         ...                                                ...\n",
       "10499  18203312  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...\n",
       "10500  16688191  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...\n",
       "10501  14644623  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...\n",
       "10502  18011979  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...\n",
       "10503  17020543  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...\n",
       "\n",
       "[10504 rows x 2 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prev_diagnoses = df_prev_diagnoses[['hadm_id', 'prev_diags']]\n",
    "df_prev_diagnoses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_length=400, pred_window=2, observing_window=3):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.max_length = max_length\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.make_matrices(idx)\n",
    "    \n",
    "    def tokenize(self, text, max_length):\n",
    "        \n",
    "        output = self.tokenizer.encode(text)\n",
    "        # padding and truncation\n",
    "        if len(output.ids) < max_length:\n",
    "            len_missing_token = max_length - len(output.ids)\n",
    "            padding_vec = [self.tokenizer.token_to_id('[PAD]') for _ in range(len_missing_token)]\n",
    "            token_output = [*output.ids, *padding_vec]\n",
    "        elif len(output.ids) > max_length:\n",
    "            token_output = output.ids[:max_length]\n",
    "        else:\n",
    "            token_output = output.ids\n",
    "        \n",
    "        return token_output\n",
    "\n",
    "    def make_matrices(self, idx):\n",
    "        day_info = self.df.days_in_visit.values[idx]\n",
    "        aki_status = self.df.aki_status_in_visit.values[idx]\n",
    "        days = self.df.days.values[idx]\n",
    "        # print(idx)\n",
    "\n",
    "        aki_happened = False\n",
    "        labels = []\n",
    "        day_info_list = []\n",
    "        label = None\n",
    "\n",
    "        for day in range(days[0], days[0]+ self.observing_window):\n",
    "            if day not in days:\n",
    "                labels.append(0)\n",
    "                day_info_list.append(self.tokenize('', self.max_length))\n",
    "            else:\n",
    "                i = days.index(day)\n",
    "                \n",
    "                if (day + self.pred_window) not in days:\n",
    "                    labels.append(0)\n",
    "                else:              \n",
    "                    if ((i + self.pred_window) >= len(aki_status)) or np.isnan(aki_status[i + self.pred_window]):\n",
    "                        labels.append(0)\n",
    "                    else:\n",
    "                        labels.append(aki_status[i + self.pred_window])\n",
    "\n",
    "                if str(day_info[i]) == 'nan':\n",
    "                    day_info_list.append(self.tokenize('[PAD]', self.max_length))\n",
    "                else:\n",
    "                    day_info_list.append(self.tokenize(day_info[i], self.max_length))\n",
    "\n",
    "        if sum(labels) > 0:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "\n",
    "        #make tensors\n",
    "        tensor_day = torch.tensor(day_info_list, dtype=torch.int64)\n",
    "        tensor_labels = torch.tensor(label, dtype=torch.float64)\n",
    "\n",
    "        return tensor_day, tensor_labels\n",
    "\n",
    "\n",
    "class EHR_MODEL(nn.Module):\n",
    "    def __init__(self, max_length, vocab_size, device, pred_window=2, observing_window=3,  H=128, embedding_size=200):\n",
    "        super(EHR_MODEL, self).__init__()\n",
    "\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.H = H\n",
    "        self.max_length = max_length\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device\n",
    "\n",
    "        # self.embedding = pretrained_model\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "\n",
    "        self.lstm_day = nn.LSTM(input_size=embedding_size,\n",
    "                            hidden_size=self.H,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "\n",
    "        self.fc_day = nn.Linear(self.max_length * 2 * self.H, 2048)\n",
    "\n",
    "        # self.fc_1 = nn.Linear(self.observing_window * 2 * self.H, 2048)\n",
    "\n",
    "        self.lstm_adm = nn.LSTM(input_size=2048*self.observing_window,\n",
    "                            hidden_size=self.H,\n",
    "                            num_layers=2,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc_2 = nn.Linear(self.H*2, 1)\n",
    "\n",
    "    def forward(self, tensor_day):\n",
    "\n",
    "        batch_size = tensor_day.size()[0]\n",
    "\n",
    "        full_output = torch.tensor([]).to(device=self.device)\n",
    "\n",
    "        for d in range(self.observing_window):\n",
    "            # embedding layer applied to all tensors [16,400,200]\n",
    "            out_emb = self.embedding(tensor_day[:, d, :].squeeze(1))\n",
    "            # print('out_emb', out_emb.size())\n",
    "\n",
    "            # lstm layer applied to embedded tensors\n",
    "            output_lstm_day= self.fc_day(\\\n",
    "                                    self.lstm_day(out_emb)[0]\\\n",
    "                                        .reshape(batch_size, self.max_length * 2 * self.H))\n",
    "\n",
    "            # print('output_lstm_day', output_lstm_day.size())                   \n",
    "            # concatenate for all * days\n",
    "            full_output = torch.cat([full_output, output_lstm_day], dim=1) # [16, 768]\n",
    "\n",
    "        # print('full_output size: ', full_output.size(), '\\n')\n",
    "        # output = self.fc_1(full_output)\n",
    "        output, _ = self.lstm_adm(full_output)\n",
    "        # print('output after lstm_adm', output.size())\n",
    "        output = self.drop(output)\n",
    "        output = self.fc_2(output)\n",
    "        # print('output after fc_2', output.size())\n",
    "        output = torch.squeeze(output, 1)\n",
    "\n",
    "        # output = nn.Sigmoid()(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(pid_train_df, tokenizer=tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_day, tensor_labels, idx = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2, 400])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_day.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 265,   44, 1122,  ...,  128,   27,  254],\n",
       "         [ 265,   44, 1122,  ...,  602,   88,  344]],\n",
       "\n",
       "        [[ 265,   37, 1151,  ...,    0,    0,    0],\n",
       "         [ 265,   37, 1151,  ...,    0,    0,    0]],\n",
       "\n",
       "        [[ 265,   44,  441,  ...,    0,    0,    0],\n",
       "         [ 265,   44,  441,  ...,    0,    0,    0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 265,   37,  441,  ...,    0,    0,    0],\n",
       "         [ 265,   37,  441,  ...,    0,    0,    0]],\n",
       "\n",
       "        [[ 265,   44,  907,  ...,   99,  170,   88],\n",
       "         [ 265,   44,  907,  ...,   15,   43,   27]],\n",
       "\n",
       "        [[ 265,   37, 1257,  ...,    0,    0,    0],\n",
       "         [ 265,   37, 1257,  ...,    0,    0,    0]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9459, 9040, 1514, 7487, 2421, 7426, 6304, 8595, 4970, 5357, 9396, 6063,\n",
       "        4573, 2308, 2358, 8064])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject_id                                                        10921932\n",
       "hadm_id                                                           21504239\n",
       "demographics_in_visit    [WHITE M 30.0, WHITE M 30.0, WHITE M 30.0, WHI...\n",
       "lab_tests_in_visit       [Hematology Blood hematocrit {36.4} %; Hematol...\n",
       "medications_in_visit     [0.9% Sodium Chloride {250} mL ; D5 1/2NS {100...\n",
       "vitals_in_visit                        [nan, nan, nan, nan, nan, nan, nan]\n",
       "days_in_visit            [WHITE M 30.0$nan$Hematology Blood hematocrit ...\n",
       "aki_status_in_visit                              [0, 0, 1, 0, 0, nan, nan]\n",
       "days                                                 [0, 1, 2, 3, 4, 5, 6]\n",
       "Name: 5140, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_train_df.iloc[1514]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHR_MODEL(nn.Module):\n",
    "    def __init__(self, max_length, vocab_size, device, pred_window=2, observing_window=3,  H=128, embedding_size=200):\n",
    "        super(EHR_MODEL, self).__init__()\n",
    "\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.H = H\n",
    "        self.max_length = max_length\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device\n",
    "\n",
    "        # self.embedding = pretrained_model\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "\n",
    "        self.lstm_day = nn.LSTM(input_size=embedding_size,\n",
    "                            hidden_size=self.H,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "\n",
    "        self.fc_day = nn.Linear(self.max_length * 2 * self.H, 2048)\n",
    "\n",
    "        # self.fc_1 = nn.Linear(self.observing_window * 2 * self.H, 2048)\n",
    "\n",
    "        self.lstm_adm = nn.LSTM(input_size=2048*self.observing_window,\n",
    "                            hidden_size=self.H,\n",
    "                            num_layers=2,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc_2 = nn.Linear(self.H*2, 1)\n",
    "\n",
    "    def forward(self, tensor_day):\n",
    "\n",
    "        batch_size = tensor_day.size()[0]\n",
    "\n",
    "        full_output = torch.tensor([]).to(device=self.device)\n",
    "\n",
    "        for d in range(self.observing_window):\n",
    "            # embedding layer applied to all tensors [16,400,200]\n",
    "            out_emb = self.embedding(tensor_day[:, d, :].squeeze(1))\n",
    "            # print('out_emb', out_emb.size())\n",
    "\n",
    "            # lstm layer applied to embedded tensors\n",
    "            output_lstm_day= self.fc_day(\\\n",
    "                                    self.lstm_day(out_emb)[0]\\\n",
    "                                        .reshape(batch_size, self.max_length * 2 * self.H))\n",
    "\n",
    "            # print('output_lstm_day', output_lstm_day.size())                   \n",
    "            # concatenate for all * days\n",
    "            full_output = torch.cat([full_output, output_lstm_day], dim=1) # [16, 768]\n",
    "\n",
    "        # print('full_output size: ', full_output.size(), '\\n')\n",
    "        # output = self.fc_1(full_output)\n",
    "        output, _ = self.lstm_adm(full_output)\n",
    "        # print('output after lstm_adm', output.size())\n",
    "        output = self.drop(output)\n",
    "        output = self.fc_2(output)\n",
    "        # print('output after fc_2', output.size())\n",
    "        output = torch.squeeze(output, 1)\n",
    "\n",
    "        # output = nn.Sigmoid()(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EHR_MODEL(400,vocab_size, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_day, tensor_labels = next(iter(train_loader))\n",
    "tensor_day = tensor_day.to(device)\n",
    "tensor_labels = tensor_labels.to(device)\n",
    "output = model(tensor_day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5238, 0.5113, 0.5122, 0.5097, 0.5113, 0.5141, 0.5060, 0.5147],\n",
       "       device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_PATH = '/home/svetlana.maslenkova/LSTM'\n",
    "PKL_PATH = CURR_PATH+'/pickles/'\n",
    "DF_PATH = CURR_PATH +'/dataframes/'\n",
    "\n",
    "# loading the data\n",
    "with open(DF_PATH + 'pid_train_df_finetuning.pkl', 'rb') as f:\n",
    "    pid_train_df = pickle.load(f)\n",
    "\n",
    "with open(DF_PATH + 'pid_val_df_finetuning.pkl', 'rb') as f:\n",
    "    pid_val_df = pickle.load(f)\n",
    "\n",
    "with open(DF_PATH + 'pid_test_df_finetuning.pkl', 'rb') as f:\n",
    "    pid_test_df = pickle.load(f)\n",
    "\n",
    "tokenizer = Tokenizer.from_file(CURR_PATH + '/tokenizer.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Functions\n",
    "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(load_path, model, optimizer, device):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "def load_checkpoint(load_path, model, optimizer, device):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device, threshold=0.5, log_res=True):\n",
    "    model = model.to(device)\n",
    "    stacked_labels = torch.tensor([]).to(device)\n",
    "    stacked_preds = torch.tensor([]).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    step = 1\n",
    "    with torch.no_grad():\n",
    "        for tensor_day, tensor_labels in test_loader:\n",
    "            # print(f'Step {step}/{len(test_loader)}' )\n",
    "            labels = tensor_labels.to(device)\n",
    "            day_info = tensor_day.to(device)\n",
    "\n",
    "            output = model(day_info)\n",
    "            output = nn.Sigmoid()(output)\n",
    "            output = (output > threshold).int()\n",
    "\n",
    "            # stacking labels and predictions\n",
    "            stacked_labels = torch.cat([stacked_labels, labels], dim=0, )\n",
    "            stacked_preds = torch.cat([stacked_preds, output], dim=0, )\n",
    "\n",
    "            step += 1\n",
    "\n",
    "    # calculate accuracy\n",
    "    acc = torch.round(torch.sum(stacked_labels==stacked_preds) / len(stacked_labels), decimals=2)\n",
    "    # transfer to device\n",
    "    stacked_labels = stacked_labels.cpu().detach().numpy()\n",
    "    stacked_preds = stacked_preds.cpu().detach().numpy()\n",
    "    # get classification metrics for all samples in the test set\n",
    "    classification_report_res = classification_report(stacked_labels, stacked_preds, zero_division=0, output_dict=True)\n",
    "    print(classification_report(stacked_labels, stacked_preds, zero_division=0, output_dict=False))\n",
    "    if log_res:\n",
    "        for k, v in classification_report_res.items():\n",
    "            if k == 'accuracy':\n",
    "                 wandb.log({'test_accuracy': v})\n",
    "            else:\n",
    "                for k1, v1 in v.items():\n",
    "                    if k1 == 'support':continue\n",
    "                    else:\n",
    "                        wandb.log({'test_'+str(k) +'_'+str(k1): v1})\n",
    "\n",
    "    return classification_report_res, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MyDataset(pid_test_df.sample(frac=0.1), tokenizer=tokenizer, max_length=400)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EHR_MODEL(max_length=400, vocab_size=vocab_size, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65        83\n",
      "         1.0       0.00      0.00      0.00        89\n",
      "\n",
      "    accuracy                           0.48       172\n",
      "   macro avg       0.24      0.50      0.33       172\n",
      "weighted avg       0.23      0.48      0.31       172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_report_res, acc = evaluate(model, test_loader, device='cpu', log_res=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_0.0_precision 0.48255813953488375\n",
      "test_0.0_recall 1.0\n",
      "test_0.0_f1-score 0.6509803921568628\n",
      "test_1.0_precision 0.0\n",
      "test_1.0_recall 0.0\n",
      "test_1.0_f1-score 0.0\n",
      "test_accuracy 0.48255813953488375\n",
      "test_macro avg_precision 0.24127906976744187\n",
      "test_macro avg_recall 0.5\n",
      "test_macro avg_f1-score 0.3254901960784314\n",
      "test_weighted avg_precision 0.23286235803136832\n",
      "test_weighted avg_recall 0.48255813953488375\n",
      "test_weighted avg_f1-score 0.31413588691290467\n"
     ]
    }
   ],
   "source": [
    "for k, v in classification_report_res.items():\n",
    "    if k == 'accuracy':\n",
    "        print('test_accuracy', v)\n",
    "    else:\n",
    "        for k1, v1 in v.items():\n",
    "            if k1 == 'support':continue\n",
    "            else:\n",
    "                print('test_'+str(k) +'_'+str(k1), v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.0': {'precision': 0.48255813953488375,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 0.6509803921568628,\n",
       "  'support': 83},\n",
       " '1.0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 89},\n",
       " 'accuracy': 0.48255813953488375,\n",
       " 'macro avg': {'precision': 0.24127906976744187,\n",
       "  'recall': 0.5,\n",
       "  'f1-score': 0.3254901960784314,\n",
       "  'support': 172},\n",
       " 'weighted avg': {'precision': 0.23286235803136832,\n",
       "  'recall': 0.48255813953488375,\n",
       "  'f1-score': 0.31413588691290467,\n",
       "  'support': 172}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, \n",
    "          optimizer,\n",
    "          train_loader,\n",
    "          valid_loader,\n",
    "          file_path,\n",
    "          device='cpu',\n",
    "          num_epochs=5,\n",
    "          criterion = nn.BCELoss(),\n",
    "          pos_weight = torch.tensor([]),\n",
    "          best_valid_loss = float(\"Inf\"),\n",
    "          dimension=128,\n",
    "          epoch_patience=15,\n",
    "          threshold=0.5,\n",
    "          scheduler=None):\n",
    "    \n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    valid_running_acc = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    train_acc_list = []\n",
    "    valid_acc_list = []\n",
    "    global_steps_list = []\n",
    "    stop_training = 0\n",
    "\n",
    "    sigmoid_fn = nn.Sigmoid()\n",
    "\n",
    "    if criterion == 'BCEWithLogitsLoss':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        criterion.pos_weight = pos_weight.to(device)\n",
    "        use_sigmoid=False\n",
    "    else:\n",
    "        criterion = nn.BCELoss()\n",
    "        use_sigmoid = True\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):  \n",
    "        stacked_labels = torch.tensor([]).to(device)\n",
    "        stacked_preds = torch.tensor([]).to(device)\n",
    "\n",
    "        model.train()\n",
    "        for tensor_day, tensor_labels in train_loader:\n",
    "            # transferring everything to GPU\n",
    "            labels = tensor_labels.to(device)\n",
    "            tensor_day = tensor_day.to(device)\n",
    "\n",
    "            output = model(tensor_day)\n",
    "\n",
    "            if use_sigmoid:\n",
    "                loss = criterion(sigmoid_fn(output), labels.type(torch.float32))\n",
    "            else:\n",
    "                loss = criterion(output, labels.type(torch.float32))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            output_pred = sigmoid_fn(output)\n",
    "            output_pred = (output_pred > threshold).int()\n",
    "\n",
    "            stacked_labels = torch.cat([stacked_labels, labels], dim=0)\n",
    "            stacked_preds = torch.cat([stacked_preds, output_pred], dim=0)\n",
    "            \n",
    "            global_step += 1\n",
    "\n",
    "            wandb.log({'step_train_loss': loss.item(), 'global_step': global_step})\n",
    "            \n",
    "        # calculate accuracy\n",
    "        epoch_train_accuracy = torch.round(torch.sum(stacked_labels==stacked_preds) / len(stacked_labels), decimals=2)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            print(f'Learning rate is {get_lr(optimizer)}')\n",
    "\n",
    "        model.eval()\n",
    "        stacked_labels = torch.tensor([]).to(device)\n",
    "        stacked_preds = torch.tensor([]).to(device)\n",
    "        with torch.no_grad():\n",
    "            # validation loop\n",
    "            for tensor_day, tensor_labels in valid_loader:\n",
    "                labels = tensor_labels.to(device)\n",
    "                tensor_day = tensor_day.to(device)\n",
    "                \n",
    "                output = model(tensor_day)\n",
    "\n",
    "                if use_sigmoid:\n",
    "                    loss = criterion(sigmoid_fn(output), labels.type(torch.float32))\n",
    "                else:\n",
    "                    loss = criterion(output, labels.type(torch.float32))\n",
    "\n",
    "                valid_running_loss += loss.item()\n",
    "\n",
    "                output_pred = sigmoid_fn(output)\n",
    "                output_pred = (output_pred > threshold).int()\n",
    "\n",
    "                # stacking labels and predictions\n",
    "                stacked_labels = torch.cat([stacked_labels, labels], dim=0)\n",
    "                stacked_preds = torch.cat([stacked_preds, output_pred], dim=0)\n",
    "\n",
    "        # transfer to device\n",
    "        stacked_labels = stacked_labels.cpu().detach().numpy()\n",
    "        stacked_preds = stacked_preds.cpu().detach().numpy()\n",
    "        # calculate accuracy\n",
    "        epoch_val_accuracy = np.round(np.sum(stacked_labels==stacked_preds) / len(stacked_labels), 2)\n",
    "        # get classification metrics for all samples in the test set\n",
    "        classification_report_res = classification_report(stacked_labels, stacked_preds, zero_division=0, output_dict=True)\n",
    "        classification_report_res.update({'epoch':epoch+1})\n",
    "\n",
    "        # log the evaluation metrics \n",
    "        for key, value in classification_report_res.items():\n",
    "            wandb.log({key:value, 'epoch':epoch+1})\n",
    "\n",
    "        # valid loss\n",
    "        epoch_average_train_loss = running_loss / len(train_loader)  \n",
    "        epoch_average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "\n",
    "        train_loss_list.append(epoch_average_train_loss)\n",
    "        valid_loss_list.append(epoch_average_valid_loss)\n",
    "        train_acc_list.append(epoch_train_accuracy)\n",
    "        valid_acc_list.append(epoch_val_accuracy)\n",
    "\n",
    "\n",
    "        global_steps_list.append(global_step)\n",
    "        wandb.log({'epoch_average_train_loss': epoch_average_train_loss,\n",
    "                    'epoch_average_valid_loss': epoch_average_valid_loss,\n",
    "                    'epoch_val_accuracy': epoch_val_accuracy, \n",
    "                    'epoch_train_accuracy': epoch_train_accuracy,\n",
    "                    'epoch': epoch+1})\n",
    "\n",
    "        # resetting running values\n",
    "        running_loss = 0.0                \n",
    "        valid_running_loss = 0.0\n",
    "        \n",
    "        \n",
    "        # print progress\n",
    "        print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}, Valid accuracy: {:.4f}'\n",
    "                .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                        epoch_average_train_loss, epoch_average_valid_loss, epoch_val_accuracy))    \n",
    "\n",
    "        # checkpoint\n",
    "        if best_valid_loss > epoch_average_valid_loss:\n",
    "            best_valid_loss = epoch_average_valid_loss\n",
    "            save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)\n",
    "            stop_training = 0\n",
    "        else:\n",
    "            stop_training +=1\n",
    "        \n",
    "        if stop_training == epoch_patience:\n",
    "            break\n",
    "\n",
    "\n",
    "    # save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Finished Training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(saving_folder_name=None, criterion='BCELoss', small_dataset=False,\\\n",
    "     use_gpu=True, project_name='test', pred_window=2, observing_window=3, BATCH_SIZE=128, LR=0.0001,\\\n",
    "         min_frequency=1, hidden_size=128, num_epochs=50, wandb_mode='online', PRETRAINED_PATH=None, run_id=None):\n",
    "    # define the device\n",
    "    if use_gpu:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    else:\n",
    "        device='cpu'\n",
    "\n",
    "    #paths\n",
    "    CURR_PATH = os.getcwd()\n",
    "    PKL_PATH = CURR_PATH+'/pickles/'\n",
    "    DF_PATH = CURR_PATH +'/dataframes/'\n",
    "    TXT_DIR_TRAIN = CURR_PATH + '/txt_files/train'\n",
    "    destination_folder = CURR_PATH + '/training/'\n",
    "\n",
    "\n",
    "    # Training the tokenizer\n",
    "    if exists(CURR_PATH + '/tokenizer.json'):\n",
    "        tokenizer = Tokenizer.from_file(CURR_PATH + '/tokenizer.json')\n",
    "    else:\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\"], min_frequency=1)\n",
    "        files = glob.glob(TXT_DIR_TRAIN+'/*')\n",
    "        tokenizer.train(files, trainer)\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    # variables for classes\n",
    "    max_length = 400\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    embedding_size = 200\n",
    "    dimension = 128\n",
    "    \n",
    "\n",
    "    # loading the data\n",
    "    with open(DF_PATH + 'pid_train_df_finetuning.pkl', 'rb') as f:\n",
    "        pid_train_df = pickle.load(f)\n",
    "\n",
    "    with open(DF_PATH + 'pid_val_df_finetuning.pkl', 'rb') as f:\n",
    "        pid_val_df = pickle.load(f)\n",
    "\n",
    "    with open(DF_PATH + 'pid_test_df_finetuning.pkl', 'rb') as f:\n",
    "        pid_test_df = pickle.load(f)\n",
    "\n",
    "    print('filtering admissions..')\n",
    "    # filter the admissions\n",
    "    train_admissions = []\n",
    "    train_admissions = []\n",
    "    for adm in pid_train_df.hadm_id.unique():   \n",
    "        if ({1,2,3,4}.issubset(set(pid_train_df[pid_train_df.hadm_id==adm].days.values[0])) or \\\n",
    "            {-1,0,1,2}.issubset(set(pid_train_df[pid_train_df.hadm_id==adm].days.values[0]))or \\\n",
    "                {0,1,2,3}.issubset(set(pid_train_df[pid_train_df.hadm_id==adm].days.values[0]))) and \\\n",
    "            (len(pid_train_df[pid_train_df.hadm_id==adm].days.values[0])>3) and\\\n",
    "                sum(pid_train_df[pid_train_df.hadm_id==adm].aki_status_in_visit.values[0][:2])==0:\n",
    "            train_admissions.append(adm)\n",
    "\n",
    "    val_admissions = []\n",
    "    for adm in pid_val_df.hadm_id.unique():   \n",
    "        if ({1,2,3,4}.issubset(set(pid_val_df[pid_val_df.hadm_id==adm].days.values[0])) or \\\n",
    "            {-1,0,1,2}.issubset(set(pid_val_df[pid_val_df.hadm_id==adm].days.values[0]))or \\\n",
    "                {0,1,2,3}.issubset(set(pid_val_df[pid_val_df.hadm_id==adm].days.values[0]))) and \\\n",
    "            (len(pid_val_df[pid_val_df.hadm_id==adm].days.values[0])>3) and\\\n",
    "                sum(pid_val_df[pid_val_df.hadm_id==adm].aki_status_in_visit.values[0][:2])==0:\n",
    "            val_admissions.append(adm)\n",
    "\n",
    "    test_admissions = []\n",
    "    for adm in pid_test_df.hadm_id.unique():   \n",
    "        if ({1,2,3,4}.issubset(set(pid_test_df[pid_test_df.hadm_id==adm].days.values[0])) or \\\n",
    "            {-1,0,1,2}.issubset(set(pid_test_df[pid_test_df.hadm_id==adm].days.values[0]))or \\\n",
    "                {0,1,2,3}.issubset(set(pid_test_df[pid_test_df.hadm_id==adm].days.values[0]))) and \\\n",
    "            (len(pid_test_df[pid_test_df.hadm_id==adm].days.values[0])>3) and\\\n",
    "                sum(pid_test_df[pid_test_df.hadm_id==adm].aki_status_in_visit.values[0][:2])==0:\n",
    "            test_admissions.append(adm)\n",
    "\n",
    "    pid_train_df = pid_train_df[pid_train_df.hadm_id.isin(train_admissions)]\n",
    "    pid_val_df = pid_val_df[pid_val_df.hadm_id.isin(val_admissions)]\n",
    "    pid_test_df = pid_test_df[pid_test_df.hadm_id.isin(test_admissions)]\n",
    "\n",
    "    if small_dataset: frac=0.1\n",
    "    else: frac=1\n",
    "\n",
    "    train_dataset = MyDataset(pid_train_df.sample(frac=frac), tokenizer=tokenizer, max_length=400)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    val_dataset = MyDataset(pid_val_df.sample(frac=frac), tokenizer=tokenizer, max_length=400)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    test_dataset = MyDataset(pid_test_df.sample(frac=frac), tokenizer=tokenizer, max_length=400)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "    #print shapes\n",
    "    tensor_day, tensor_labels = next(iter(train_loader))\n",
    "    print('\\n\\n DATA SHAPES: ')\n",
    "    print('train data shape: ', pid_train_df.shape)\n",
    "    print('val data shape: ', pid_val_df.shape)\n",
    "    print('test data shape: ', pid_test_df.shape)\n",
    "\n",
    "    print('tensor_day', tensor_day.shape)\n",
    "    print('tensor_labels', tensor_labels.shape)\n",
    "\n",
    "    # file_path = destination_folder + '/88087_no_weights-lr0.00005-adam'\n",
    "    \n",
    "    model = EHR_MODEL(vocab_size=vocab_size, max_length=max_length, device=device, pred_window=2, observing_window=3).to(device)\n",
    "\n",
    "    if PRETRAINED_PATH is not None:\n",
    "        model.load_state_dict(torch.load(PRETRAINED_PATH, map_location=device)['model_state_dict'])\n",
    "        model.embedding.weight.data = model.embedding.weight.data\n",
    "        print(f\"Pretrained model loaded from <=== {PRETRAINED_PATH}\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    # exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=45, gamma=0.1)\n",
    "    exp_lr_scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[15, 50, 80], gamma=0.1)\n",
    "\n",
    "    train_params = {\n",
    "                    'model':model,\n",
    "                    'device':device,\n",
    "                    'optimizer':optimizer,\n",
    "                    'criterion':criterion,\n",
    "                    'train_loader':train_loader,\n",
    "                    'valid_loader':val_loader,\n",
    "                    'num_epochs':num_epochs, \n",
    "                    'file_path':destination_folder,\n",
    "                    'best_valid_loss':float(\"Inf\"),\n",
    "                    'dimension':128,\n",
    "                    'epoch_patience':10,\n",
    "                    'threshold':0.5,\n",
    "                    'scheduler':exp_lr_scheduler\n",
    "                }\n",
    "\n",
    "    # path for the model\n",
    "    if saving_folder_name is None:\n",
    "        saving_folder_name = 'FX_NO_PRETRAINING_' + str(len(train_loader)*BATCH_SIZE // 1000) + 'k_'  \\\n",
    "            + 'lr' + str(LR) + '_h'+ str(hidden_size) + '_pw' + str(pred_window) + '_ow' + str(observing_window)\n",
    "    \n",
    "    file_path = destination_folder + saving_folder_name\n",
    "    train_params['file_path'] = file_path\n",
    "\n",
    "    print(f'\\n\\nMODEL PATH: {file_path}')\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "    run_name = saving_folder_name\n",
    "\n",
    "    # wandb setup\n",
    "    os.environ['WANDB_API_KEY'] = '8e859a0fc58f296096842a367ca532717d3b4059'\n",
    "    \n",
    "    if run_id is None:    \n",
    "        run_id = wandb.util.generate_id()  \n",
    "        resume = 'allow' \n",
    "    else:\n",
    "        resume = 'must'\n",
    "        \n",
    "    args = {'optimizer':optimizer, 'criterion':'BCELoss', 'LR':LR, 'min_frequency':min_frequency, 'hidden_size':hidden_size, 'pred_window':pred_window, 'experiment':'no_pretraining'}\n",
    "    wandb.init(project=project_name, name=run_name, mode=wandb_mode, config=args, id=run_id, resume=resume)\n",
    "    print('Run id is: ', run_id)\n",
    "\n",
    "    # training\n",
    "    print('Training started..')\n",
    "    train(**train_params)\n",
    "\n",
    "    # testing\n",
    "    print('\\nTesting the model...')\n",
    "    load_checkpoint(file_path + '/model.pt', model, optimizer, device=device)\n",
    "    evaluate(model, test_loader, device, threshold=0.5, log_res=True)\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the ratio of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "with open(DF_PATH + 'pid_train_df_finetuning.pkl', 'rb') as f:\n",
    "    pid_train_df = pickle.load(f)\n",
    "\n",
    "with open(DF_PATH + 'pid_val_df_finetuning.pkl', 'rb') as f:\n",
    "    pid_val_df = pickle.load(f)\n",
    "\n",
    "with open(DF_PATH + 'pid_test_df_finetuning.pkl', 'rb') as f:\n",
    "    pid_test_df = pickle.load(f)\n",
    "\n",
    "train_admissions = []\n",
    "for adm in pid_train_df.hadm_id.unique():   \n",
    "    if ({1,2,3,4}.issubset(set(pid_train_df[pid_train_df.hadm_id==adm].days.values[0])) or \\\n",
    "        {-1,0,1,2}.issubset(set(pid_train_df[pid_train_df.hadm_id==adm].days.values[0]))or \\\n",
    "            {0,1,2,3}.issubset(set(pid_train_df[pid_train_df.hadm_id==adm].days.values[0]))) and \\\n",
    "        (len(pid_train_df[pid_train_df.hadm_id==adm].days.values[0])>3) and\\\n",
    "            sum(pid_train_df[pid_train_df.hadm_id==adm].aki_status_in_visit.values[0][:2])==0:\n",
    "        train_admissions.append(adm)\n",
    "\n",
    "val_admissions = []\n",
    "for adm in pid_val_df.hadm_id.unique():   \n",
    "    if ({1,2,3,4}.issubset(set(pid_val_df[pid_val_df.hadm_id==adm].days.values[0])) or \\\n",
    "        {-1,0,1,2}.issubset(set(pid_val_df[pid_val_df.hadm_id==adm].days.values[0]))or \\\n",
    "            {0,1,2,3}.issubset(set(pid_val_df[pid_val_df.hadm_id==adm].days.values[0]))) and \\\n",
    "        (len(pid_val_df[pid_val_df.hadm_id==adm].days.values[0])>3) and\\\n",
    "            sum(pid_val_df[pid_val_df.hadm_id==adm].aki_status_in_visit.values[0][:2])==0:\n",
    "        val_admissions.append(adm)\n",
    "\n",
    "test_admissions = []\n",
    "for adm in pid_test_df.hadm_id.unique():   \n",
    "    if ({1,2,3,4}.issubset(set(pid_test_df[pid_test_df.hadm_id==adm].days.values[0])) or \\\n",
    "        {-1,0,1,2}.issubset(set(pid_test_df[pid_test_df.hadm_id==adm].days.values[0]))or \\\n",
    "            {0,1,2,3}.issubset(set(pid_test_df[pid_test_df.hadm_id==adm].days.values[0]))) and \\\n",
    "        (len(pid_test_df[pid_test_df.hadm_id==adm].days.values[0])>3) and\\\n",
    "            sum(pid_test_df[pid_test_df.hadm_id==adm].aki_status_in_visit.values[0][:2])==0:\n",
    "        test_admissions.append(adm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_train_df = pid_train_df[pid_train_df.hadm_id.isin(train_admissions)]\n",
    "pid_val_df = pid_val_df[pid_val_df.hadm_id.isin(val_admissions)]\n",
    "pid_test_df = pid_test_df[pid_test_df.hadm_id.isin(test_admissions)]\n",
    "\n",
    "\n",
    "train_dataset = MyDataset(pid_train_df.sample(frac=1), tokenizer=tokenizer, max_length=400)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "val_dataset = MyDataset(pid_val_df.sample(frac=1), tokenizer=tokenizer, max_length=400)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_negatives = 0\n",
    "train_positives = 0\n",
    "for _, row in pid_train_df.iterrows():\n",
    "    if (row.aki_status_in_visit[3:5] == [0, 0]) or (row.aki_status_in_visit[3:5] == [np.nan, 0]) or (row.aki_status_in_visit[3:5] == [0, np.nan]):\n",
    "        train_negatives += 1\n",
    "    else:\n",
    "        train_positives += 1\n",
    "\n",
    "val_negatives = 0\n",
    "val_positives = 0\n",
    "for _, row in pid_val_df.iterrows():\n",
    "    if (row.aki_status_in_visit[3:5] == [0, 0]) or (row.aki_status_in_visit[3:5] == [np.nan, 0]) or (row.aki_status_in_visit[3:5] == [0, np.nan]):\n",
    "        val_negatives += 1\n",
    "    else:\n",
    "        val_positives += 1\n",
    "\n",
    "test_negatives = 0\n",
    "test_positives = 0\n",
    "for _, row in pid_test_df.iterrows():\n",
    "    if (row.aki_status_in_visit[3:5] == [0, 0]) or (row.aki_status_in_visit[3:5] == [np.nan, 0]) or (row.aki_status_in_visit[3:5] == [0, np.nan]):\n",
    "        test_negatives += 1\n",
    "    else:\n",
    "        test_positives += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -  Number of pos: 4962, neg: 5005, total: 9967, ratio of positives: 0.49784288150897965\n",
      "Validation -  Number of pos: 492, neg: 405, total: 897, ratio of positives: 0.5484949832775919\n",
      "Test -  Number of pos: 439, neg: 397, total: 836, ratio of positives: 0.5251196172248804\n"
     ]
    }
   ],
   "source": [
    "print(f'Train -  Number of pos: {train_positives}, neg: {train_negatives}, total: {pid_train_df.shape[0]}, ratio of positives: {train_positives/(pid_train_df.shape[0])}')\n",
    "print(f'Validation -  Number of pos: {val_positives}, neg: {val_negatives}, total: {pid_val_df.shape[0]}, ratio of positives: {val_positives/(pid_val_df.shape[0])}')\n",
    "print(f'Test -  Number of pos: {test_positives}, neg: {test_negatives}, total: {pid_test_df.shape[0]}, ratio of positives: {test_positives/(pid_test_df.shape[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "CURR_PATH = os.getcwd()\n",
    "PKL_PATH = CURR_PATH+'/pickles/'\n",
    "DF_PATH = CURR_PATH +'/dataframes/'\n",
    "TXT_DIR_TRAIN = CURR_PATH + '/txt_files/train'\n",
    "destination_folder = CURR_PATH + '/training/'\n",
    "\n",
    "tokenizer = Tokenizer.from_file(CURR_PATH + '/tokenizer.json')\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EHR_MODEL(vocab_size=vocab_size, max_length=400, device=device, pred_window=2, observing_window=3)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-7, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_PATH = '/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/FX_NO_PRETRAINING_10k_lr0.001_h128_pw2_ow3/model.pt'\n",
    "run_id = '3uu5b8vw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " DATA SHAPES: \n",
      "train data shape:  (9967, 9)\n",
      "val data shape:  (897, 9)\n",
      "test data shape:  (836, 9)\n",
      "tensor_day torch.Size([512, 3, 400])\n",
      "tensor_labels torch.Size([512])\n",
      "Pretrained model loaded from <=== /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/FX_NO_PRETRAINING_10k_lr0.001_h128_pw2_ow3/model.pt\n",
      "\n",
      "\n",
      "MODEL PATH: /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/FX_NO_PRETRAINING_10k_lr1e-05_h128_pw2_ow3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.18 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/wandb/run-20220622_134901-3uu5b8vw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href=\"https://wandb.ai/maslenkovas/Fixed_obs_window_model/runs/3uu5b8vw\" target=\"_blank\">FX_NO_PRETRAINING_10k_lr1e-05_h128_pw2_ow3</a></strong> to <a href=\"https://wandb.ai/maslenkovas/Fixed_obs_window_model\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run id is:  3uu5b8vw\n",
      "Training started..\n",
      "Learning rate is 1e-05\n",
      "Epoch [1/30], Step [20/600], Train Loss: 0.6470, Valid Loss: 0.6299, Valid accuracy: 0.6500\n",
      "Model saved to ==> /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/FX_NO_PRETRAINING_10k_lr1e-05_h128_pw2_ow3/model.pt\n",
      "Learning rate is 1e-05\n",
      "Epoch [2/30], Step [40/600], Train Loss: 0.6458, Valid Loss: 0.6298, Valid accuracy: 0.6400\n",
      "Model saved to ==> /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/FX_NO_PRETRAINING_10k_lr1e-05_h128_pw2_ow3/model.pt\n",
      "Learning rate is 1e-05\n",
      "Epoch [3/30], Step [60/600], Train Loss: 0.6453, Valid Loss: 0.6292, Valid accuracy: 0.6500\n",
      "Model saved to ==> /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/FX_NO_PRETRAINING_10k_lr1e-05_h128_pw2_ow3/model.pt\n",
      "Learning rate is 1e-05\n",
      "Epoch [4/30], Step [80/600], Train Loss: 0.6449, Valid Loss: 0.6326, Valid accuracy: 0.6400\n",
      "Learning rate is 1e-05\n",
      "Epoch [5/30], Step [100/600], Train Loss: 0.6441, Valid Loss: 0.6299, Valid accuracy: 0.6500\n",
      "Learning rate is 1e-05\n",
      "Epoch [6/30], Step [120/600], Train Loss: 0.6461, Valid Loss: 0.6282, Valid accuracy: 0.6400\n",
      "Model saved to ==> /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/FX_NO_PRETRAINING_10k_lr1e-05_h128_pw2_ow3/model.pt\n",
      "Learning rate is 1e-05\n",
      "Epoch [7/30], Step [140/600], Train Loss: 0.6444, Valid Loss: 0.6260, Valid accuracy: 0.6400\n",
      "Model saved to ==> /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/FX_NO_PRETRAINING_10k_lr1e-05_h128_pw2_ow3/model.pt\n",
      "Learning rate is 1e-05\n",
      "Epoch [8/30], Step [160/600], Train Loss: 0.6447, Valid Loss: 0.6293, Valid accuracy: 0.6500\n",
      "Learning rate is 1e-05\n",
      "Epoch [9/30], Step [180/600], Train Loss: 0.6437, Valid Loss: 0.6268, Valid accuracy: 0.6400\n",
      "Learning rate is 1.0000000000000002e-06\n",
      "Epoch [10/30], Step [200/600], Train Loss: 0.6442, Valid Loss: 0.6277, Valid accuracy: 0.6400\n",
      "Learning rate is 1.0000000000000002e-06\n",
      "Epoch [11/30], Step [220/600], Train Loss: 0.6438, Valid Loss: 0.6271, Valid accuracy: 0.6500\n",
      "Learning rate is 1.0000000000000002e-06\n",
      "Epoch [12/30], Step [240/600], Train Loss: 0.6445, Valid Loss: 0.6297, Valid accuracy: 0.6500\n",
      "Finished Training!\n",
      "\n",
      "Testing the model...\n",
      "Model loaded from <== /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/FX_NO_PRETRAINING_10k_lr1e-05_h128_pw2_ow3/model.pt\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.92      0.74       500\n",
      "         1.0       0.57      0.16      0.25       336\n",
      "\n",
      "    accuracy                           0.61       836\n",
      "   macro avg       0.59      0.54      0.50       836\n",
      "weighted avg       0.60      0.61      0.54       836\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fb501a294c4318b621d05bd4002811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>█▆▇▇█▇▇█▆▆▇▇▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>epoch_average_train_loss</td><td>█▆▄▄▂▆▂▃▁▂▁▃</td></tr><tr><td>epoch_average_valid_loss</td><td>▅▅▄█▅▃▁▄▂▃▂▅</td></tr><tr><td>epoch_train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch_val_accuracy</td><td>█▁█▁█▁▁█▁▁██</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>step_train_loss</td><td>▆▅▄█▃█▆▅▄▄▅▅▄▄▃▆▅▄▄▄▃▅▆▂▄▃▄▃▅▄▅▁▄▆▃▅▄▄▄▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.61364</td></tr><tr><td>epoch</td><td>12</td></tr><tr><td>epoch_average_train_loss</td><td>0.64451</td></tr><tr><td>epoch_average_valid_loss</td><td>0.62971</td></tr><tr><td>epoch_train_accuracy</td><td>0.63</td></tr><tr><td>epoch_val_accuracy</td><td>0.65</td></tr><tr><td>global_step</td><td>240</td></tr><tr><td>step_train_loss</td><td>0.66949</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">FX_NO_PRETRAINING_10k_lr1e-05_h128_pw2_ow3</strong>: <a href=\"https://wandb.ai/maslenkovas/Fixed_obs_window_model/runs/3uu5b8vw\" target=\"_blank\">https://wandb.ai/maslenkovas/Fixed_obs_window_model/runs/3uu5b8vw</a><br/>Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220622_134901-3uu5b8vw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main(saving_folder_name=None, criterion='BCELoss', small_dataset=False,\\\n",
    "     use_gpu=True, project_name='Fixed_obs_window_model', pred_window=2, BATCH_SIZE=800, LR=1e-05,\\\n",
    "         min_frequency=1, hidden_size=128, num_epochs=100, wandb_mode='online', PRETRAINED_PATH=PRETRAINED_PATH, run_id=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f41d6bb73514239eeae9d84db1aabfbecf43feae35a358a827a9ce792198f6fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
