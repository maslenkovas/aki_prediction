{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from distutils.command.config import config\n",
    "import pickle5 as pickle\n",
    "\n",
    "# Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import os\n",
    "from os.path import exists\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import f1_score, multilabel_confusion_matrix, accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Tokenization\n",
    "from tokenizers import  Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import glob\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>admittime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000019</td>\n",
       "      <td>25058216</td>\n",
       "      <td>DV290 DV502 DV3000 DV053</td>\n",
       "      <td>2129-05-21 19:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853</td>\n",
       "      <td>D5715 D496 D5723 D78959 D30981 DV1582 D07070 D...</td>\n",
       "      <td>2180-05-06 22:23:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22841357</td>\n",
       "      <td>D07071 DV08 D2761 D5715 D78959 D496 D3051 D2875</td>\n",
       "      <td>2180-06-26 18:27:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000032</td>\n",
       "      <td>25742920</td>\n",
       "      <td>D5715 DV08 D78959 D07054 D2761 D2767 D3051 D49...</td>\n",
       "      <td>2180-08-05 23:44:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034</td>\n",
       "      <td>D2761 D29680 DV4986 D5715 D7994 D07044 D3051 D...</td>\n",
       "      <td>2180-07-23 12:35:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id                                           icd_code  \\\n",
       "0    10000019  25058216                           DV290 DV502 DV3000 DV053   \n",
       "1    10000032  22595853  D5715 D496 D5723 D78959 D30981 DV1582 D07070 D...   \n",
       "2    10000032  22841357    D07071 DV08 D2761 D5715 D78959 D496 D3051 D2875   \n",
       "3    10000032  25742920  D5715 DV08 D78959 D07054 D2761 D2767 D3051 D49...   \n",
       "4    10000032  29079034  D2761 D29680 DV4986 D5715 D7994 D07044 D3051 D...   \n",
       "\n",
       "            admittime  \n",
       "0 2129-05-21 19:16:00  \n",
       "1 2180-05-06 22:23:00  \n",
       "2 2180-06-26 18:27:00  \n",
       "3 2180-08-05 23:44:00  \n",
       "4 2180-07-23 12:35:00  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PKL_PATH = os.getcwd() + '/pickles/'\n",
    "\n",
    "with open(PKL_PATH + 'pid_hid_dignoses.pkl', 'rb') as handle:\n",
    "    data_diagnoses = pickle.load(handle)\n",
    "\n",
    "data_diagnoses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>admittime</th>\n",
       "      <th>dischtime</th>\n",
       "      <th>deathtime</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>admission_location</th>\n",
       "      <th>discharge_location</th>\n",
       "      <th>insurance</th>\n",
       "      <th>language</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>edregtime</th>\n",
       "      <th>edouttime</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14679932</td>\n",
       "      <td>21038362</td>\n",
       "      <td>2139-09-26 14:16:00</td>\n",
       "      <td>2139-09-28 11:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Other</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15585972</td>\n",
       "      <td>24941086</td>\n",
       "      <td>2123-10-07 23:56:00</td>\n",
       "      <td>2123-10-12 11:22:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOME</td>\n",
       "      <td>Other</td>\n",
       "      <td>ENGLISH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id           admittime            dischtime deathtime  \\\n",
       "0    14679932  21038362 2139-09-26 14:16:00  2139-09-28 11:30:00       NaN   \n",
       "1    15585972  24941086 2123-10-07 23:56:00  2123-10-12 11:22:00       NaN   \n",
       "\n",
       "  admission_type admission_location discharge_location insurance language  \\\n",
       "0       ELECTIVE                NaN               HOME     Other  ENGLISH   \n",
       "1       ELECTIVE                NaN               HOME     Other  ENGLISH   \n",
       "\n",
       "  marital_status ethnicity edregtime edouttime  hospital_expire_flag  \n",
       "0         SINGLE   UNKNOWN       NaN       NaN                     0  \n",
       "1            NaN     WHITE       NaN       NaN                     0  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = '/home/svetlanamaslenkova/Documents/data/'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_admissions = pd.read_csv(DATA_PATH+'mimic-iv-1.0/core/admissions.csv')\n",
    "\n",
    "data_admissions.columns = data_admissions.columns.str.lower()\n",
    "data_admissions['hadm_id'] = data_admissions['hadm_id'].astype(int)\n",
    "format_ = '%Y-%m-%d %H:%M:%S'\n",
    "data_admissions['admittime'] = pd.to_datetime(data_admissions['admittime'], format=format_)\n",
    "\n",
    "data_admissions.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>seq_num</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>icd_version</th>\n",
       "      <th>admittime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15734973</td>\n",
       "      <td>20475282</td>\n",
       "      <td>3</td>\n",
       "      <td>2825</td>\n",
       "      <td>9</td>\n",
       "      <td>2112-02-02 07:53:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15734973</td>\n",
       "      <td>20475282</td>\n",
       "      <td>2</td>\n",
       "      <td>V0251</td>\n",
       "      <td>9</td>\n",
       "      <td>2112-02-02 07:53:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15734973</td>\n",
       "      <td>20475282</td>\n",
       "      <td>5</td>\n",
       "      <td>V270</td>\n",
       "      <td>9</td>\n",
       "      <td>2112-02-02 07:53:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15734973</td>\n",
       "      <td>20475282</td>\n",
       "      <td>1</td>\n",
       "      <td>64891</td>\n",
       "      <td>9</td>\n",
       "      <td>2112-02-02 07:53:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15734973</td>\n",
       "      <td>20475282</td>\n",
       "      <td>4</td>\n",
       "      <td>66481</td>\n",
       "      <td>9</td>\n",
       "      <td>2112-02-02 07:53:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id  seq_num icd_code  icd_version           admittime\n",
       "0    15734973  20475282        3     2825            9 2112-02-02 07:53:00\n",
       "1    15734973  20475282        2    V0251            9 2112-02-02 07:53:00\n",
       "2    15734973  20475282        5     V270            9 2112-02-02 07:53:00\n",
       "3    15734973  20475282        1    64891            9 2112-02-02 07:53:00\n",
       "4    15734973  20475282        4    66481            9 2112-02-02 07:53:00"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_diagnoses = pd.read_csv(DATA_PATH+'mimic-iv-1.0/hosp/diagnoses_icd.csv')\n",
    "\n",
    "data_diagnoses.columns = data_diagnoses.columns.str.lower()\n",
    "\n",
    "data_diagnoses = data_diagnoses.merge(data_admissions[['hadm_id', 'admittime']])\n",
    "\n",
    "data_diagnoses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>seq_num</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>icd_version</th>\n",
       "      <th>admittime</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hadm_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21825811</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26961782</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25253144</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28756559</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21128663</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25069705</th>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25668484</th>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24773199</th>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23076003</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27635276</th>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>521110 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          subject_id  seq_num  icd_code  icd_version  admittime\n",
       "hadm_id                                                        \n",
       "21825811           1        1         1            1          1\n",
       "26961782           1        1         1            1          1\n",
       "25253144           1        1         1            1          1\n",
       "28756559           1        1         1            1          1\n",
       "21128663           1        1         1            1          1\n",
       "...              ...      ...       ...          ...        ...\n",
       "25069705          39       39        39           39         39\n",
       "25668484          39       39        39           39         39\n",
       "24773199          42       42        42           42         42\n",
       "23076003          50       50        50           50         50\n",
       "27635276          57       57        57           57         57\n",
       "\n",
       "[521110 rows x 5 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_diagnoses.groupby('hadm_id').count().sort_values('icd_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>demographics_in_visit</th>\n",
       "      <th>lab_tests_in_visit</th>\n",
       "      <th>medications_in_visit</th>\n",
       "      <th>vitals_in_visit</th>\n",
       "      <th>days_in_visit</th>\n",
       "      <th>aki_status_in_visit</th>\n",
       "      <th>days</th>\n",
       "      <th>admittime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17223646</td>\n",
       "      <td>20023461</td>\n",
       "      <td>[WHITE F 69.0, WHITE F 69.0, WHITE F 69.0, WHI...</td>\n",
       "      <td>[Hematology Blood hematocrit {37.7} %; Hematol...</td>\n",
       "      <td>[Sodium Chloride 0.9%  Flush {3} mL ; Heparin ...</td>\n",
       "      <td>[temp {97.7} heartrate {68.0} resprate {18.0} ...</td>\n",
       "      <td>[WHITE F 69.0$temp {97.7} heartrate {68.0} res...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, nan...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>2152-10-31 20:11:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18371764</td>\n",
       "      <td>20046098</td>\n",
       "      <td>[WHITE M 56.0, WHITE M 56.0, WHITE M 56.0, WHI...</td>\n",
       "      <td>[Hematology Blood hematocrit {40.1} %; Hematol...</td>\n",
       "      <td>[Sodium Chloride 0.9%  Flush {3} mL ; D5 1/2NS...</td>\n",
       "      <td>[temp {100.3} heartrate {103.0} resprate {18.0...</td>\n",
       "      <td>[WHITE M 56.0$temp {100.3} heartrate {103.0} r...</td>\n",
       "      <td>[0, 0, nan, nan]</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>2162-09-15 13:43:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17416292</td>\n",
       "      <td>20051972</td>\n",
       "      <td>[UNKNOWN F 56.0, UNKNOWN F 56.0, UNKNOWN F 56....</td>\n",
       "      <td>[Hematology Blood hematocrit {27.5} %; Hematol...</td>\n",
       "      <td>[ARIPiprazole {10} mg ; Docusate Sodium {100} ...</td>\n",
       "      <td>[temp {} heartrate {68.0} resprate {22.0} o2sa...</td>\n",
       "      <td>[UNKNOWN F 56.0$temp {} heartrate {68.0} respr...</td>\n",
       "      <td>[0, 0, 0, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8]</td>\n",
       "      <td>2168-04-06 03:17:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15109929</td>\n",
       "      <td>20058401</td>\n",
       "      <td>[WHITE M 77.0, WHITE M 77.0, WHITE M 77.0, WHI...</td>\n",
       "      <td>[Hematology Blood hematocrit {24.0} %; Hematol...</td>\n",
       "      <td>[nan, Sodium Chloride 0.9%  Flush {3} mL ; Ace...</td>\n",
       "      <td>[temp {97.9} heartrate {64.0} resprate {18.0} ...</td>\n",
       "      <td>[WHITE M 77.0$temp {97.9} heartrate {64.0} res...</td>\n",
       "      <td>[0, 0, 0, 0, 0, nan, nan]</td>\n",
       "      <td>[-1, 0, 1, 2, 3, 4, 5]</td>\n",
       "      <td>2157-11-11 01:29:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14687005</td>\n",
       "      <td>20064641</td>\n",
       "      <td>[HISPANIC/LATINO M 48.0, HISPANIC/LATINO M 48....</td>\n",
       "      <td>[Hematology Blood hematocrit {27.2} %; Hematol...</td>\n",
       "      <td>[nan, nan, nan, Influenza Vaccine Quadrivalent...</td>\n",
       "      <td>[temp {98.8} heartrate {91.0} resprate {18.0} ...</td>\n",
       "      <td>[HISPANIC/LATINO M 48.0$temp {98.8} heartrate ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, nan...</td>\n",
       "      <td>[-3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,...</td>\n",
       "      <td>2159-04-05 10:13:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id                              demographics_in_visit  \\\n",
       "0    17223646  20023461  [WHITE F 69.0, WHITE F 69.0, WHITE F 69.0, WHI...   \n",
       "1    18371764  20046098  [WHITE M 56.0, WHITE M 56.0, WHITE M 56.0, WHI...   \n",
       "2    17416292  20051972  [UNKNOWN F 56.0, UNKNOWN F 56.0, UNKNOWN F 56....   \n",
       "3    15109929  20058401  [WHITE M 77.0, WHITE M 77.0, WHITE M 77.0, WHI...   \n",
       "4    14687005  20064641  [HISPANIC/LATINO M 48.0, HISPANIC/LATINO M 48....   \n",
       "\n",
       "                                  lab_tests_in_visit  \\\n",
       "0  [Hematology Blood hematocrit {37.7} %; Hematol...   \n",
       "1  [Hematology Blood hematocrit {40.1} %; Hematol...   \n",
       "2  [Hematology Blood hematocrit {27.5} %; Hematol...   \n",
       "3  [Hematology Blood hematocrit {24.0} %; Hematol...   \n",
       "4  [Hematology Blood hematocrit {27.2} %; Hematol...   \n",
       "\n",
       "                                medications_in_visit  \\\n",
       "0  [Sodium Chloride 0.9%  Flush {3} mL ; Heparin ...   \n",
       "1  [Sodium Chloride 0.9%  Flush {3} mL ; D5 1/2NS...   \n",
       "2  [ARIPiprazole {10} mg ; Docusate Sodium {100} ...   \n",
       "3  [nan, Sodium Chloride 0.9%  Flush {3} mL ; Ace...   \n",
       "4  [nan, nan, nan, Influenza Vaccine Quadrivalent...   \n",
       "\n",
       "                                     vitals_in_visit  \\\n",
       "0  [temp {97.7} heartrate {68.0} resprate {18.0} ...   \n",
       "1  [temp {100.3} heartrate {103.0} resprate {18.0...   \n",
       "2  [temp {} heartrate {68.0} resprate {22.0} o2sa...   \n",
       "3  [temp {97.9} heartrate {64.0} resprate {18.0} ...   \n",
       "4  [temp {98.8} heartrate {91.0} resprate {18.0} ...   \n",
       "\n",
       "                                       days_in_visit  \\\n",
       "0  [WHITE F 69.0$temp {97.7} heartrate {68.0} res...   \n",
       "1  [WHITE M 56.0$temp {100.3} heartrate {103.0} r...   \n",
       "2  [UNKNOWN F 56.0$temp {} heartrate {68.0} respr...   \n",
       "3  [WHITE M 77.0$temp {97.9} heartrate {64.0} res...   \n",
       "4  [HISPANIC/LATINO M 48.0$temp {98.8} heartrate ...   \n",
       "\n",
       "                                 aki_status_in_visit  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, nan...   \n",
       "1                                   [0, 0, nan, nan]   \n",
       "2                        [0, 0, 0, 1, 1, 1, 1, 1, 1]   \n",
       "3                          [0, 0, 0, 0, 0, nan, nan]   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, nan...   \n",
       "\n",
       "                                                days           admittime  \n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,... 2152-10-31 20:11:00  \n",
       "1                                       [0, 1, 2, 3] 2162-09-15 13:43:00  \n",
       "2                        [0, 1, 2, 3, 4, 5, 6, 7, 8] 2168-04-06 03:17:00  \n",
       "3                             [-1, 0, 1, 2, 3, 4, 5] 2157-11-11 01:29:00  \n",
       "4  [-3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,... 2159-04-05 10:13:00  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res = pid_train_df.merge(data_admissions[['hadm_id', 'admittime']])\n",
    "# res = pid_val_df.merge(data_admissions[['hadm_id', 'admittime']])\n",
    "res = pid_test_df.merge(data_admissions[['hadm_id', 'admittime']])\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = 13131863\n",
    "admittime = res[res.subject_id==patient].admittime.values[0]\n",
    "data_diagnoses[(data_diagnoses.subject_id==patient) & (data_diagnoses.admittime < admittime)].sort_values('admittime', ascending=False).icd_code.tolist()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 827 patients\n",
      "0 patients\n",
      "100 patients\n",
      "200 patients\n",
      "300 patients\n",
      "400 patients\n",
      "500 patients\n",
      "600 patients\n",
      "700 patients\n",
      "800 patients\n"
     ]
    }
   ],
   "source": [
    "# patient_previous_diag_dict = {}\n",
    "i=0\n",
    "print(f'Total {len(res.subject_id.unique())} patients')\n",
    "for patient in res.subject_id.unique():\n",
    "    if i%100==0:\n",
    "        print(f'{i} patients')\n",
    "    admittime = res[res.subject_id==patient].admittime.values[0]\n",
    "    prev_diags = data_diagnoses[(data_diagnoses.subject_id==patient) & (data_diagnoses.admittime < admittime) ].sort_values('admittime', ascending=False).icd_code.tolist()[:30]\n",
    "    patient_previous_diag_dict[patient] = prev_diags\n",
    "    i += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10504"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(patient_previous_diag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prev_diagnoses = pd.DataFrame.from_dict(patient_previous_diag_dict, orient='index').reset_index().rename(columns={'index':'hadm_id'}).fillna('PAD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prev_diagnoses['prev_diags'] = df_prev_diagnoses.drop(labels=['hadm_id'], axis=1).apply(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>prev_diags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16679562</td>\n",
       "      <td>F17210 E785 J111 K219 F1020 C61 N189 J441 N400...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10189736</td>\n",
       "      <td>K921 I7103 J9691 J90 I959 D62 Z87891 K269 D696...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19669999</td>\n",
       "      <td>42833 V4581 4280 73313 41400 2724 5770 57450 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12377638</td>\n",
       "      <td>PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18423382</td>\n",
       "      <td>3051 V1581 42789 44422 99674 6826 4019 44022 V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10499</th>\n",
       "      <td>18203312</td>\n",
       "      <td>PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10500</th>\n",
       "      <td>16688191</td>\n",
       "      <td>PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10501</th>\n",
       "      <td>14644623</td>\n",
       "      <td>PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10502</th>\n",
       "      <td>18011979</td>\n",
       "      <td>PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10503</th>\n",
       "      <td>17020543</td>\n",
       "      <td>PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10504 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        hadm_id                                         prev_diags\n",
       "0      16679562  F17210 E785 J111 K219 F1020 C61 N189 J441 N400...\n",
       "1      10189736  K921 I7103 J9691 J90 I959 D62 Z87891 K269 D696...\n",
       "2      19669999  42833 V4581 4280 73313 41400 2724 5770 57450 4...\n",
       "3      12377638  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...\n",
       "4      18423382  3051 V1581 42789 44422 99674 6826 4019 44022 V...\n",
       "...         ...                                                ...\n",
       "10499  18203312  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...\n",
       "10500  16688191  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...\n",
       "10501  14644623  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...\n",
       "10502  18011979  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...\n",
       "10503  17020543  PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PA...\n",
       "\n",
       "[10504 rows x 2 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prev_diagnoses = df_prev_diagnoses[['hadm_id', 'prev_diags']]\n",
    "df_prev_diagnoses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = {'demographics':5, 'lab_tests':400, 'vitals':200, 'medications':255}\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_length=400, pred_window=2, observing_window=2):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.max_length = max_length\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.make_matrices(idx)\n",
    "    \n",
    "    def tokenize(self, text, max_length):\n",
    "        \n",
    "        output = self.tokenizer.encode(text)\n",
    "        # padding and truncation\n",
    "        if len(output.ids) < max_length:\n",
    "            len_missing_token = max_length - len(output.ids)\n",
    "            padding_vec = [self.tokenizer.token_to_id('[PAD]') for _ in range(len_missing_token)]\n",
    "            token_output = [*output.ids, *padding_vec]\n",
    "        elif len(output.ids) > max_length:\n",
    "            token_output = output.ids[:max_length]\n",
    "        else:\n",
    "            token_output = output.ids\n",
    "        \n",
    "        return token_output\n",
    "\n",
    "    def make_matrices(self, idx):\n",
    "        day_info = self.df.days_in_visit.values[idx]\n",
    "        aki_status = self.df.aki_status_in_visit.values[idx]\n",
    "        days = self.df.days.values[idx]\n",
    "        # print(idx)\n",
    "\n",
    "        aki_happened = False\n",
    "        labels = []\n",
    "        day_info_list = []\n",
    "        label = None\n",
    "\n",
    "        for day in range(days[0], days[0]+ self.observing_window):\n",
    "            if day not in days:\n",
    "                labels.append(0)\n",
    "                day_info_list.append(self.tokenize('', self.max_length))\n",
    "            else:\n",
    "                i = days.index(day)\n",
    "                \n",
    "                if (day + self.pred_window) not in days:\n",
    "                    labels.append(0)\n",
    "                else:              \n",
    "                    if ((i + self.pred_window) >= len(aki_status)) or np.isnan(aki_status[i + self.pred_window]):\n",
    "                        labels.append(0)\n",
    "                    else:\n",
    "                        labels.append(aki_status[i + self.pred_window])\n",
    "\n",
    "                if str(day_info[i]) == 'nan':\n",
    "                    day_info_list.append(self.tokenize('[PAD]', self.max_length))\n",
    "                else:\n",
    "                    day_info_list.append(self.tokenize(day_info[i], self.max_length))\n",
    "\n",
    "        if sum(labels[-self.pred_window:]) > 0:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "\n",
    "        #make tensors\n",
    "        tensor_day = torch.tensor(day_info_list, dtype=torch.int64)\n",
    "        tensor_labels = torch.tensor(label, dtype=torch.float64)\n",
    "\n",
    "        return tensor_day, tensor_labels, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0,0,0,0,1]\n",
    "a[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(pid_train_df, tokenizer=tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_day, tensor_labels, idx = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2, 400])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_day.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 265,   44, 1122,  ...,  128,   27,  254],\n",
       "         [ 265,   44, 1122,  ...,  602,   88,  344]],\n",
       "\n",
       "        [[ 265,   37, 1151,  ...,    0,    0,    0],\n",
       "         [ 265,   37, 1151,  ...,    0,    0,    0]],\n",
       "\n",
       "        [[ 265,   44,  441,  ...,    0,    0,    0],\n",
       "         [ 265,   44,  441,  ...,    0,    0,    0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 265,   37,  441,  ...,    0,    0,    0],\n",
       "         [ 265,   37,  441,  ...,    0,    0,    0]],\n",
       "\n",
       "        [[ 265,   44,  907,  ...,   99,  170,   88],\n",
       "         [ 265,   44,  907,  ...,   15,   43,   27]],\n",
       "\n",
       "        [[ 265,   37, 1257,  ...,    0,    0,    0],\n",
       "         [ 265,   37, 1257,  ...,    0,    0,    0]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9459, 9040, 1514, 7487, 2421, 7426, 6304, 8595, 4970, 5357, 9396, 6063,\n",
       "        4573, 2308, 2358, 8064])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject_id                                                        10921932\n",
       "hadm_id                                                           21504239\n",
       "demographics_in_visit    [WHITE M 30.0, WHITE M 30.0, WHITE M 30.0, WHI...\n",
       "lab_tests_in_visit       [Hematology Blood hematocrit {36.4} %; Hematol...\n",
       "medications_in_visit     [0.9% Sodium Chloride {250} mL ; D5 1/2NS {100...\n",
       "vitals_in_visit                        [nan, nan, nan, nan, nan, nan, nan]\n",
       "days_in_visit            [WHITE M 30.0$nan$Hematology Blood hematocrit ...\n",
       "aki_status_in_visit                              [0, 0, 1, 0, 0, nan, nan]\n",
       "days                                                 [0, 1, 2, 3, 4, 5, 6]\n",
       "Name: 5140, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_train_df.iloc[1514]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHR_MODEL(nn.Module):\n",
    "    def __init__(self, max_length, vocab_size, device, pred_window=2, observing_window=3,  H=128, embedding_size=200):\n",
    "        super(EHR_MODEL, self).__init__()\n",
    "\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.H = H\n",
    "        self.max_length = max_length\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device\n",
    "\n",
    "        # self.embedding = pretrained_model\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "\n",
    "        self.lstm_day = nn.LSTM(input_size=embedding_size,\n",
    "                            hidden_size=self.H,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "\n",
    "        self.fc_day = nn.Linear(self.max_length * 2 * self.H, 2048)\n",
    "\n",
    "        # self.fc_1 = nn.Linear(self.observing_window * 2 * self.H, 2048)\n",
    "\n",
    "        self.lstm_adm = nn.LSTM(input_size=2048*self.observing_window,\n",
    "                            hidden_size=self.H,\n",
    "                            num_layers=2,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc_2 = nn.Linear(self.H*2, 1)\n",
    "\n",
    "    def forward(self, tensor_day):\n",
    "\n",
    "        batch_size = tensor_day.size()[0]\n",
    "\n",
    "        full_output = torch.tensor([]).to(device=self.device)\n",
    "\n",
    "        for d in range(self.observing_window):\n",
    "            # embedding layer applied to all tensors [16,400,200]\n",
    "            out_emb = self.embedding(tensor_day[:, d, :].squeeze(1))\n",
    "            # print('out_emb', out_emb.size())\n",
    "\n",
    "            # lstm layer applied to embedded tensors\n",
    "            output_lstm_day= self.fc_day(\\\n",
    "                                    self.lstm_day(out_emb)[0]\\\n",
    "                                        .reshape(batch_size, self.max_length * 2 * self.H))\n",
    "\n",
    "            # print('output_lstm_day', output_lstm_day.size())                   \n",
    "            # concatenate for all * days\n",
    "            full_output = torch.cat([full_output, output_lstm_day], dim=1) # [16, 768]\n",
    "\n",
    "        # print('full_output size: ', full_output.size(), '\\n')\n",
    "        # output = self.fc_1(full_output)\n",
    "        output, _ = self.lstm_adm(full_output)\n",
    "        # print('output after lstm_adm', output.size())\n",
    "        output = self.drop(output)\n",
    "        output = self.fc_2(output)\n",
    "        # print('output after fc_2', output.size())\n",
    "        output = torch.squeeze(output, 1)\n",
    "\n",
    "        # output = nn.Sigmoid()(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EHR_MODEL(400,vocab_size, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_day, tensor_labels = next(iter(train_loader))\n",
    "tensor_day = tensor_day.to(device)\n",
    "tensor_labels = tensor_labels.to(device)\n",
    "output = model(tensor_day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5238, 0.5113, 0.5122, 0.5097, 0.5113, 0.5141, 0.5060, 0.5147],\n",
       "       device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_PATH = os.getcwd()\n",
    "PKL_PATH = CURR_PATH+'/pickles/'\n",
    "DF_PATH = CURR_PATH +'/dataframes/'\n",
    "\n",
    "# loading the data\n",
    "with open(DF_PATH + 'pid_train_df_finetuning.pkl', 'rb') as f:\n",
    "    pid_train_df = pickle.load(f)\n",
    "\n",
    "with open(DF_PATH + 'pid_val_df_finetuning.pkl', 'rb') as f:\n",
    "    pid_val_df = pickle.load(f)\n",
    "\n",
    "with open(DF_PATH + 'pid_test_df_finetuning.pkl', 'rb') as f:\n",
    "    pid_test_df = pickle.load(f)\n",
    "\n",
    "tokenizer = Tokenizer.from_file(CURR_PATH + '/tokenizer.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Functions\n",
    "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(load_path, model, optimizer, device):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "def load_checkpoint(load_path, model, optimizer, device):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device, threshold=0.5, log_res=True):\n",
    "    model = model.to(device)\n",
    "    stacked_labels = torch.tensor([]).to(device)\n",
    "    stacked_preds = torch.tensor([]).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    step = 1\n",
    "    with torch.no_grad():\n",
    "        for tensor_day, tensor_labels in test_loader:\n",
    "            # print(f'Step {step}/{len(test_loader)}' )\n",
    "            labels = tensor_labels.to(device)\n",
    "            day_info = tensor_day.to(device)\n",
    "\n",
    "            output = model(day_info)\n",
    "            output = nn.Sigmoid()(output)\n",
    "            output = (output > threshold).int()\n",
    "\n",
    "            # stacking labels and predictions\n",
    "            stacked_labels = torch.cat([stacked_labels, labels], dim=0, )\n",
    "            stacked_preds = torch.cat([stacked_preds, output], dim=0, )\n",
    "\n",
    "            step += 1\n",
    "\n",
    "    # calculate accuracy\n",
    "    acc = torch.round(torch.sum(stacked_labels==stacked_preds) / len(stacked_labels), decimals=2)\n",
    "    # transfer to device\n",
    "    stacked_labels = stacked_labels.cpu().detach().numpy()\n",
    "    stacked_preds = stacked_preds.cpu().detach().numpy()\n",
    "    # get classification metrics for all samples in the test set\n",
    "    classification_report_res = classification_report(stacked_labels, stacked_preds, zero_division=0, output_dict=True)\n",
    "    print(classification_report(stacked_labels, stacked_preds, zero_division=0, output_dict=False))\n",
    "    if log_res:\n",
    "        wandb.log(classification_report_res)\n",
    "\n",
    "    return classification_report_res, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MyDataset(pid_test_df.sample(frac=0.1), tokenizer=tokenizer, max_length=400)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EHR_MODEL(max_length=400, vocab_size=vocab_size, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_res = classification_report(stacked_labels, stacked_preds, zero_division=0, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['0.0', '1.0', 'accuracy', 'macro avg', 'weighted avg'])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report_res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.96      0.70        47\n",
      "         1.0       0.33      0.03      0.05        37\n",
      "\n",
      "    accuracy                           0.55        84\n",
      "   macro avg       0.44      0.49      0.38        84\n",
      "weighted avg       0.46      0.55      0.42        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_report_res, acc = evaluate(model, test_loader, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('0.0', {'precision': 0.5714285714285714, 'recall': 0.9361702127659575, 'f1-score': 0.7096774193548386, 'support': 47}), ('1.0', {'precision': 0.5714285714285714, 'recall': 0.10810810810810811, 'f1-score': 0.18181818181818182, 'support': 37}), ('accuracy', 0.5714285714285714), ('macro avg', {'precision': 0.5714285714285714, 'recall': 0.5221391604370328, 'f1-score': 0.4457478005865102, 'support': 84}), ('weighted avg', {'precision': 0.5714285714285714, 'recall': 0.5714285714285714, 'f1-score': 0.47716799329702553, 'support': 84})])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report_res.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, \n",
    "          optimizer,\n",
    "          train_loader,\n",
    "          valid_loader,\n",
    "          file_path,\n",
    "          device='cpu',\n",
    "          num_epochs=5,\n",
    "          criterion = nn.BCELoss(),\n",
    "          pos_weight = torch.tensor([]),\n",
    "          best_valid_loss = float(\"Inf\"),\n",
    "          dimension=128,\n",
    "          epoch_patience=15,\n",
    "          threshold=0.5,\n",
    "          scheduler=None):\n",
    "    \n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    valid_running_acc = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    train_acc_list = []\n",
    "    valid_acc_list = []\n",
    "    global_steps_list = []\n",
    "    stop_training = 0\n",
    "\n",
    "    sigmoid_fn = nn.Sigmoid()\n",
    "\n",
    "    if criterion == 'BCEWithLogitsLoss':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        criterion.pos_weight = pos_weight.to(device)\n",
    "        use_sigmoid=False\n",
    "    else:\n",
    "        criterion = nn.BCELoss()\n",
    "        use_sigmoid = True\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):  \n",
    "        stacked_labels = torch.tensor([]).to(device)\n",
    "        stacked_preds = torch.tensor([]).to(device)\n",
    "\n",
    "        model.train()\n",
    "        for tensor_day, tensor_labels in train_loader:\n",
    "            # transferring everything to GPU\n",
    "            labels = tensor_labels.to(device)\n",
    "            tensor_day = tensor_day.to(device)\n",
    "\n",
    "            output = model(tensor_day)\n",
    "\n",
    "            if use_sigmoid:\n",
    "                loss = criterion(sigmoid_fn(output), labels.type(torch.float32))\n",
    "            else:\n",
    "                loss = criterion(output, labels.type(torch.float32))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            output_pred = sigmoid_fn(output)\n",
    "            output_pred = (output_pred > threshold).int()\n",
    "\n",
    "            stacked_labels = torch.cat([stacked_labels, labels], dim=0)\n",
    "            stacked_preds = torch.cat([stacked_preds, output_pred], dim=0)\n",
    "            \n",
    "            global_step += 1\n",
    "\n",
    "            wandb.log({'step_train_loss': loss.item(), 'global_step': global_step})\n",
    "            \n",
    "        # calculate accuracy\n",
    "        epoch_train_accuracy = torch.round(torch.sum(stacked_labels==stacked_preds) / len(stacked_labels), decimals=2)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            print(f'Learning rate is {get_lr(optimizer)}')\n",
    "\n",
    "        model.eval()\n",
    "        stacked_labels = torch.tensor([]).to(device)\n",
    "        stacked_preds = torch.tensor([]).to(device)\n",
    "        with torch.no_grad():\n",
    "            # validation loop\n",
    "            for tensor_day, tensor_labels in valid_loader:\n",
    "                labels = tensor_labels.to(device)\n",
    "                tensor_day = tensor_day.to(device)\n",
    "                \n",
    "                output = model(tensor_day)\n",
    "\n",
    "                if use_sigmoid:\n",
    "                    loss = criterion(sigmoid_fn(output), labels.type(torch.float32))\n",
    "                else:\n",
    "                    loss = criterion(output, labels.type(torch.float32))\n",
    "\n",
    "                valid_running_loss += loss.item()\n",
    "\n",
    "                output_pred = sigmoid_fn(output)\n",
    "                output_pred = (output_pred > threshold).int()\n",
    "\n",
    "                # stacking labels and predictions\n",
    "                stacked_labels = torch.cat([stacked_labels, labels], dim=0)\n",
    "                stacked_preds = torch.cat([stacked_preds, output_pred], dim=0)\n",
    "\n",
    "        # transfer to device\n",
    "        stacked_labels = stacked_labels.cpu().detach().numpy()\n",
    "        stacked_preds = stacked_preds.cpu().detach().numpy()\n",
    "        # calculate accuracy\n",
    "        epoch_val_accuracy = np.round(np.sum(stacked_labels==stacked_preds) / len(stacked_labels), 2)\n",
    "        # get classification metrics for all samples in the test set\n",
    "        classification_report_res = classification_report(stacked_labels, stacked_preds, zero_division=0, output_dict=True)\n",
    "        classification_report_res.update({'epoch':epoch+1})\n",
    "\n",
    "        # log the evaluation metrics \n",
    "        for key, value in classification_report_res.items():\n",
    "            wandb.log({key:value, 'epoch':epoch+1})\n",
    "\n",
    "        # valid loss\n",
    "        epoch_average_train_loss = running_loss / len(train_loader)  \n",
    "        epoch_average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "\n",
    "        train_loss_list.append(epoch_average_train_loss)\n",
    "        valid_loss_list.append(epoch_average_valid_loss)\n",
    "        train_acc_list.append(epoch_train_accuracy)\n",
    "        valid_acc_list.append(epoch_val_accuracy)\n",
    "\n",
    "\n",
    "        global_steps_list.append(global_step)\n",
    "        wandb.log({'epoch_average_train_loss': epoch_average_train_loss,\n",
    "                    'epoch_average_valid_loss': epoch_average_valid_loss,\n",
    "                    'epoch_val_accuracy': epoch_val_accuracy, \n",
    "                    'epoch_train_accuracy': epoch_train_accuracy,\n",
    "                    'epoch': epoch+1})\n",
    "\n",
    "        # resetting running values\n",
    "        running_loss = 0.0                \n",
    "        valid_running_loss = 0.0\n",
    "        \n",
    "        \n",
    "        # print progress\n",
    "        print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}, Valid accuracy: {:.4f}'\n",
    "                .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                        epoch_average_train_loss, epoch_average_valid_loss, epoch_val_accuracy))    \n",
    "\n",
    "        # checkpoint\n",
    "        if best_valid_loss > epoch_average_valid_loss:\n",
    "            best_valid_loss = epoch_average_valid_loss\n",
    "            save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)\n",
    "            stop_training = 0\n",
    "        else:\n",
    "            stop_training +=1\n",
    "        \n",
    "        if stop_training == epoch_patience:\n",
    "            break\n",
    "\n",
    "\n",
    "    # save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Finished Training!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(saving_folder_name=None, criterion='BCELoss', small_dataset=False,\\\n",
    "     use_gpu=True, project_name='test', pred_window=2, observing_window=3, BATCH_SIZE=128, LR=0.0001,\\\n",
    "         min_frequency=1, hidden_size=128, num_epochs=50, wandb_mode='online', PRETRAINED_PATH=None, run_id=None):\n",
    "    # define the device\n",
    "    if use_gpu:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    else:\n",
    "        device='cpu'\n",
    "\n",
    "    #paths\n",
    "    CURR_PATH = os.getcwd()\n",
    "    PKL_PATH = CURR_PATH+'/pickles/'\n",
    "    DF_PATH = CURR_PATH +'/dataframes/'\n",
    "    TXT_DIR_TRAIN = CURR_PATH + '/txt_files/train'\n",
    "    destination_folder = CURR_PATH + '/training/'\n",
    "\n",
    "\n",
    "    # Training the tokenizer\n",
    "    if exists(CURR_PATH + '/tokenizer.json'):\n",
    "        tokenizer = Tokenizer.from_file(CURR_PATH + '/tokenizer.json')\n",
    "    else:\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\"], min_frequency=1)\n",
    "        files = glob.glob(TXT_DIR_TRAIN+'/*')\n",
    "        tokenizer.train(files, trainer)\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    # variables for classes\n",
    "    max_length = 400\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    embedding_size = 200\n",
    "    dimension = 128\n",
    "    \n",
    "\n",
    "    # loading the data\n",
    "    with open(DF_PATH + 'pid_train_df_finetuning.pkl', 'rb') as f:\n",
    "        pid_train_df = pickle.load(f)\n",
    "\n",
    "    with open(DF_PATH + 'pid_val_df_finetuning.pkl', 'rb') as f:\n",
    "        pid_val_df = pickle.load(f)\n",
    "\n",
    "    with open(DF_PATH + 'pid_test_df_finetuning.pkl', 'rb') as f:\n",
    "        pid_test_df = pickle.load(f)\n",
    "\n",
    "    # print('filtering admissions..')\n",
    "    # filter the admissions\n",
    "\n",
    "    pid_train_df = pid_train_df[pid_train_df.hadm_id.isin(train_admissions)]\n",
    "    pid_val_df = pid_val_df[pid_val_df.hadm_id.isin(val_admissions)]\n",
    "    pid_test_df = pid_test_df[pid_test_df.hadm_id.isin(test_admissions)]\n",
    "\n",
    "    if small_dataset: frac=0.1\n",
    "    else: frac=1\n",
    "\n",
    "    train_dataset = MyDataset(pid_train_df.sample(frac=frac), tokenizer=tokenizer, max_length=400)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    val_dataset = MyDataset(pid_val_df.sample(frac=frac), tokenizer=tokenizer, max_length=400)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    test_dataset = MyDataset(pid_test_df.sample(frac=frac), tokenizer=tokenizer, max_length=400)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "    #print shapes\n",
    "    tensor_day, tensor_labels = next(iter(train_loader))\n",
    "    print('\\n\\n DATA SHAPES: ')\n",
    "    print('train data shape: ', pid_train_df.shape)\n",
    "    print('val data shape: ', pid_val_df.shape)\n",
    "    print('test data shape: ', pid_test_df.shape)\n",
    "\n",
    "    print('tensor_day', tensor_day.shape)\n",
    "    print('tensor_labels', tensor_labels.shape)\n",
    "\n",
    "    # file_path = destination_folder + '/88087_no_weights-lr0.00005-adam'\n",
    "    \n",
    "    model = EHR_MODEL(vocab_size=vocab_size, max_length=max_length, device=device, pred_window=2, observing_window=3).to(device)\n",
    "\n",
    "    if PRETRAINED_PATH is not None:\n",
    "        model.load_state_dict(torch.load(PRETRAINED_PATH, map_location=device)['model_state_dict'])\n",
    "        model.embedding.weight.data = model.embedding.weight.data\n",
    "        print(f\"Pretrained model loaded from <=== {PRETRAINED_PATH}\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    train_params = {\n",
    "                    'model':model,\n",
    "                    'device':device,\n",
    "                    'optimizer':optimizer,\n",
    "                    'criterion':criterion,\n",
    "                    'train_loader':train_loader,\n",
    "                    'valid_loader':val_loader,\n",
    "                    'num_epochs':num_epochs, \n",
    "                    'file_path':destination_folder,\n",
    "                    'best_valid_loss':float(\"Inf\"),\n",
    "                    'dimension':128,\n",
    "                    'epoch_patience':5,\n",
    "                    'threshold':0.5,\n",
    "                    'scheduler':exp_lr_scheduler\n",
    "                }\n",
    "\n",
    "    # path for the model\n",
    "    if saving_folder_name is None:\n",
    "        saving_folder_name = 'FX_NO_PRETRAINING_' + str(len(train_loader)*BATCH_SIZE // 1000) + 'k_'  \\\n",
    "            + 'lr' + str(LR) + '_h'+ str(hidden_size) + '_pw' + str(pred_window) + '_ow' + str(observing_window)\n",
    "    \n",
    "    file_path = destination_folder + saving_folder_name\n",
    "    train_params['file_path'] = file_path\n",
    "\n",
    "    print(f'\\n\\nMODEL PATH: {file_path}')\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "    run_name = saving_folder_name\n",
    "\n",
    "    # wandb setup\n",
    "    os.environ['WANDB_API_KEY'] = '8e859a0fc58f296096842a367ca532717d3b4059'\n",
    "    \n",
    "    if run_id is None:    \n",
    "        run_id = wandb.util.generate_id()  \n",
    "        resume = 'allow' \n",
    "    else:\n",
    "        resume = 'must'\n",
    "        \n",
    "    args = {'optimizer':optimizer, 'criterion':'BCELoss', 'LR':LR, 'min_frequency':min_frequency, 'hidden_size':hidden_size, 'pred_window':pred_window, 'experiment':'no_pretraining'}\n",
    "    wandb.init(project=project_name, name=run_name, mode=wandb_mode, config=args, id=run_id, resume=resume)\n",
    "    print('Run id is: ', run_id)\n",
    "\n",
    "    # training\n",
    "    print('Training started..')\n",
    "    train(**train_params)\n",
    "\n",
    "    # testing\n",
    "    print('\\nTesting the model...')\n",
    "    load_checkpoint(file_path + '/model.pt', model, optimizer, device=device)\n",
    "    evaluate(model, test_loader, device, threshold=0.5, log_res=True)\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lerning rate finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "with open(DF_PATH + 'pid_train_df_finetuning.pkl', 'rb') as f:\n",
    "    pid_train_df = pickle.load(f)\n",
    "\n",
    "with open(DF_PATH + 'pid_val_df_finetuning.pkl', 'rb') as f:\n",
    "    pid_val_df = pickle.load(f)\n",
    "\n",
    "with open(DF_PATH + 'pid_test_df_finetuning.pkl', 'rb') as f:\n",
    "    pid_test_df = pickle.load(f)\n",
    "\n",
    "train_admissions = []\n",
    "for adm in pid_train_df.hadm_id.unique():   \n",
    "    if ({1,2,3,4}.issubset(set(pid_train_df[pid_train_df.hadm_id==adm].days.values[0])) or \\\n",
    "        {-1,0,1,2}.issubset(set(pid_train_df[pid_train_df.hadm_id==adm].days.values[0]))or \\\n",
    "            {0,1,2,3}.issubset(set(pid_train_df[pid_train_df.hadm_id==adm].days.values[0]))) and \\\n",
    "        (len(pid_train_df[pid_train_df.hadm_id==adm].days.values[0])>3) and\\\n",
    "            sum(pid_train_df[pid_train_df.hadm_id==adm].aki_status_in_visit.values[0][:2])==0:\n",
    "        train_admissions.append(adm)\n",
    "\n",
    "val_admissions = []\n",
    "for adm in pid_val_df.hadm_id.unique():   \n",
    "    if ({1,2,3,4}.issubset(set(pid_val_df[pid_val_df.hadm_id==adm].days.values[0])) or \\\n",
    "        {-1,0,1,2}.issubset(set(pid_val_df[pid_val_df.hadm_id==adm].days.values[0]))or \\\n",
    "            {0,1,2,3}.issubset(set(pid_val_df[pid_val_df.hadm_id==adm].days.values[0]))) and \\\n",
    "        (len(pid_val_df[pid_val_df.hadm_id==adm].days.values[0])>3) and\\\n",
    "            sum(pid_val_df[pid_val_df.hadm_id==adm].aki_status_in_visit.values[0][:2])==0:\n",
    "        val_admissions.append(adm)\n",
    "\n",
    "test_admissions = []\n",
    "for adm in pid_test_df.hadm_id.unique():   \n",
    "    if ({1,2,3,4}.issubset(set(pid_test_df[pid_test_df.hadm_id==adm].days.values[0])) or \\\n",
    "        {-1,0,1,2}.issubset(set(pid_test_df[pid_test_df.hadm_id==adm].days.values[0]))or \\\n",
    "            {0,1,2,3}.issubset(set(pid_test_df[pid_test_df.hadm_id==adm].days.values[0]))) and \\\n",
    "        (len(pid_test_df[pid_test_df.hadm_id==adm].days.values[0])>3) and\\\n",
    "            sum(pid_test_df[pid_test_df.hadm_id==adm].aki_status_in_visit.values[0][:2])==0:\n",
    "        test_admissions.append(adm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_train_df = pid_train_df[pid_train_df.hadm_id.isin(train_admissions)]\n",
    "pid_val_df = pid_val_df[pid_val_df.hadm_id.isin(val_admissions)]\n",
    "pid_test_df = pid_test_df[pid_test_df.hadm_id.isin(test_admissions)]\n",
    "\n",
    "\n",
    "train_dataset = MyDataset(pid_train_df.sample(frac=1), tokenizer=tokenizer, max_length=400)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "val_dataset = MyDataset(pid_val_df.sample(frac=1), tokenizer=tokenizer, max_length=400)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "CURR_PATH = os.getcwd()\n",
    "PKL_PATH = CURR_PATH+'/pickles/'\n",
    "DF_PATH = CURR_PATH +'/dataframes/'\n",
    "TXT_DIR_TRAIN = CURR_PATH + '/txt_files/train'\n",
    "destination_folder = CURR_PATH + '/training/'\n",
    "\n",
    "tokenizer = Tokenizer.from_file(CURR_PATH + '/tokenizer.json')\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EHR_MODEL(vocab_size=vocab_size, max_length=400, device=device, pred_window=2, observing_window=3)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-7, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Double but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21944/2368432330.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlr_finder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlr_finder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlr_finder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlr_finder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda_envs/aki_env/lib/python3.7/site-packages/torch_lr_finder/lr_finder.py\u001b[0m in \u001b[0;36mrange_test\u001b[0;34m(self, train_loader, val_loader, start_lr, end_lr, num_iter, step_mode, smooth_f, diverge_th, accumulation_steps, non_blocking_transfer)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mnon_blocking_transfer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking_transfer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             )\n\u001b[1;32m    322\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda_envs/aki_env/lib/python3.7/site-packages/torch_lr_finder/lr_finder.py\u001b[0m in \u001b[0;36m_train_batch\u001b[0;34m(self, train_iter, accumulation_steps, non_blocking_transfer)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;31m# Loss should be averaged in each step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda_envs/aki_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda_envs/aki_env/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda_envs/aki_env/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3063\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3065\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Double but expected Float"
     ]
    }
   ],
   "source": [
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "lr_finder.range_test(train_loader, val_loader=val_loader, end_lr=1, num_iter=100, step_mode=\"linear\")\n",
    "lr_finder.plot(log_lr=False)\n",
    "lr_finder.reset() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_PATH = '/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/FX_NO_PRETRAINING_10k_lr0.001_h128_pw2_ow3/model.pt'\n",
    "run_id = '3uu5b8vw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " DATA SHAPES: \n",
      "train data shape:  (9967, 9)\n",
      "val data shape:  (897, 9)\n",
      "test data shape:  (836, 9)\n",
      "tensor_day torch.Size([512, 3, 400])\n",
      "tensor_labels torch.Size([512])\n",
      "Pretrained model loaded from <=== /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/FX_NO_PRETRAINING_10k_lr0.001_h128_pw2_ow3/model.pt\n",
      "\n",
      "\n",
      "MODEL PATH: /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/FX_NO_PRETRAINING_10k_lr1e-05_h128_pw2_ow3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.18 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/wandb/run-20220622_134901-3uu5b8vw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href=\"https://wandb.ai/maslenkovas/Fixed_obs_window_model/runs/3uu5b8vw\" target=\"_blank\">FX_NO_PRETRAINING_10k_lr1e-05_h128_pw2_ow3</a></strong> to <a href=\"https://wandb.ai/maslenkovas/Fixed_obs_window_model\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run id is:  3uu5b8vw\n",
      "Training started..\n",
      "Learning rate is 1e-05\n",
      "Epoch [1/30], Step [20/600], Train Loss: 0.6470, Valid Loss: 0.6299, Valid accuracy: 0.6500\n",
      "Model saved to ==> /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/FX_NO_PRETRAINING_10k_lr1e-05_h128_pw2_ow3/model.pt\n",
      "Learning rate is 1e-05\n",
      "Epoch [2/30], Step [40/600], Train Loss: 0.6458, Valid Loss: 0.6298, Valid accuracy: 0.6400\n",
      "Model saved to ==> /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/FX_NO_PRETRAINING_10k_lr1e-05_h128_pw2_ow3/model.pt\n",
      "Learning rate is 1e-05\n",
      "Epoch [3/30], Step [60/600], Train Loss: 0.6453, Valid Loss: 0.6292, Valid accuracy: 0.6500\n",
      "Model saved to ==> /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/FX_NO_PRETRAINING_10k_lr1e-05_h128_pw2_ow3/model.pt\n",
      "Learning rate is 1e-05\n",
      "Epoch [4/30], Step [80/600], Train Loss: 0.6449, Valid Loss: 0.6326, Valid accuracy: 0.6400\n",
      "Learning rate is 1e-05\n",
      "Epoch [5/30], Step [100/600], Train Loss: 0.6441, Valid Loss: 0.6299, Valid accuracy: 0.6500\n",
      "Learning rate is 1e-05\n",
      "Epoch [6/30], Step [120/600], Train Loss: 0.6461, Valid Loss: 0.6282, Valid accuracy: 0.6400\n",
      "Model saved to ==> /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/FX_NO_PRETRAINING_10k_lr1e-05_h128_pw2_ow3/model.pt\n",
      "Learning rate is 1e-05\n",
      "Epoch [7/30], Step [140/600], Train Loss: 0.6444, Valid Loss: 0.6260, Valid accuracy: 0.6400\n",
      "Model saved to ==> /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/FX_NO_PRETRAINING_10k_lr1e-05_h128_pw2_ow3/model.pt\n",
      "Learning rate is 1e-05\n",
      "Epoch [8/30], Step [160/600], Train Loss: 0.6447, Valid Loss: 0.6293, Valid accuracy: 0.6500\n",
      "Learning rate is 1e-05\n",
      "Epoch [9/30], Step [180/600], Train Loss: 0.6437, Valid Loss: 0.6268, Valid accuracy: 0.6400\n",
      "Learning rate is 1.0000000000000002e-06\n",
      "Epoch [10/30], Step [200/600], Train Loss: 0.6442, Valid Loss: 0.6277, Valid accuracy: 0.6400\n",
      "Learning rate is 1.0000000000000002e-06\n",
      "Epoch [11/30], Step [220/600], Train Loss: 0.6438, Valid Loss: 0.6271, Valid accuracy: 0.6500\n",
      "Learning rate is 1.0000000000000002e-06\n",
      "Epoch [12/30], Step [240/600], Train Loss: 0.6445, Valid Loss: 0.6297, Valid accuracy: 0.6500\n",
      "Finished Training!\n",
      "\n",
      "Testing the model...\n",
      "Model loaded from <== /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/FX_NO_PRETRAINING_10k_lr1e-05_h128_pw2_ow3/model.pt\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.92      0.74       500\n",
      "         1.0       0.57      0.16      0.25       336\n",
      "\n",
      "    accuracy                           0.61       836\n",
      "   macro avg       0.59      0.54      0.50       836\n",
      "weighted avg       0.60      0.61      0.54       836\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fb501a294c4318b621d05bd4002811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>█▆▇▇█▇▇█▆▆▇▇▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>epoch_average_train_loss</td><td>█▆▄▄▂▆▂▃▁▂▁▃</td></tr><tr><td>epoch_average_valid_loss</td><td>▅▅▄█▅▃▁▄▂▃▂▅</td></tr><tr><td>epoch_train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch_val_accuracy</td><td>█▁█▁█▁▁█▁▁██</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>step_train_loss</td><td>▆▅▄█▃█▆▅▄▄▅▅▄▄▃▆▅▄▄▄▃▅▆▂▄▃▄▃▅▄▅▁▄▆▃▅▄▄▄▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.61364</td></tr><tr><td>epoch</td><td>12</td></tr><tr><td>epoch_average_train_loss</td><td>0.64451</td></tr><tr><td>epoch_average_valid_loss</td><td>0.62971</td></tr><tr><td>epoch_train_accuracy</td><td>0.63</td></tr><tr><td>epoch_val_accuracy</td><td>0.65</td></tr><tr><td>global_step</td><td>240</td></tr><tr><td>step_train_loss</td><td>0.66949</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">FX_NO_PRETRAINING_10k_lr1e-05_h128_pw2_ow3</strong>: <a href=\"https://wandb.ai/maslenkovas/Fixed_obs_window_model/runs/3uu5b8vw\" target=\"_blank\">https://wandb.ai/maslenkovas/Fixed_obs_window_model/runs/3uu5b8vw</a><br/>Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220622_134901-3uu5b8vw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main(saving_folder_name=None, criterion='BCELoss', small_dataset=False,\\\n",
    "     use_gpu=True, project_name='Fixed_obs_window_model', pred_window=2, BATCH_SIZE=800, LR=1e-05,\\\n",
    "         min_frequency=1, hidden_size=128, num_epochs=100, wandb_mode='online', PRETRAINED_PATH=PRETRAINED_PATH, run_id=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb5fdc613a097f32ece1e000bc60f17f3e33b8b9e39d8b8753abf3e45200e5d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
