{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from distutils.command.config import config\n",
    "import pickle5 as pickle\n",
    "\n",
    "# Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import os\n",
    "from os.path import exists\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score, \\\n",
    "    accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Tokenization\n",
    "from tokenizers import  Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Punctuation, Whitespace\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers import pre_tokenizers, normalizers\n",
    "from tokenizers.processors import BertProcessing\n",
    "import glob\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torch.utils.data import WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Functions\n",
    "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(load_path, model, optimizer, device):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURR_PATH = '/home/svetlanamaslenkova/Documents/AKI_deep/LSTM'\n",
    "DF_PATH = CURR_PATH +'/dataframes_2/'\n",
    "TXT_FILES_CODES_PATH = '/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/txt_files/icu_train'\n",
    "TXT_FILES_NAMES_PATH = '/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/txt_files/icu_train_titles'\n",
    "TOKENIZER_CODES_PATH = '/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/aki_prediction/tokenizer_icu_codes.json'\n",
    "TOKENIZER_NAMES_PATH = '/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/aki_prediction/tokenizer_icu_names.json'\n",
    "\n",
    "PKL_PATH = CURR_PATH+'/pickles_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/pickles_2/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PKL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DF_PATH + 'pid_train_dataset_icu.pkl', 'rb') as f:\n",
    "    pid_train_dataset_icu = pickle.load(f)\n",
    "\n",
    "with open(DF_PATH + 'pid_test_dataset_icu.pkl', 'rb') as f:\n",
    "    pid_test_dataset_icu = pickle.load(f)\n",
    "\n",
    "with open(DF_PATH + 'pid_val_dataset_icu.pkl', 'rb') as f:\n",
    "    pid_val_dataset_icu = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135.0\n",
      "45.0\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "test_data = pid_test_dataset_icu[pid_test_dataset_icu.icu_day_id==1].sort_values(['AKI_1', 'AKI_2', 'AKI_3'], ascending=False).drop_duplicates('stay_id')\n",
    "print(test_data.AKI_1.sum())\n",
    "print(test_data.AKI_2.sum())\n",
    "print(test_data.AKI_3.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11409\n",
      "1044.0\n",
      "339.0\n",
      "60.0\n"
     ]
    }
   ],
   "source": [
    "train_data = pid_train_dataset_icu[pid_train_dataset_icu.icu_day_id==1].sort_values(['AKI_1', 'AKI_2', 'AKI_3'], ascending=False).drop_duplicates('stay_id')\n",
    "print(len(train_data))\n",
    "print(train_data.AKI_1.sum())\n",
    "print(train_data.AKI_2.sum())\n",
    "print(train_data.AKI_3.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1425\n",
      "136.0\n",
      "40.0\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "val_data = pid_val_dataset_icu[pid_val_dataset_icu.icu_day_id==1].sort_values(['AKI_1', 'AKI_2', 'AKI_3'], ascending=False).drop_duplicates('stay_id')\n",
    "print(len(val_data))\n",
    "print(val_data.AKI_1.sum())\n",
    "print(val_data.AKI_2.sum())\n",
    "print(val_data.AKI_3.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>AKI_1</th>\n",
       "      <th>AKI_2</th>\n",
       "      <th>AKI_3</th>\n",
       "      <th>ANY_AKI</th>\n",
       "      <th>NO_AKI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18421337</td>\n",
       "      <td>22413411</td>\n",
       "      <td>30000484</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12168737</td>\n",
       "      <td>29283664</td>\n",
       "      <td>30001336</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34923988</td>\n",
       "      <td>59074770</td>\n",
       "      <td>30001471</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13269859</td>\n",
       "      <td>26734917</td>\n",
       "      <td>30002521</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15978672</td>\n",
       "      <td>26652960</td>\n",
       "      <td>30002654</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33801</th>\n",
       "      <td>38093900</td>\n",
       "      <td>48704302</td>\n",
       "      <td>39998622</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33802</th>\n",
       "      <td>15669140</td>\n",
       "      <td>29818488</td>\n",
       "      <td>39999172</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33803</th>\n",
       "      <td>13651601</td>\n",
       "      <td>22584645</td>\n",
       "      <td>39999230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33804</th>\n",
       "      <td>15403458</td>\n",
       "      <td>25335698</td>\n",
       "      <td>39999562</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33805</th>\n",
       "      <td>17840864</td>\n",
       "      <td>22695803</td>\n",
       "      <td>39999810</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33806 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id   hadm_id   stay_id  AKI_1  AKI_2  AKI_3  ANY_AKI  NO_AKI\n",
       "0        18421337  22413411  30000484      0      0      0        0       1\n",
       "1        12168737  29283664  30001336      0      0      0        0       1\n",
       "2        34923988  59074770  30001471      0      0      0        0       1\n",
       "3        13269859  26734917  30002521      0      0      0        0       1\n",
       "4        15978672  26652960  30002654      1      0      0        1       0\n",
       "...           ...       ...       ...    ...    ...    ...      ...     ...\n",
       "33801    38093900  48704302  39998622      0      1      0        1       0\n",
       "33802    15669140  29818488  39999172      0      0      0        0       1\n",
       "33803    13651601  22584645  39999230      0      1      0        1       0\n",
       "33804    15403458  25335698  39999562      0      0      0        0       1\n",
       "33805    17840864  22695803  39999810      0      0      0        0       1\n",
       "\n",
       "[33806 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(PKL_PATH + 'aki_stage_labels_second_day.pkl', 'rb') as f:\n",
    "    aki_stage_labels_second_day = pickle.load(f)\n",
    "aki_stage_labels_second_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PKL_PATH + 'aki_stage_labels.pkl', 'rb') as f:\n",
    "    aki_stage_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_test_dataset_icu[pid_test_dataset_icu.icu_day_id==1].AKI_1.values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_train_dataset_icu = pid_train_dataset_icu.replace(r'[{}]', '', regex=True)\n",
    "pid_test_dataset_icu = pid_test_dataset_icu.replace(r'[{}]', '', regex=True)\n",
    "pid_val_dataset_icu = pid_val_dataset_icu.replace(r'[{}]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ALT 21.0 ; AST 104.0 ; Absolute Count - Basos 0.01 ; Absolute Count - Eos 0.0 ; Absolute Count - Lymphs 0.24 ; Absolute Count - Monos 0.03 ; Absolute Count - Neuts 0.11 ; Alkaline Phosphate 54.0 ; Anion gap 16.0 ; BUN 49.0 ; Calcium non-ionized 9.3 ; Chloride (serum) 104.0 ; Creatinine (serum) 1.1 ; Differential-Atyps 0.0 ; Differential-Bands 0.0 ; Differential-Basos 2.0 ; Differential-Eos 1.0 ; Differential-Lymphs 61.0 ; Differential-Monos 8.0 ; Differential-Neuts 28.0 ; Direct Bilirubin 0.5 ; Fibrinogen 489.0 ; Glucose (serum) 117.0 ; Glucose finger stick (range 70-100) 163.0 ; HCO3 (serum) 24.0 ; Hematocrit (serum) 19.7 ; Hemoglobin 6.8 ; INR 1.2 ; LDH 1943.0 ; Magnesium 2.1 ; PH (dipstick) 7.5 ; PTT 25.3 ; Phosphorous 3.0 ; Platelet Count 16.0 ; Potassium (serum) 4.5 ; Prothrombin time 13.2 ; Sodium (serum) 140.0 ; Specific Gravity (urine) 1.015 ; Total Bilirubin 4.6 ; Uric Acid 6.3 ; WBC 0.6 '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_val_dataset_icu[pid_val_dataset_icu.demographics.str.len() > 20].drop_duplicates('stay_id').iloc[0].labs_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l220228 6.8; l220545 19.7; l220546 0.6; l220587 104.0; l220602 104.0; l220615 1.1; l220621 117.0; l220632 1943.0; l220635 2.1; l220644 21.0; l220645 140.0; l220734 7.5; l225612 54.0; l225624 49.0; l225625 9.3; l225637 0.0; l225638 0.0; l225639 2.0; l225640 1.0; l225641 61.0; l225642 8.0; l225643 28.0; l225651 0.5; l225664 163.0; l225677 3.0; l225690 4.6; l225695 6.3; l227073 16.0; l227442 4.5; l227443 24.0; l227457 16.0; l227465 13.2; l227466 25.3; l227467 1.2; l227468 489.0; l227471 1.015; l229357 0.11; l229358 0.24; l229359 0.03; l229360 0.0; l229361 0.01'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_val_dataset_icu[pid_val_dataset_icu.demographics.str.len() > 20].drop_duplicates('stay_id').iloc[0].labs_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11425\n",
      "1149.0\n",
      "366.0\n",
      "66.0\n"
     ]
    }
   ],
   "source": [
    "print(pid_train_dataset_icu.drop_duplicates('stay_id').shape[0])\n",
    "print(np.sum(pid_train_dataset_icu[(pid_train_dataset_icu.icu_day_id==1)].AKI_1.values))\n",
    "print(np.sum(pid_train_dataset_icu[(pid_train_dataset_icu.icu_day_id==1)].AKI_2.values))\n",
    "print(np.sum(pid_train_dataset_icu[(pid_train_dataset_icu.icu_day_id==1)].AKI_3.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1428\n",
      "144.0\n",
      "48.0\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "print(pid_test_dataset_icu.drop_duplicates('stay_id').shape[0])\n",
    "print(np.sum(pid_test_dataset_icu[(pid_test_dataset_icu.icu_day_id==1)].AKI_1.values))\n",
    "print(np.sum(pid_test_dataset_icu[(pid_test_dataset_icu.icu_day_id==1)].AKI_2.values))\n",
    "print(np.sum(pid_test_dataset_icu[(pid_test_dataset_icu.icu_day_id==1)].AKI_3.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1429\n",
      "143.0\n",
      "42.0\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "print(pid_val_dataset_icu.drop_duplicates('stay_id').shape[0])\n",
    "print(np.sum(pid_val_dataset_icu[(pid_val_dataset_icu.icu_day_id==1)].AKI_1.values))\n",
    "print(np.sum(pid_val_dataset_icu[(pid_val_dataset_icu.icu_day_id==1)].AKI_2.values))\n",
    "print(np.sum(pid_val_dataset_icu[(pid_val_dataset_icu.icu_day_id==1)].AKI_3.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11425"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pid_train_dataset_icu.drop_duplicates('stay_id').AKI_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>day_id</th>\n",
       "      <th>icu_12h_window_id</th>\n",
       "      <th>icu_day_id</th>\n",
       "      <th>demographics</th>\n",
       "      <th>previous_diags_codes</th>\n",
       "      <th>previous_diags_names</th>\n",
       "      <th>vitals_names</th>\n",
       "      <th>...</th>\n",
       "      <th>labs_codes</th>\n",
       "      <th>outputs_names</th>\n",
       "      <th>outputs_codes</th>\n",
       "      <th>medications_names</th>\n",
       "      <th>medications_codes</th>\n",
       "      <th>AKI_1</th>\n",
       "      <th>AKI_2</th>\n",
       "      <th>AKI_3</th>\n",
       "      <th>icu_12h_info_codes</th>\n",
       "      <th>icu_12h_info_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>10002155</td>\n",
       "      <td>28994087</td>\n",
       "      <td>31090461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WHITE F 82 Blood Pressure 99/51  BMI  21.4  He...</td>\n",
       "      <td>DE9331 D2761 DV4582 D2469 DE8498 D1628 DV4581 ...</td>\n",
       "      <td>Antineoplastic and immunosuppressive drugs cau...</td>\n",
       "      <td>HR 68.0  96.0 bpm; HR Alarm - High 120.0  120....</td>\n",
       "      <td>...</td>\n",
       "      <td>l220224 69.0; l220228 6.0; l220235 37.0; l2205...</td>\n",
       "      <td>Foley 1420.0 ml</td>\n",
       "      <td>o226559 1420.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>v220045 68.0  96.0; v220046 120.0  120.0; v220...</td>\n",
       "      <td>HR 68.0  96.0 bpm; HR Alarm - High 120.0  120....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject_id   hadm_id   stay_id  day_id  icu_12h_window_id  icu_day_id  \\\n",
       "33    10002155  28994087  31090461     0.0                0.0           0   \n",
       "\n",
       "                                         demographics  \\\n",
       "33  WHITE F 82 Blood Pressure 99/51  BMI  21.4  He...   \n",
       "\n",
       "                                 previous_diags_codes  \\\n",
       "33  DE9331 D2761 DV4582 D2469 DE8498 D1628 DV4581 ...   \n",
       "\n",
       "                                 previous_diags_names  \\\n",
       "33  Antineoplastic and immunosuppressive drugs cau...   \n",
       "\n",
       "                                         vitals_names  ...  \\\n",
       "33  HR 68.0  96.0 bpm; HR Alarm - High 120.0  120....  ...   \n",
       "\n",
       "                                           labs_codes    outputs_names  \\\n",
       "33  l220224 69.0; l220228 6.0; l220235 37.0; l2205...  Foley 1420.0 ml   \n",
       "\n",
       "     outputs_codes medications_names medications_codes AKI_1 AKI_2  AKI_3  \\\n",
       "33  o226559 1420.0                                       0.0   0.0    0.0   \n",
       "\n",
       "                                   icu_12h_info_codes  \\\n",
       "33  v220045 68.0  96.0; v220046 120.0  120.0; v220...   \n",
       "\n",
       "                                   icu_12h_info_names  \n",
       "33  HR 68.0  96.0 bpm; HR Alarm - High 120.0  120....  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_train_dataset_icu.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33                                       \n",
       "34                                m225975\n",
       "35                        m225911 m225975\n",
       "36                                m225975\n",
       "37                        m225911 m225975\n",
       "                       ...               \n",
       "476196                    m221794 m221906\n",
       "476197                    m221794 m221906\n",
       "476198    m221794 m225909 m225166 m221906\n",
       "476199                    m221794 m225166\n",
       "476200                            m221794\n",
       "Name: medications_codes, Length: 126464, dtype: object"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_train_dataset_icu.medications_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_frequency = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer is loaded from ==> /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/aki_prediction/tokenizer_icu_codes.json/tokenizer.json. Vocab size is 8791\n"
     ]
    }
   ],
   "source": [
    "# Training the tokenizer\n",
    "if exists(TOKENIZER_CODES_PATH):\n",
    "    tokenizer = Tokenizer.from_file(TOKENIZER_CODES_PATH)\n",
    "    print(f'Tokenizer is loaded from ==> {TOKENIZER_CODES_PATH}/tokenizer.json. Vocab size is {tokenizer.get_vocab_size()}')\n",
    "else:\n",
    "#     print('Training tokenizer...')\n",
    "#     os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "#     tokenizer = Tokenizer(BPE(unk_token=\"UNK\"))\n",
    "#     tokenizer.normalizer = normalizers.Sequence([Lowercase()])\n",
    "#     # tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=False), Punctuation( behavior = 'removed')])\n",
    "#     tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation(behavior = 'isolated')])\n",
    "\n",
    "#     trainer = BpeTrainer(special_tokens=[\"<s>\", \"</s>\", \"PAD\", \"UNK\", \"$\"], min_frequency=min_frequency)\n",
    "\n",
    "#     files = glob.glob(TXT_FILES_CODES_PATH+'/*')\n",
    "#     tokenizer.train(files, trainer)\n",
    "#     tokenizer.post_processor = BertProcessing(\n",
    "#             (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "#             (\"<s>\", tokenizer.token_to_id(\"<s>\")), \n",
    "#             )\n",
    "#     os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#     print(f'Vocab size is {tokenizer.get_vocab_size()}')\n",
    "    print('!')\n",
    "\n",
    "# tokenizer.save(TOKENIZER_CODES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l220228 13.0; l220545 39.3; l220546 4.3; l220602 107.0; l220615 0.8; l220621 127.0; l220635 1.6; l220645 142.0; l225624 26.0; l225625 9.4; l225664 126.0; l225677 3.6; l227073 12.0; l227442 4.8; l227443 23.0; l227457 273.0; l227465 11.0; l227466 29.7; l227467 1.0'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input = pid_train_dataset_icu.iloc[1,:].previous_diags_icd\n",
    "# input = pid_train_dataset_icu.iloc[1,:].demographics\n",
    "# input = pid_train_dataset_icu.iloc[1,:].previous_diags_titles\n",
    "# input = pid_train_dataset_icu.iloc[1,:].vitals\n",
    "# input = pid_train_dataset_icu.iloc[1,:].vitals_codes\n",
    "# input = pid_train_dataset_icu.iloc[9,:].labs\n",
    "input = pid_train_dataset_icu.iloc[9,:].labs_codes\n",
    "# input = pid_train_dataset_icu.iloc[1,:].outputs\n",
    "# input = pid_train_dataset_icu.iloc[1,:].medications_names\n",
    "# input = pid_train_dataset_icu.iloc[1,:].medications_codes\n",
    "# input = pid_train_dataset_icu.iloc[1,:].icu_12h_info\n",
    "# input = pid_train_dataset_icu.iloc[1,:].icu_12h_info_titles\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens: 96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'l220228 13 . 0 ; l220545 39 . 3 ; l220546 4 . 3 ; l220602 107 . 0 ; l220615 0 . 8 ; l220621 127 . 0 ; l220635 1 . 6 ; l220645 142 . 0 ; l225624 26 . 0 ; l225625 9 . 4 ; l225664 126 . 0 ; l225677 3 . 6 ; l227073 12 . 0 ; l227442 4 . 8 ; l227443 23 . 0 ; l227457 273 . 0 ; l227465 11 . 0 ; l227466 29 . 7 ; l227467 1 . 0'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.encode(input)\n",
    "print(f'number of tokens: {len(output.ids)}')\n",
    "tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject_id', 'hadm_id', 'stay_id', 'day_id', 'icu_12h_window_id',\n",
       "       'icu_day_id', 'demographics', 'previous_diags_icd',\n",
       "       'previous_diags_titles', 'vitals', 'vitals_codes', 'labs', 'labs_codes',\n",
       "       'outputs', 'outputs_codes', 'medications_names', 'medications_codes',\n",
       "       'AKI_1', 'AKI_2', 'AKI_3', 'icu_12h_info', 'icu_12h_info_titles'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_train_dataset_icu.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve distribution of lengths in tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = {'icu_12h_info':[], 'icu_12h_info_titles':[], 'demographics':[], 'previous_diags_icd':[], \\\n",
    "    'previous_diags_titles':[], 'vitals':[], 'vitals_codes':[], 'labs':[], 'labs_codes':[], 'outputs':[], 'outputs_codes':[], 'medications_names':[], 'medications_codes':[]}\n",
    "\n",
    "for _, row in pid_train_dataset_icu.iterrows():\n",
    "    for k,v in lengths.items():\n",
    "        # print(type(row[k]), row[k], row['hadm_id'])\n",
    "        length = len(tokenizer.encode(row[k]))\n",
    "        if length != 0:\n",
    "            lengths[k].append(length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2160x720 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ((ax1,ax2,ax3,ax4),(ax5,ax6,ax7,ax8)) = plt.subplots(2, 4, figsize=(30, 10))\n",
    "\n",
    "ax1.hist(lengths['icu_12h_info'], bins=100, alpha=0.75, label='icu_12h_info')\n",
    "ax1.hist(lengths['icu_12h_info_titles'], bins=250, alpha=0.5, label='icu_12h_info_names')\n",
    "ax1.legend()\n",
    "ax1.set_xlim([10,250])\n",
    "ax1.set_ylim([0,10000])\n",
    "ax1.set_xticks(np.arange(0,250,20))\n",
    "ax1.set_xlabel('n of tokens')\n",
    "ax1.set_title('Vitals, outputs and mediactions during 12h in ICU')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.hist(lengths['demographics'], bins=100, alpha=0.75, label='demographics')\n",
    "ax2.legend()\n",
    "ax2.set_xlim([10,50])\n",
    "ax2.set_ylim([0,10000])\n",
    "ax2.set_xticks(np.arange(0,50,5))\n",
    "ax2.set_xlabel('n of tokens')\n",
    "ax2.grid(True)\n",
    "\n",
    "ax3.hist(lengths['previous_diags_icd'], bins=100, alpha=0.75, label='previous_diags_icd')\n",
    "# ax3.hist(lengths['previous_diags_titles'], bins=250, alpha=0.75, label='previous_diags_titles')\n",
    "ax3.legend()\n",
    "ax3.set_xlim([5,70])\n",
    "ax3.set_ylim([0,7000])\n",
    "ax3.set_xlabel('n of tokens')\n",
    "ax3.grid(True)\n",
    "\n",
    "# ax4.hist(lengths['previous_diags_icd'], bins=100, alpha=0.75, label='previous_diags_icd')\n",
    "ax4.hist(lengths['previous_diags_titles'], bins=100, alpha=0.75, label='previous_diags_names')\n",
    "ax4.legend()\n",
    "ax4.set_xlim([10,1400])\n",
    "ax4.set_ylim([0,1000])\n",
    "ax4.set_xlabel('n of tokens')\n",
    "ax4.grid(True)\n",
    "\n",
    "ax7.hist(lengths['vitals_codes'], bins=100, alpha=0.75, label='vitals_codes')\n",
    "ax7.hist(lengths['vitals'], bins=100, alpha=0.5, label='vitals_names')\n",
    "ax7.legend()\n",
    "ax7.set_xlim([10,150])\n",
    "ax7.set_ylim([0,27000])\n",
    "ax7.set_xlabel('n of tokens')\n",
    "ax7.grid(True)\n",
    "\n",
    "ax5.hist(lengths['labs_codes'], bins=100, alpha=0.75, label='labs_codes')\n",
    "ax5.hist(lengths['labs'], bins=100, alpha=0.5, label='labs_names')\n",
    "ax5.legend()\n",
    "ax5.set_xlim([10,500])\n",
    "ax5.set_ylim([0,5000])\n",
    "ax5.set_xticks(np.arange(0,500,50))\n",
    "ax5.set_xlabel('n of tokens')\n",
    "ax5.grid(True)\n",
    "\n",
    "ax6.hist(lengths['outputs_codes'], bins=100, alpha=0.75, label='outputs_codes')\n",
    "ax6.hist(lengths['outputs'], bins=100, alpha=0.5, label='outputs_names')\n",
    "ax6.legend()\n",
    "ax6.set_xlim([10,60])\n",
    "ax6.set_ylim([0,7000])\n",
    "ax6.set_xticks(np.arange(0,60,5))\n",
    "ax6.set_xlabel('n of tokens')\n",
    "ax6.grid(True)\n",
    "\n",
    "ax8.hist(lengths['medications_codes'], bins=50, alpha=0.95, label='medications_codes')\n",
    "ax8.hist(lengths['medications_names'], bins=300, alpha=0.75, label='medications_names')\n",
    "ax8.legend()\n",
    "ax8.set_xlim([10,80])\n",
    "# ax8.set_ylim([0,2000])\n",
    "ax8.set_xticks(np.arange(0,80,5))\n",
    "ax8.set_xlabel('n of tokens')\n",
    "ax8.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max outputs_codes 46\n",
      "max medications codes 22\n"
     ]
    }
   ],
   "source": [
    "print('max outputs_codes', np.max(lengths['outputs_codes']))\n",
    "print('max medications codes', np.max(lengths['medications_codes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lengths_dict = {'demographics':30, 'previous_diags_codes':65,'previous_diags_names':1400, 'vitals_names':120, 'vitals_codes':100, 'labs_names':400, \\\n",
    "    'labs_codes':300, 'outputs_names':50, 'outputs_codes':40, 'medications_names':100, 'medications_codes':20,\\\n",
    "         'icu_12h_info_codes':120, 'icu_12h_info_names':220}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_length = max_lengths_dict['icu_12h_info_codes'] + max_lengths_dict['labs_codes'] + max_lengths_dict['demographics'] + max_lengths_dict['previous_diags_codes']\n",
    "final_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since icu_12h_info column already contains info about vitals, outputs and medications, we do not need length of them separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lengths_dict = {'demographics':30, 'previous_diags_codes':65,'labs_codes':240, 'icu_12h_info_codes':120}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lengths_dict = {'demographics':30, 'previous_diags_codes':65,'labs_codes':240, 'icu_12h_info_codes':120}\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path, tokenizer, labels_df=None, max_length_dict=max_lengths_dict, names=True, pred_window=2, observing_window=2):\n",
    "        # pred_window: number of 12h windows to predict AKI onset\n",
    "        # observing_window: number of 12h windows to observe\n",
    "        self.data_path = data_path\n",
    "        file_list = glob.glob(self.data_path + '*')\n",
    "        self.data = []\n",
    "        for sample in file_list:\n",
    "            self.data.append(sample)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.max_length_12h = max_lengths_dict['icu_12h_info_codes']\n",
    "        self.max_length_24h = max_lengths_dict['labs_codes']\n",
    "        self.max_length_demo = max_lengths_dict['demographics']\n",
    "        self.max_length_diags = max_lengths_dict['previous_diags_codes']\n",
    "        self.labels_df = labels_df[labels_df.icu_day_id==1]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.make_matrices(idx)\n",
    "    \n",
    "    def tokenize(self, text, max_length):\n",
    "        \n",
    "        try:\n",
    "            output = self.tokenizer.encode(text)\n",
    "        except:\n",
    "            print( type(text), text, max_length, self.stay_id)\n",
    "            output = self.tokenizer.encode(text[0])\n",
    "\n",
    "        # padding and truncation\n",
    "        if len(output.ids) < max_length:\n",
    "            len_missing_token = max_length - len(output.ids)\n",
    "            padding_vec = [self.tokenizer.token_to_id('PAD') for _ in range(len_missing_token)]\n",
    "            token_output = [*output.ids, *padding_vec]\n",
    "        elif len(output.ids) > max_length:\n",
    "            token_output = output.ids[:max_length]\n",
    "        else:\n",
    "            token_output = output.ids\n",
    "        \n",
    "        return token_output\n",
    "\n",
    "    def preproccess(self, df):\n",
    "        df['demographics'] = df['demographics'].fillna('')\n",
    "        df['previous_diags_codes'] = df['previous_diags_codes'].fillna('')\n",
    "        df['previous_diags_names'] = df['previous_diags_names'].fillna('')\n",
    "        df['vitals_names'] = df['vitals_names'].fillna('')\n",
    "        df['vitals_codes'] = df['vitals_codes'].fillna('')\n",
    "        df['labs_names'] = df['labs_names'].fillna('')\n",
    "        df['labs_codes'] = df['labs_codes'].fillna('')\n",
    "        df['outputs_names'] = df['outputs_names'].fillna('')\n",
    "        df['outputs_codes'] = df['outputs_codes'].fillna('')\n",
    "        df['medications_names'] = df['medications_names'].fillna('')\n",
    "        df['medications_codes'] = df['medications_codes'].fillna('')\n",
    "\n",
    "        df['AKI_1'] = df['AKI_1'].fillna(0)\n",
    "        df['AKI_2'] = df['AKI_2'].fillna(0)\n",
    "        df['AKI_3'] = df['AKI_3'].fillna(0)\n",
    "\n",
    "        df = df[(df.icu_12h_window_id.isin(np.arange(self.min_wind, self.min_wind + self.observing_window +  self.pred_window))) | (df.icu_day_id.isin(np.arange(self.min_day, self.observing_window//2 +  self.pred_window//2)))]\n",
    "        return df\n",
    "\n",
    "    def make_matrices(self, idx):\n",
    "        # load csv file\n",
    "        sample_path = self.data[idx]\n",
    "        df = pd.read_csv(sample_path)\n",
    "        # print('Loaded from ', sample_path)\n",
    "        \n",
    "        windows_12h = df.icu_12h_window_id.values\n",
    "        days = df.icu_day_id.values\n",
    "        self.min_wind = int( np.max([np.min(windows_12h[~np.isnan(windows_12h)]),0] ) )       \n",
    "        self.min_day = int( np.max( [np.min(days[~np.isnan(days)]), 0] ))  \n",
    "        self.df = self.preproccess(df)\n",
    "        self.stay_id = self.df.stay_id.values[0]  \n",
    "        # print(stay_id)\n",
    "        sort = np.argsort(self.df.icu_12h_window_id.values)\n",
    "        windows_12h = self.df.icu_12h_window_id.values[sort]\n",
    "        days = self.df.icu_day_id.values[sort]\n",
    "\n",
    "        info_12h = self.df.icu_12h_info_codes.values[sort]\n",
    "        info_24h_labs = self.df.labs_codes.values[sort]\n",
    "        info_demo = self.df.demographics.values[0]\n",
    "        info_diagnoses = self.df.previous_diags_codes.values[0]\n",
    "\n",
    "        AKI_1_status = self.df.AKI_1.values[sort]\n",
    "        AKI_2_status = self.df.AKI_2.values[sort]\n",
    "        AKI_3_status = self.df.AKI_3.values[sort]\n",
    "\n",
    "        AKI_1_labels_l = []\n",
    "        AKI_2_labels_l = []\n",
    "        AKI_3_labels_l = []\n",
    "        info_12h_list = []\n",
    "        info_24h_list = []\n",
    "        used_day_id_l = []\n",
    "        used_wind_id_l = []\n",
    "\n",
    "        wind_12h_pairs = [(i, i+1) for i in range(0, 2*(self.min_day + self.observing_window//2 +  self.pred_window//2), 2)]\n",
    "\n",
    "        for day in range(self.min_day, self.min_day + self.observing_window//2 +  self.pred_window//2):\n",
    "            for wind in wind_12h_pairs[day]:\n",
    "                if wind not in windows_12h:\n",
    "                    if day not in days:\n",
    "                    # print('not in days')\n",
    "                        AKI_1_labels_l.append(0)\n",
    "                        AKI_2_labels_l.append(0)\n",
    "                        AKI_3_labels_l.append(0)\n",
    "                        info_12h_list.append( self.tokenize('',  self.max_length_12h))\n",
    "                        if day not in used_day_id_l:\n",
    "                            info_24h_list.append( self.tokenize('',  self.max_length_24h))\n",
    "                            used_day_id_l.append(day)\n",
    "                    else:\n",
    "                        AKI_1_labels_l.append(self.df[self.df.icu_day_id==day].AKI_1.values[0])\n",
    "                        AKI_2_labels_l.append(self.df[self.df.icu_day_id==day].AKI_2.values[0])\n",
    "                        AKI_3_labels_l.append(self.df[self.df.icu_day_id==day].AKI_3.values[0])\n",
    "                        info_12h_list.append(self.tokenize(self.df[self.df.icu_day_id==day].icu_12h_info_codes.values[0],  self.max_length_12h))\n",
    "                        if day not in used_day_id_l:\n",
    "                            info_24h_list.append( self.tokenize(self.df[self.df.icu_day_id==day].labs_codes.values[0],  self.max_length_24h))\n",
    "                            used_day_id_l.append(day)\n",
    "                else:\n",
    "                    i = list(windows_12h).index(wind)\n",
    "\n",
    "                    AKI_1_labels_l.append(AKI_1_status[i])\n",
    "                    AKI_2_labels_l.append(AKI_2_status[i])\n",
    "                    AKI_3_labels_l.append(AKI_3_status[i])\n",
    "                    info_12h_list.append(self.tokenize(info_12h[i], self.max_length_12h))\n",
    "                    if day not in used_day_id_l:\n",
    "                        info_24h_list.append(self.tokenize(info_24h_labs[i], self.max_length_24h))\n",
    "                        used_day_id_l.append(day)\n",
    "                used_wind_id_l.append(wind)\n",
    "\n",
    "        demographics = self.tokenize(info_demo, self.max_length_demo)\n",
    "        diagnoses = self.tokenize(info_diagnoses, self.max_length_diags)\n",
    "\n",
    "        if self.labels_df is None:\n",
    "            # making 24h labels from 12h\n",
    "            AKI_1_labels = [int(bool(np.sum(AKI_1_labels_l[i:i+2]))) for i in np.arange(0, self.observing_window + self.pred_window, 2)]\n",
    "            AKI_2_labels = [int(bool(np.sum(AKI_2_labels_l[i:i+2]))) for i in np.arange(0, self.observing_window + self.pred_window, 2)]\n",
    "            AKI_3_labels = [int(bool(np.sum(AKI_3_labels_l[i:i+2]))) for i in np.arange(0, self.observing_window + self.pred_window, 2)]\n",
    "            \n",
    "            tensor_labels = torch.tensor([*AKI_1_labels[self.observing_window//2:self.observing_window//2 + self.pred_window//2],\\\n",
    "                                        *AKI_2_labels[self.observing_window//2:self.observing_window//2 + self.pred_window//2],\\\n",
    "                                        *AKI_3_labels[self.observing_window//2:self.observing_window//2 + self.pred_window//2] ]\\\n",
    "                                            , dtype=torch.float64)\n",
    "        else:\n",
    "            AKI_1_label = (np.sum(self.labels_df[self.labels_df.stay_id==self.stay_id].AKI_1.values) > 0).astype(int)\n",
    "            AKI_2_label = (np.sum(self.labels_df[self.labels_df.stay_id==self.stay_id].AKI_2.values) > 0).astype(int)\n",
    "            AKI_3_label = (np.sum(self.labels_df[self.labels_df.stay_id==self.stay_id].AKI_3.values) > 0).astype(int)\n",
    "            NO_AKI_label = (np.sum(self.labels_df[self.labels_df.stay_id==self.stay_id].NO_AKI.values) > 0).astype(int)\n",
    "            tensor_labels = torch.tensor([AKI_1_label, AKI_2_label, AKI_3_label, NO_AKI_label])\n",
    "\n",
    "        #make tensors\n",
    "        tensor_12h_info = torch.tensor(info_12h_list[:self.observing_window], dtype=torch.int64)\n",
    "        tensor_24h_labs = torch.tensor(info_24h_list[:self.observing_window//2], dtype=torch.int64)\n",
    "        tensor_diagnoses = torch.tensor(diagnoses, dtype=torch.int64)\n",
    "        tensor_demographics = torch.tensor(demographics, dtype=torch.int64)\n",
    "    \n",
    "\n",
    "        return tensor_12h_info, tensor_24h_labs, tensor_diagnoses, tensor_demographics, tensor_labels, {'stay_id':self.stay_id, 'day_id':used_day_id_l, 'wind_id':used_wind_id_l}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path,  tokenizer, labels_df=None, max_length=300, names=True, pred_window=2, observing_window=2):\n",
    "        # pred_window: number of 12h windows to predict AKI onset\n",
    "        # observing_window: number of 12h windows to observe\n",
    "        self.data_path = data_path\n",
    "        file_list = glob.glob(self.data_path + '*')\n",
    "        self.data = []\n",
    "        for sample in file_list:\n",
    "            self.data.append(sample)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.max_length = max_length\n",
    "        if labels_df is not None:\n",
    "            self.labels_df = labels_df[labels_df.icu_day_id==1]\n",
    "        else:\n",
    "            self.labels_df = None\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.make_matrices(idx)\n",
    "    \n",
    "    def tokenize(self, text, max_length):\n",
    "        \n",
    "        try:\n",
    "            output = self.tokenizer.encode(text)\n",
    "        except:\n",
    "            print( type(text), text, max_length, self.stay_id)\n",
    "            output = self.tokenizer.encode(text[0])\n",
    "\n",
    "        # padding and truncation\n",
    "        if len(output.ids) < max_length:\n",
    "            len_missing_token = max_length - len(output.ids)\n",
    "            padding_vec = [self.tokenizer.token_to_id('PAD') for _ in range(len_missing_token)]\n",
    "            token_output = [*output.ids, *padding_vec]\n",
    "        elif len(output.ids) > max_length:\n",
    "            token_output = output.ids[:max_length]\n",
    "        else:\n",
    "            token_output = output.ids\n",
    "        \n",
    "        return token_output\n",
    "\n",
    "    def preproccess(self, df):\n",
    "        df['demographics'] = df['demographics'].fillna('')\n",
    "        df['previous_diags_codes'] = df['previous_diags_codes'].fillna('')\n",
    "        df['previous_diags_names'] = df['previous_diags_names'].fillna('')\n",
    "        df['vitals_names'] = df['vitals_names'].fillna('')\n",
    "        df['vitals_codes'] = df['vitals_codes'].fillna('')\n",
    "        df['labs_names'] = df['labs_names'].fillna('')\n",
    "        df['labs_codes'] = df['labs_codes'].fillna('')\n",
    "        df['outputs_names'] = df['outputs_names'].fillna('')\n",
    "        df['outputs_codes'] = df['outputs_codes'].fillna('')\n",
    "        df['medications_names'] = df['medications_names'].fillna('')\n",
    "        df['medications_codes'] = df['medications_codes'].fillna('')\n",
    "        df = df.replace(r';', '', regex=True)\n",
    "        df['AKI_1'] = df['AKI_1'].fillna(0)\n",
    "        df['AKI_2'] = df['AKI_2'].fillna(0)\n",
    "        df['AKI_3'] = df['AKI_3'].fillna(0)\n",
    "\n",
    "        df = df[(df.icu_12h_window_id.isin(np.arange(self.min_wind, self.min_wind + self.observing_window +  self.pred_window))) | (df.icu_day_id.isin(np.arange(self.min_day, self.observing_window//2 +  self.pred_window//2)))]\n",
    "        return df\n",
    "\n",
    "    def make_matrices(self, idx):\n",
    "        # load csv file\n",
    "        sample_path = self.data[idx]\n",
    "        df = pd.read_csv(sample_path)\n",
    "        # print('Loaded from ', sample_path)\n",
    "        \n",
    "        windows_12h = df.icu_12h_window_id.values\n",
    "        days = df.icu_day_id.values\n",
    "        self.min_wind = int( np.max([np.min(windows_12h[~np.isnan(windows_12h)]),0] ) )       \n",
    "        self.min_day = int( np.max( [np.min(days[~np.isnan(days)]), 0] ))  \n",
    "        self.df = self.preproccess(df)\n",
    "        self.stay_id = self.df.stay_id.values[0]  \n",
    "        # print(stay_id)\n",
    "        sort = np.argsort(self.df.icu_12h_window_id.values)\n",
    "        windows_12h = self.df.icu_12h_window_id.values[sort]\n",
    "        days = self.df.icu_day_id.values[sort]\n",
    "\n",
    "        info_12h = self.df.icu_12h_info_codes.values[sort]\n",
    "        info_24h_labs = self.df.labs_codes.values[sort]\n",
    "        info_demo = self.df.demographics.values[0]\n",
    "        info_diagnoses = self.df.previous_diags_codes.values[0]\n",
    "\n",
    "        AKI_1_status = self.df.AKI_1.values[sort]\n",
    "        AKI_2_status = self.df.AKI_2.values[sort]\n",
    "        AKI_3_status = self.df.AKI_3.values[sort]\n",
    "\n",
    "        AKI_1_labels_l = []\n",
    "        AKI_2_labels_l = []\n",
    "        AKI_3_labels_l = []\n",
    "        info_12h_list = []\n",
    "        info_24h_list = []\n",
    "        day_l = []\n",
    "        info_l = []\n",
    "        used_day_id_l = []\n",
    "        used_wind_id_l = []\n",
    "\n",
    "        wind_12h_pairs = [(i, i+1) for i in range(0, 2*(self.min_day + self.observing_window//2 +  self.pred_window//2), 2)]\n",
    "\n",
    "        for day in range(self.min_day, self.min_day + self.observing_window//2 +  self.pred_window//2):\n",
    "            for wind in wind_12h_pairs[day]:\n",
    "                if wind not in windows_12h:\n",
    "                    if day not in days:\n",
    "                    # print('not in days')\n",
    "                        AKI_1_labels_l.append(0)\n",
    "                        AKI_2_labels_l.append(0)\n",
    "                        AKI_3_labels_l.append(0)\n",
    "                        day_l.append('')\n",
    "                        if day not in used_day_id_l:\n",
    "                            day_l.append('')\n",
    "                            used_day_id_l.append(day)\n",
    "                    else:\n",
    "                        AKI_1_labels_l.append(self.df[self.df.icu_day_id==day].AKI_1.values[0])\n",
    "                        AKI_2_labels_l.append(self.df[self.df.icu_day_id==day].AKI_2.values[0])\n",
    "                        AKI_3_labels_l.append(self.df[self.df.icu_day_id==day].AKI_3.values[0])\n",
    "                        day_l.append(self.df[self.df.icu_day_id==day].icu_12h_info_codes.values[0])\n",
    "                        if day not in used_day_id_l:\n",
    "                            day_l.append(self.df[self.df.icu_day_id==day].labs_codes.values[0])\n",
    "                            used_day_id_l.append(day)\n",
    "                else:\n",
    "                    i = list(windows_12h).index(wind)\n",
    "\n",
    "                    AKI_1_labels_l.append(AKI_1_status[i])\n",
    "                    AKI_2_labels_l.append(AKI_2_status[i])\n",
    "                    AKI_3_labels_l.append(AKI_3_status[i])\n",
    "                    day_l.append(info_12h[i])\n",
    "                    if day not in used_day_id_l:\n",
    "                        day_l.append(info_24h_labs[i])\n",
    "                        used_day_id_l.append(day)\n",
    "                used_wind_id_l.append(wind)\n",
    "            day_info = ' '.join(day_l)\n",
    "            info_l.append(self.tokenize(day_info, self.max_length))\n",
    "\n",
    "        demographics = info_demo\n",
    "        diagnoses = info_diagnoses\n",
    "        demo_diags = ' '.join([demographics, diagnoses])\n",
    "        demo_diags = self.tokenize(demo_diags, 50)\n",
    "\n",
    "        #make tensors\n",
    "        tensor_day_info = torch.tensor(info_l[:self.observing_window//2], dtype=torch.int64)\n",
    "        tensor_demo_diags = torch.tensor(demo_diags, dtype=torch.int64)\n",
    "\n",
    "        if self.labels_df is None:\n",
    "            # making 24h labels from 12h\n",
    "            AKI_1_labels = [int(bool(np.sum(AKI_1_labels_l[i:i+2]))) for i in np.arange(0, self.observing_window + self.pred_window, 2)]\n",
    "            AKI_2_labels = [int(bool(np.sum(AKI_2_labels_l[i:i+2]))) for i in np.arange(0, self.observing_window + self.pred_window, 2)]\n",
    "            AKI_3_labels = [int(bool(np.sum(AKI_3_labels_l[i:i+2]))) for i in np.arange(0, self.observing_window + self.pred_window, 2)]\n",
    "            \n",
    "            tensor_labels = torch.tensor([*AKI_1_labels[self.observing_window//2:self.observing_window//2 + self.pred_window//2],\\\n",
    "                                        *AKI_2_labels[self.observing_window//2:self.observing_window//2 + self.pred_window//2],\\\n",
    "                                        *AKI_3_labels[self.observing_window//2:self.observing_window//2 + self.pred_window//2] ]\\\n",
    "                                            , dtype=torch.float64)\n",
    "        else:\n",
    "            AKI_1_label = (np.sum(self.labels_df[self.labels_df.stay_id==self.stay_id].AKI_1.values) > 0).astype(int)\n",
    "            AKI_2_label = (np.sum(self.labels_df[self.labels_df.stay_id==self.stay_id].AKI_2.values) > 0).astype(int)\n",
    "            AKI_3_label = (np.sum(self.labels_df[self.labels_df.stay_id==self.stay_id].AKI_3.values) > 0).astype(int)\n",
    "            NO_AKI_label = (np.sum([AKI_1_label, AKI_2_label, AKI_3_label]) == 0).astype(int)\n",
    "            tensor_labels = torch.tensor([AKI_1_label, AKI_2_label, AKI_3_label, NO_AKI_label])\n",
    "\n",
    "        return tensor_day_info, tensor_demo_diags, tensor_labels, {'stay_id':self.stay_id, 'day_id':used_day_id_l, 'wind_id':used_wind_id_l}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aki_stage_labels_second_day[aki_stage_labels_second_day.stay_id==37143047].AKI_1.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 1\n",
    "BATCH_SIZE = 1\n",
    "test_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/test_data/'\n",
    "train_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/train_data/'\n",
    "val_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/val_data/'\n",
    "\n",
    "test_dataset = MyDataset(data_path=test_data_path, labels_df=aki_stage_labels, tokenizer=tokenizer, max_length_dict=max_lengths_dict)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = MyDataset(data_path=val_data_path, labels_df=aki_stage_labels, tokenizer=tokenizer, max_length_dict=max_lengths_dict)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "train_dataset = MyDataset(data_path=train_data_path, labels_df=aki_stage_labels, tokenizer=tokenizer, max_length_dict=max_lengths_dict)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "test_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/test_data/'\n",
    "train_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/train_data/'\n",
    "val_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/val_data/'\n",
    "\n",
    "test_dataset = MyDataset(data_path=test_data_path, labels_df=None, tokenizer=tokenizer, max_length=350)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "train_dataset = MyDataset(data_path=train_data_path, labels_df=None, tokenizer=tokenizer, max_length=350)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = MyDataset(data_path=val_data_path, labels_df=None, tokenizer=tokenizer, max_length=350)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1428"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to retrieve all samples from dataset to test for any bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "idx_l = []\n",
    "cash_l = []\n",
    "for i in range(0, len(train_dataset)):\n",
    "    if i%100==0:\n",
    "        print(f'{i}')\n",
    "    cash_l.append(cash)\n",
    "    tensor_12h_info, tensor_24h_labs, tensor_diagnoses, tensor_demographics, tensor_labels, cash = train_dataset[i]\n",
    "\n",
    "    if i == 0:\n",
    "        labels_size = tensor_labels.size()\n",
    "\n",
    "    if tensor_labels.size() != labels_size:\n",
    "        print(i)\n",
    "        print(tensor_labels.size())\n",
    "        print(labels_size)\n",
    "        print(cash)\n",
    "        print(tensor_labels)\n",
    "        break\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0])"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_12h_info, tensor_24h_labs, tensor_diagnoses, tensor_demographics, tensor_labels, cash = train_dataset[47]\n",
    "tensor_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>AKI_1</th>\n",
       "      <th>AKI_2</th>\n",
       "      <th>AKI_3</th>\n",
       "      <th>ANY_AKI</th>\n",
       "      <th>NO_AKI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26651</th>\n",
       "      <td>16958246</td>\n",
       "      <td>21152721</td>\n",
       "      <td>37848539</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id   hadm_id   stay_id  AKI_1  AKI_2  AKI_3  ANY_AKI  NO_AKI\n",
       "26651    16958246  21152721  37848539      0      1      0        1       0"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aki_stage_labels_second_day[aki_stage_labels_second_day.stay_id==37848539]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "batch 1\n",
      "batch 2\n",
      "batch 3\n",
      "batch 4\n",
      "batch 5\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for tensor_day_info, tensor_demo_diags, tensor_labels, cash in train_loader:\n",
    "    print(f'batch {i}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_day_info, tensor_demo_diags, tensor_labels, cash = next(iter(test_loader))\n",
    "# tensor_12h_info, tensor_24h_labs, tensor_diagnoses, tensor_demographics, tensor_labels, cash = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_12h_info, tensor_24h_labs, tensor_diagnoses, tensor_demographics, tensor_labels, cash = next(iter(test_loader))\n",
    "tensor_12h_info = tensor_12h_info.cpu().detach().numpy()\n",
    "tensor_24h_labs = tensor_24h_labs.cpu().detach().numpy()\n",
    "tensor_diagnoses = tensor_diagnoses.cpu().detach().numpy()\n",
    "tensor_demographics = tensor_demographics.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stay_id': 36800797, 'day_id': [0, 1], 'wind_id': [0, 1, 2, 3]}\n",
      "tensor([0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "tensor_12h_info, tensor_24h_labs, tensor_diagnoses, tensor_demographics, tensor_labels, cash  = test_dataset[3]\n",
    "print(cash)\n",
    "print(tensor_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_day_info = tensor_day_info.cpu().detach().numpy()\n",
    "tensor_demo_diags = tensor_demo_diags.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stay_id': tensor([38917828, 32678181, 35455848, 31525638, 32602793, 30257222, 31854533,\n",
       "         39223118]),\n",
       " 'day_id': [tensor([0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1])],\n",
       " 'wind_id': [tensor([0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  tensor([1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  tensor([2, 2, 2, 2, 2, 2, 2, 2]),\n",
       "  tensor([3, 3, 3, 3, 3, 3, 3, 3])]}"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 1],\n",
       "        [0, 0, 0, 1],\n",
       "        [0, 0, 0, 1],\n",
       "        [0, 0, 0, 1],\n",
       "        [0, 1, 0, 0],\n",
       "        [0, 1, 0, 0],\n",
       "        [0, 0, 0, 1],\n",
       "        [0, 0, 0, 1]])"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v220045 68 . 0 105 . 0 v220046 130 . 0 130 . 0 v220047 50 . 0 50 . 0 v220050 120 . 0 152 . 0 v220051 48 . 0 67 . 0 v220052 69 . 0 136 . 0 v220179 138 . 0 153 . 0 v220180 64 . 0 81 . 0 v220181 82 . 0 97 . 0 v223761 98 . 3 99 . 1 o226560 370 . 0 m221828 m228315 m221833 m222042 m225153 l220228 15 . 1 l220545 44 . 0 l220546 8 . 8 l220587 25 . 0 l220602 105 . 0 l220603 171 . 0 l220615 0 . 9 l220621 137 . 0 l220624 59 . 0 l220632 286 . 0 l220635 2 . 0 l220644 23 . 0 l220645 141 . 0 l225612 60 . 0 l225624 19 . 0 l225625 8 . 8 l225634 74 . 0 l225664 124 . 0 l225671 93 . 0 l225677 2 . 8 l225690 0 . 7 l225693 97 . 0 l227073 16 . 0 l227429 0 . 01 l227442 4 . 2 l227443 23 . 0 l227445 3 . 0 l227456 4 . 2 l227457 142 . 0 l227465 11 . 2 l227466 25 . 2 l227467 1 . 0 v220045 61 . 0 94 . 0 v220046 130 . 0 130 . 0 v220047 50 . 0 50 . 0 v220050 117 . 0 159 . 0 v220051 43 . 0 70 . 0 v220052 66 . 0 98 . 0 v220179 129 . 0 129 . 0 v220180 67 . 0 67 . 0 v220181 81 . 0 81 . 0 v223761 97 . 7 98 . 6 o226560 300 . 0 m222042'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tensor_day_info[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'white m 68'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tensor_demo_diags[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>specimen_id</th>\n",
       "      <th>day_id</th>\n",
       "      <th>icu_day_id</th>\n",
       "      <th>icu_12h_window_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>intime</th>\n",
       "      <th>AKI_1_scr</th>\n",
       "      <th>AKI_2_scr</th>\n",
       "      <th>AKI_3_scr</th>\n",
       "      <th>AKI_1_urine</th>\n",
       "      <th>AKI_2_urine</th>\n",
       "      <th>AKI_3_urine</th>\n",
       "      <th>AKI_1</th>\n",
       "      <th>AKI_2</th>\n",
       "      <th>AKI_3</th>\n",
       "      <th>ANY_AKI</th>\n",
       "      <th>NO_AKI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>184535</th>\n",
       "      <td>13073052</td>\n",
       "      <td>28265498</td>\n",
       "      <td>30067772</td>\n",
       "      <td>72964827.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2169-08-11 04:16:00</td>\n",
       "      <td>2169-08-09 20:15:04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184536</th>\n",
       "      <td>13073052</td>\n",
       "      <td>28265498</td>\n",
       "      <td>30067772</td>\n",
       "      <td>71115289.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2169-08-11 13:20:00</td>\n",
       "      <td>2169-08-09 20:15:04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184537</th>\n",
       "      <td>13073052</td>\n",
       "      <td>28265498</td>\n",
       "      <td>30067772</td>\n",
       "      <td>12723801.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2169-08-13 02:14:00</td>\n",
       "      <td>2169-08-09 20:15:04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184538</th>\n",
       "      <td>13073052</td>\n",
       "      <td>28265498</td>\n",
       "      <td>30067772</td>\n",
       "      <td>20496494.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2169-08-13 21:14:00</td>\n",
       "      <td>2169-08-09 20:15:04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        subject_id   hadm_id   stay_id  specimen_id  day_id  icu_day_id  \\\n",
       "184535    13073052  28265498  30067772   72964827.0       1           1   \n",
       "184536    13073052  28265498  30067772   71115289.0       2           1   \n",
       "184537    13073052  28265498  30067772   12723801.0       3           3   \n",
       "184538    13073052  28265498  30067772   20496494.0       4           4   \n",
       "\n",
       "        icu_12h_window_id           charttime              intime  AKI_1_scr  \\\n",
       "184535                  2 2169-08-11 04:16:00 2169-08-09 20:15:04          0   \n",
       "184536                  3 2169-08-11 13:20:00 2169-08-09 20:15:04          0   \n",
       "184537                  6 2169-08-13 02:14:00 2169-08-09 20:15:04          0   \n",
       "184538                  8 2169-08-13 21:14:00 2169-08-09 20:15:04          0   \n",
       "\n",
       "        AKI_2_scr  AKI_3_scr  AKI_1_urine  AKI_2_urine  AKI_3_urine  AKI_1  \\\n",
       "184535          0          0          0.0          0.0          0.0      0   \n",
       "184536          0          0          0.0          0.0          0.0      0   \n",
       "184537          0          0          0.0          0.0          0.0      0   \n",
       "184538          0          0          0.0          0.0          0.0      0   \n",
       "\n",
       "        AKI_2  AKI_3  ANY_AKI  NO_AKI  \n",
       "184535      0      0        0       1  \n",
       "184536      0      0        0       1  \n",
       "184537      0      0        0       1  \n",
       "184538      0      0        0       1  "
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aki_stage_labels[aki_stage_labels.stay_id==30067772]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>day_id</th>\n",
       "      <th>icu_12h_window_id</th>\n",
       "      <th>icu_day_id</th>\n",
       "      <th>demographics</th>\n",
       "      <th>previous_diags_codes</th>\n",
       "      <th>previous_diags_names</th>\n",
       "      <th>vitals_names</th>\n",
       "      <th>...</th>\n",
       "      <th>labs_codes</th>\n",
       "      <th>outputs_names</th>\n",
       "      <th>outputs_codes</th>\n",
       "      <th>medications_names</th>\n",
       "      <th>medications_codes</th>\n",
       "      <th>AKI_1</th>\n",
       "      <th>AKI_2</th>\n",
       "      <th>AKI_3</th>\n",
       "      <th>icu_12h_info_codes</th>\n",
       "      <th>icu_12h_info_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13073052</td>\n",
       "      <td>28265498</td>\n",
       "      <td>30067772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WHITE F 72</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ABPd 44.0  62.0 mmHg; ABPm 64.0  84.0 mmHg; AB...</td>\n",
       "      <td>...</td>\n",
       "      <td>l220224 112.0; l220227 97.0; l220228 10.0; l22...</td>\n",
       "      <td>Foley 920.0 ml; OR EBL 750.0 ml; OR Urine 350....</td>\n",
       "      <td>o226559 920.0; o226576 110.0; o226626 750.0; o...</td>\n",
       "      <td>Heparin Sodium (Prophylaxis); Fentanyl (Concen...</td>\n",
       "      <td>m225975 m225942 m222168 m221668 m223258 m221749</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>v220045 81.0  100.0; v220046 120.0  130.0; v22...</td>\n",
       "      <td>ABPd 44.0  62.0 mmHg; ABPm 64.0  84.0 mmHg; AB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13073052</td>\n",
       "      <td>28265498</td>\n",
       "      <td>30067772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WHITE F 72</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ABPd 46.0  66.0 mmHg; ABPm 68.0  87.0 mmHg; AB...</td>\n",
       "      <td>...</td>\n",
       "      <td>l220224 112.0; l220227 97.0; l220228 10.0; l22...</td>\n",
       "      <td>Foley 810.0 ml</td>\n",
       "      <td>o226559 810.0</td>\n",
       "      <td>Fentanyl (Concentrate); Potassium Chloride; KC...</td>\n",
       "      <td>m225942 m225166 m227522 m221456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>v220045 92.0  123.0; v220046 130.0  130.0; v22...</td>\n",
       "      <td>ABPd 46.0  66.0 mmHg; ABPm 68.0  87.0 mmHg; AB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13073052</td>\n",
       "      <td>28265498</td>\n",
       "      <td>30067772</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>WHITE F 72</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ABPd 50.0  75.0 mmHg; ABPm 66.0  96.0 mmHg; AB...</td>\n",
       "      <td>...</td>\n",
       "      <td>l220224 84.0; l220227 95.0; l220228 9.9; l2202...</td>\n",
       "      <td>Foley 840.0 ml; Oral Gastric 200.0 ml</td>\n",
       "      <td>o226559 840.0; o226576 200.0</td>\n",
       "      <td>Fentanyl (Concentrate); Propofol</td>\n",
       "      <td>m225942 m222168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>v220045 91.0  121.0; v220046 120.0  120.0; v22...</td>\n",
       "      <td>ABPd 50.0  75.0 mmHg; ABPm 66.0  96.0 mmHg; AB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13073052</td>\n",
       "      <td>28265498</td>\n",
       "      <td>30067772</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>WHITE F 72</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HR 90.0  140.0 bpm; HR Alarm - High 120.0  120...</td>\n",
       "      <td>...</td>\n",
       "      <td>l220224 84.0; l220227 95.0; l220228 9.9; l2202...</td>\n",
       "      <td>Foley 495.0 ml</td>\n",
       "      <td>o226559 495.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>v220045 90.0  140.0; v220046 120.0  120.0; v22...</td>\n",
       "      <td>HR 90.0  140.0 bpm; HR Alarm - High 120.0  120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13073052</td>\n",
       "      <td>28265498</td>\n",
       "      <td>30067772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>WHITE F 72</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HR 84.0  105.0 bpm; HR Alarm - High 145.0  145...</td>\n",
       "      <td>...</td>\n",
       "      <td>l220224 250.0; l220227 99.0; l220228 9.0; l220...</td>\n",
       "      <td>Foley 760.0 ml</td>\n",
       "      <td>o226559 760.0</td>\n",
       "      <td>Insulin - Regular; Na Phos</td>\n",
       "      <td>m223258 m225835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>v220045 84.0  105.0; v220046 145.0  145.0; v22...</td>\n",
       "      <td>HR 84.0  105.0 bpm; HR Alarm - High 145.0  145...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13073052</td>\n",
       "      <td>28265498</td>\n",
       "      <td>30067772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>WHITE F 72</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HR 76.0  106.0 bpm; HR Alarm - High 120.0  125...</td>\n",
       "      <td>...</td>\n",
       "      <td>l220224 250.0; l220227 99.0; l220228 9.0; l220...</td>\n",
       "      <td>Foley 1015.0 ml; OR Urine 1600.0 ml</td>\n",
       "      <td>o226559 1015.0; o226627 1600.0</td>\n",
       "      <td>Heparin Sodium; Propofol; Na Phos</td>\n",
       "      <td>m225152 m222168 m225835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>v220045 76.0  106.0; v220046 120.0  125.0; v22...</td>\n",
       "      <td>HR 76.0  106.0 bpm; HR Alarm - High 120.0  125...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13073052</td>\n",
       "      <td>28265498</td>\n",
       "      <td>30067772</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "      <td>WHITE F 72</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HR 83.0  95.0 bpm; HR Alarm - High 120.0  120....</td>\n",
       "      <td>...</td>\n",
       "      <td>l220228 9.4; l220545 28.2; l220546 11.5; l2206...</td>\n",
       "      <td>Foley 740.0 ml</td>\n",
       "      <td>o226559 740.0</td>\n",
       "      <td>Insulin - Regular; Potassium Chloride; KCL (Bo...</td>\n",
       "      <td>m223258 m225166 m227522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>v220045 83.0  95.0; v220046 120.0  120.0; v220...</td>\n",
       "      <td>HR 83.0  95.0 bpm; HR Alarm - High 120.0  120....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13073052</td>\n",
       "      <td>28265498</td>\n",
       "      <td>30067772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>WHITE F 72</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HR 87.0  109.0 bpm; HR Alarm - High 120.0  120...</td>\n",
       "      <td>...</td>\n",
       "      <td>l220228 9.4; l220545 28.2; l220546 11.5; l2206...</td>\n",
       "      <td>Foley 1040.0 ml</td>\n",
       "      <td>o226559 1040.0</td>\n",
       "      <td>Furosemide (Lasix); Insulin - Regular</td>\n",
       "      <td>m221794 m223258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>v220045 87.0  109.0; v220046 120.0  120.0; v22...</td>\n",
       "      <td>HR 87.0  109.0 bpm; HR Alarm - High 120.0  120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13073052</td>\n",
       "      <td>28265498</td>\n",
       "      <td>30067772</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4</td>\n",
       "      <td>WHITE F 72</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HR 0.0  134.0 bpm; HR Alarm - High 120.0  120....</td>\n",
       "      <td>...</td>\n",
       "      <td>l220228 9.6; l220545 28.9; l220546 9.2; l22060...</td>\n",
       "      <td>Foley 780.0 ml</td>\n",
       "      <td>o226559 780.0</td>\n",
       "      <td>Ranitidine (Prophylaxis); Magnesium Sulfate; M...</td>\n",
       "      <td>m225911 m222011 m227523 m223258 m225166 m22752...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>v220045 0.0  134.0; v220046 120.0  120.0; v220...</td>\n",
       "      <td>HR 0.0  134.0 bpm; HR Alarm - High 120.0  120....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13073052</td>\n",
       "      <td>28265498</td>\n",
       "      <td>30067772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4</td>\n",
       "      <td>WHITE F 72</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HR 79.0  90.0 bpm; HR Alarm - High 120.0  120....</td>\n",
       "      <td>...</td>\n",
       "      <td>l220228 9.6; l220545 28.9; l220546 9.2; l22060...</td>\n",
       "      <td>Foley 1450.0 ml</td>\n",
       "      <td>o226559 1450.0</td>\n",
       "      <td>Furosemide (Lasix); Magnesium Sulfate; Magnesi...</td>\n",
       "      <td>m221794 m222011 m227523 m223258 m225834 m221347</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>v220045 79.0  90.0; v220046 120.0  120.0; v220...</td>\n",
       "      <td>HR 79.0  90.0 bpm; HR Alarm - High 120.0  120....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id   stay_id  day_id  icu_12h_window_id  icu_day_id  \\\n",
       "0    13073052  28265498  30067772     NaN                0.0           0   \n",
       "1    13073052  28265498  30067772     NaN                1.0           0   \n",
       "2    13073052  28265498  30067772     1.0                2.0           1   \n",
       "3    13073052  28265498  30067772     2.0                3.0           1   \n",
       "4    13073052  28265498  30067772     NaN                4.0           2   \n",
       "5    13073052  28265498  30067772     NaN                5.0           2   \n",
       "6    13073052  28265498  30067772     3.0                6.0           3   \n",
       "7    13073052  28265498  30067772     NaN                7.0           3   \n",
       "8    13073052  28265498  30067772     4.0                8.0           4   \n",
       "9    13073052  28265498  30067772     NaN                9.0           4   \n",
       "\n",
       "  demographics                             previous_diags_codes  \\\n",
       "0  WHITE F 72                                                     \n",
       "1  WHITE F 72                                                     \n",
       "2  WHITE F 72                                                     \n",
       "3  WHITE F 72                                                     \n",
       "4  WHITE F 72                                                     \n",
       "5  WHITE F 72                                                     \n",
       "6  WHITE F 72                                                     \n",
       "7  WHITE F 72                                                     \n",
       "8  WHITE F 72                                                     \n",
       "9  WHITE F 72                                                     \n",
       "\n",
       "                              previous_diags_names  \\\n",
       "0                                                    \n",
       "1                                                    \n",
       "2                                                    \n",
       "3                                                    \n",
       "4                                                    \n",
       "5                                                    \n",
       "6                                                    \n",
       "7                                                    \n",
       "8                                                    \n",
       "9                                                    \n",
       "\n",
       "                                        vitals_names  ...  \\\n",
       "0  ABPd 44.0  62.0 mmHg; ABPm 64.0  84.0 mmHg; AB...  ...   \n",
       "1  ABPd 46.0  66.0 mmHg; ABPm 68.0  87.0 mmHg; AB...  ...   \n",
       "2  ABPd 50.0  75.0 mmHg; ABPm 66.0  96.0 mmHg; AB...  ...   \n",
       "3  HR 90.0  140.0 bpm; HR Alarm - High 120.0  120...  ...   \n",
       "4  HR 84.0  105.0 bpm; HR Alarm - High 145.0  145...  ...   \n",
       "5  HR 76.0  106.0 bpm; HR Alarm - High 120.0  125...  ...   \n",
       "6  HR 83.0  95.0 bpm; HR Alarm - High 120.0  120....  ...   \n",
       "7  HR 87.0  109.0 bpm; HR Alarm - High 120.0  120...  ...   \n",
       "8  HR 0.0  134.0 bpm; HR Alarm - High 120.0  120....  ...   \n",
       "9  HR 79.0  90.0 bpm; HR Alarm - High 120.0  120....  ...   \n",
       "\n",
       "                                          labs_codes  \\\n",
       "0  l220224 112.0; l220227 97.0; l220228 10.0; l22...   \n",
       "1  l220224 112.0; l220227 97.0; l220228 10.0; l22...   \n",
       "2  l220224 84.0; l220227 95.0; l220228 9.9; l2202...   \n",
       "3  l220224 84.0; l220227 95.0; l220228 9.9; l2202...   \n",
       "4  l220224 250.0; l220227 99.0; l220228 9.0; l220...   \n",
       "5  l220224 250.0; l220227 99.0; l220228 9.0; l220...   \n",
       "6  l220228 9.4; l220545 28.2; l220546 11.5; l2206...   \n",
       "7  l220228 9.4; l220545 28.2; l220546 11.5; l2206...   \n",
       "8  l220228 9.6; l220545 28.9; l220546 9.2; l22060...   \n",
       "9  l220228 9.6; l220545 28.9; l220546 9.2; l22060...   \n",
       "\n",
       "                                       outputs_names  \\\n",
       "0  Foley 920.0 ml; OR EBL 750.0 ml; OR Urine 350....   \n",
       "1                                     Foley 810.0 ml   \n",
       "2              Foley 840.0 ml; Oral Gastric 200.0 ml   \n",
       "3                                     Foley 495.0 ml   \n",
       "4                                     Foley 760.0 ml   \n",
       "5                Foley 1015.0 ml; OR Urine 1600.0 ml   \n",
       "6                                     Foley 740.0 ml   \n",
       "7                                    Foley 1040.0 ml   \n",
       "8                                     Foley 780.0 ml   \n",
       "9                                    Foley 1450.0 ml   \n",
       "\n",
       "                                       outputs_codes  \\\n",
       "0  o226559 920.0; o226576 110.0; o226626 750.0; o...   \n",
       "1                                      o226559 810.0   \n",
       "2                       o226559 840.0; o226576 200.0   \n",
       "3                                      o226559 495.0   \n",
       "4                                      o226559 760.0   \n",
       "5                     o226559 1015.0; o226627 1600.0   \n",
       "6                                      o226559 740.0   \n",
       "7                                     o226559 1040.0   \n",
       "8                                      o226559 780.0   \n",
       "9                                     o226559 1450.0   \n",
       "\n",
       "                                   medications_names  \\\n",
       "0  Heparin Sodium (Prophylaxis); Fentanyl (Concen...   \n",
       "1  Fentanyl (Concentrate); Potassium Chloride; KC...   \n",
       "2                   Fentanyl (Concentrate); Propofol   \n",
       "3                                                NaN   \n",
       "4                         Insulin - Regular; Na Phos   \n",
       "5                  Heparin Sodium; Propofol; Na Phos   \n",
       "6  Insulin - Regular; Potassium Chloride; KCL (Bo...   \n",
       "7              Furosemide (Lasix); Insulin - Regular   \n",
       "8  Ranitidine (Prophylaxis); Magnesium Sulfate; M...   \n",
       "9  Furosemide (Lasix); Magnesium Sulfate; Magnesi...   \n",
       "\n",
       "                                   medications_codes AKI_1 AKI_2  AKI_3  \\\n",
       "0    m225975 m225942 m222168 m221668 m223258 m221749   0.0   0.0    0.0   \n",
       "1                    m225942 m225166 m227522 m221456   0.0   0.0    0.0   \n",
       "2                                    m225942 m222168   0.0   0.0    0.0   \n",
       "3                                                NaN   0.0   0.0    0.0   \n",
       "4                                    m223258 m225835   0.0   0.0    0.0   \n",
       "5                            m225152 m222168 m225835   0.0   0.0    0.0   \n",
       "6                            m223258 m225166 m227522   0.0   0.0    0.0   \n",
       "7                                    m221794 m223258   0.0   0.0    0.0   \n",
       "8  m225911 m222011 m227523 m223258 m225166 m22752...   0.0   0.0    0.0   \n",
       "9    m221794 m222011 m227523 m223258 m225834 m221347   0.0   0.0    0.0   \n",
       "\n",
       "                                  icu_12h_info_codes  \\\n",
       "0  v220045 81.0  100.0; v220046 120.0  130.0; v22...   \n",
       "1  v220045 92.0  123.0; v220046 130.0  130.0; v22...   \n",
       "2  v220045 91.0  121.0; v220046 120.0  120.0; v22...   \n",
       "3  v220045 90.0  140.0; v220046 120.0  120.0; v22...   \n",
       "4  v220045 84.0  105.0; v220046 145.0  145.0; v22...   \n",
       "5  v220045 76.0  106.0; v220046 120.0  125.0; v22...   \n",
       "6  v220045 83.0  95.0; v220046 120.0  120.0; v220...   \n",
       "7  v220045 87.0  109.0; v220046 120.0  120.0; v22...   \n",
       "8  v220045 0.0  134.0; v220046 120.0  120.0; v220...   \n",
       "9  v220045 79.0  90.0; v220046 120.0  120.0; v220...   \n",
       "\n",
       "                                  icu_12h_info_names  \n",
       "0  ABPd 44.0  62.0 mmHg; ABPm 64.0  84.0 mmHg; AB...  \n",
       "1  ABPd 46.0  66.0 mmHg; ABPm 68.0  87.0 mmHg; AB...  \n",
       "2  ABPd 50.0  75.0 mmHg; ABPm 66.0  96.0 mmHg; AB...  \n",
       "3  HR 90.0  140.0 bpm; HR Alarm - High 120.0  120...  \n",
       "4  HR 84.0  105.0 bpm; HR Alarm - High 145.0  145...  \n",
       "5  HR 76.0  106.0 bpm; HR Alarm - High 120.0  125...  \n",
       "6  HR 83.0  95.0 bpm; HR Alarm - High 120.0  120....  \n",
       "7  HR 87.0  109.0 bpm; HR Alarm - High 120.0  120...  \n",
       "8  HR 0.0  134.0 bpm; HR Alarm - High 120.0  120....  \n",
       "9  HR 79.0  90.0 bpm; HR Alarm - High 120.0  120....  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/train_data/icu_stay_30067772.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_labels.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "observing_window = 2\n",
    "pred_window = 2\n",
    "\n",
    "def preproccess( df):\n",
    "    df['demographics'] = df['demographics'].fillna('')\n",
    "    df['previous_diags_codes'] = df['previous_diags_codes'].fillna('')\n",
    "    df['previous_diags_names'] = df['previous_diags_names'].fillna('')\n",
    "    df['vitals_names'] = df['vitals_names'].fillna('')\n",
    "    df['vitals_codes'] = df['vitals_codes'].fillna('')\n",
    "    df['labs_names'] = df['labs_names'].fillna('')\n",
    "    df['labs_codes'] = df['labs_codes'].fillna('')\n",
    "    df['outputs_names'] = df['outputs_names'].fillna('')\n",
    "    df['outputs_codes'] = df['outputs_codes'].fillna('')\n",
    "    df['medications_names'] = df['medications_names'].fillna('')\n",
    "    df['medications_codes'] = df['medications_codes'].fillna('')\n",
    "\n",
    "    df['AKI_1'] = df['AKI_1'].fillna(0)\n",
    "    df['AKI_2'] = df['AKI_2'].fillna(0)\n",
    "    df['AKI_3'] = df['AKI_3'].fillna(0)\n",
    "\n",
    "    df = df[(df.icu_12h_window_id.isin(np.arange(min_wind, min_wind + observing_window +  pred_window))) | (df.icu_day_id.isin(np.arange(min_day, observing_window//2 +  pred_window//2)))]\n",
    "    return df\n",
    "\n",
    "def tokenize(text, max_length):\n",
    "    \n",
    "    try:\n",
    "        output = tokenizer.encode(text)\n",
    "    except:\n",
    "        print( type(text), text, max_length)\n",
    "        output = tokenizer.encode(text[0])\n",
    "\n",
    "    # padding and truncation\n",
    "    if len(output.ids) < max_length:\n",
    "        len_missing_token = max_length - len(output.ids)\n",
    "        padding_vec = [tokenizer.token_to_id('PAD') for _ in range(len_missing_token)]\n",
    "        token_output = [*output.ids, *padding_vec]\n",
    "    elif len(output.ids) > max_length:\n",
    "        token_output = output.ids[:max_length]\n",
    "    else:\n",
    "        token_output = output.ids\n",
    "    \n",
    "    return token_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/train_data/icu_stay_31230214.csv')\n",
    "\n",
    "windows_12h = df.icu_12h_window_id.values\n",
    "days = df.icu_day_id.values\n",
    "min_wind = int( np.max([np.min(windows_12h[~np.isnan(windows_12h)]),0] ) )      \n",
    "min_day = int( np.max( [np.min(days[~np.isnan(days)]), 0] ))    \n",
    "df = preproccess(df)\n",
    "stay_id = df.stay_id.values[0]  \n",
    "# print(stay_id)\n",
    "sort = np.argsort(df.icu_12h_window_id.values)\n",
    "windows_12h = df.icu_12h_window_id.values[sort]\n",
    "days = df.icu_day_id.values[sort]\n",
    "\n",
    "info_12h = df.icu_12h_info_codes.values[sort]\n",
    "info_24h_labs = df.labs_codes.values[sort]\n",
    "info_demo = df.demographics.values[0]\n",
    "info_diagnoses = df.previous_diags_codes.values[0]\n",
    "\n",
    "AKI_1_status = df.AKI_1.values[sort]\n",
    "AKI_2_status = df.AKI_2.values[sort]\n",
    "AKI_3_status = df.AKI_3.values[sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lengths_dict = {'demographics':30, 'previous_diags_codes':65,'labs_codes':240, 'icu_12h_info_codes':120}\n",
    "max_length_12h = max_lengths_dict['icu_12h_info_codes']\n",
    "max_length_24h = max_lengths_dict['labs_codes']\n",
    "max_length_demo = max_lengths_dict['demographics']\n",
    "max_length_diags = max_lengths_dict['previous_diags_codes']\n",
    "\n",
    "AKI_1_labels_l = []\n",
    "AKI_2_labels_l = []\n",
    "AKI_3_labels_l = []\n",
    "info_12h_list = []\n",
    "info_24h_list = []\n",
    "used_day_id_l = []\n",
    "\n",
    "wind_12h_pairs = [(i, i+1) for i in range(0, 2*(min_day + observing_window//2 +  pred_window//2), 2)]\n",
    "\n",
    "for day in range(min_day, min_day + observing_window//2 +  pred_window//2):\n",
    "    for wind in wind_12h_pairs[day]:\n",
    "        if wind not in windows_12h:\n",
    "            if day not in days:\n",
    "            # print('not in days')\n",
    "                AKI_1_labels_l.append(0)\n",
    "                AKI_2_labels_l.append(0)\n",
    "                AKI_3_labels_l.append(0)\n",
    "                info_12h_list.append( tokenize('',  max_length_12h))\n",
    "                if day not in used_day_id_l:\n",
    "                    info_24h_list.append( tokenize('',  max_length_24h))\n",
    "                    used_day_id_l.append(day)\n",
    "            else:\n",
    "                AKI_1_labels_l.append(df[df.icu_day_id==day].AKI_1.values[0])\n",
    "                AKI_2_labels_l.append(df[df.icu_day_id==day].AKI_2.values[0])\n",
    "                AKI_3_labels_l.append(df[df.icu_day_id==day].AKI_3.values[0])\n",
    "                info_12h_list.append(tokenize(df[df.icu_day_id==day].icu_12h_info_codes.values[0],  max_length_12h))\n",
    "                if day not in used_day_id_l:\n",
    "                    info_24h_list.append( tokenize(df[df.icu_day_id==day].labs_codes.values[0], max_length_24h))\n",
    "                    used_day_id_l.append(day)\n",
    "        else:\n",
    "            i = list(windows_12h).index(wind)\n",
    "\n",
    "            AKI_1_labels_l.append(AKI_1_status[i])\n",
    "            AKI_2_labels_l.append(AKI_2_status[i])\n",
    "            AKI_3_labels_l.append(AKI_3_status[i])\n",
    "            info_12h_list.append(tokenize(info_12h[i], max_length_12h))\n",
    "            if day not in used_day_id_l:\n",
    "                info_24h_list.append(tokenize(info_24h_labs[i], max_length_24h))\n",
    "                used_day_id_l.append(day)\n",
    "\n",
    "    demographics = tokenize(info_demo, max_length_demo)\n",
    "    diagnoses = tokenize(info_diagnoses, max_length_diags)\n",
    "\n",
    "    # making 24h labels from 12h\n",
    "    AKI_1_labels = [int(bool(np.sum(AKI_1_labels_l[i:i+2]))) for i in np.arange(0, observing_window + pred_window, 2)]\n",
    "    AKI_2_labels = [int(bool(np.sum(AKI_2_labels_l[i:i+2]))) for i in np.arange(0, observing_window + pred_window, 2)]\n",
    "    AKI_3_labels = [int(bool(np.sum(AKI_3_labels_l[i:i+2]))) for i in np.arange(0, observing_window + pred_window, 2)]\n",
    "\n",
    "    #make tensors\n",
    "    tensor_12h_info = torch.tensor(info_12h_list[:observing_window], dtype=torch.int64)\n",
    "    tensor_24h_labs = torch.tensor(info_24h_list[:observing_window//2], dtype=torch.int64)\n",
    "    tensor_diagnoses = torch.tensor(diagnoses, dtype=torch.int64)\n",
    "    tensor_demographics = torch.tensor(demographics, dtype=torch.int64)\n",
    "    tensor_labels = torch.tensor([*AKI_1_labels[observing_window//2:observing_window//2 + pred_window//2],\\\n",
    "                                    *AKI_2_labels[observing_window//2:observing_window//2 + pred_window//2],\\\n",
    "                                    *AKI_3_labels[observing_window//2:observing_window//2 + pred_window//2]  \\\n",
    "                                    ], dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AKI_1_labels[observing_window//2:observing_window//2 + pred_window//2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 120])\n",
      "torch.Size([1, 240])\n",
      "torch.Size([65])\n",
      "torch.Size([30])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_12h_info.shape)\n",
    "print(tensor_24h_labs.shape)\n",
    "print(tensor_diagnoses.shape)\n",
    "print(tensor_demographics.shape)\n",
    "print(tensor_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from  /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/train_data/icu_stay_36870065.csv\n"
     ]
    }
   ],
   "source": [
    "tensor_12h_info, tensor_24h_labs, tensor_diagnoses, tensor_demographics, tensor_labels, cash = train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_12h_info = tensor_12h_info.cpu().detach().numpy()\n",
    "tensor_24h_labs = tensor_24h_labs.cpu().detach().numpy()\n",
    "tensor_diagnoses = tensor_diagnoses.cpu().detach().numpy()\n",
    "tensor_demographics = tensor_demographics.cpu().detach().numpy()\n",
    "tensor_labels = tensor_labels.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stay_id': 36870065, 'day_id': [0, 1], 'wind_id': [0, 1, 2, 3]}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/train_data/icu_stay_36870065.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 120])\n",
      "torch.Size([1, 300])\n",
      "torch.Size([65])\n",
      "torch.Size([30])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_12h_info.shape)\n",
    "print(tensor_24h_labs.shape)\n",
    "print(tensor_diagnoses.shape)\n",
    "print(tensor_demographics.shape)\n",
    "print(tensor_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tensor_diagnoses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WHITE M 42 Blood Pressure 110/60 '"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.icu_12h_window_id.isin(cash['wind_id'])].demographics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>day_id</th>\n",
       "      <th>icu_12h_window_id</th>\n",
       "      <th>icu_day_id</th>\n",
       "      <th>demographics</th>\n",
       "      <th>previous_diags_codes</th>\n",
       "      <th>previous_diags_names</th>\n",
       "      <th>vitals_names</th>\n",
       "      <th>...</th>\n",
       "      <th>labs_codes</th>\n",
       "      <th>outputs_names</th>\n",
       "      <th>outputs_codes</th>\n",
       "      <th>medications_names</th>\n",
       "      <th>medications_codes</th>\n",
       "      <th>AKI_1</th>\n",
       "      <th>AKI_2</th>\n",
       "      <th>AKI_3</th>\n",
       "      <th>icu_12h_info_codes</th>\n",
       "      <th>icu_12h_info_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39482</th>\n",
       "      <td>10840421</td>\n",
       "      <td>22617919</td>\n",
       "      <td>36870065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WHITE M 42 Blood Pressure 110/60</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ABPd 53.0  81.0 mmHg; ABPm 74.0  111.0 mmHg; A...</td>\n",
       "      <td>...</td>\n",
       "      <td>l220228 13.0; l220545 40.0; l220546 13.4; l220...</td>\n",
       "      <td>Emesis 500.0 ml; Foley 1670.0 ml; OR EBL 100.0...</td>\n",
       "      <td>o226559 1670.0; o226571 500.0; o226626 100.0; ...</td>\n",
       "      <td>Hydralazine; Morphine Sulfate; Hydromorphone (...</td>\n",
       "      <td>m221828 m225154 m221833 m225907 m222042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>v220045 49.0  106.0; v220046 120.0  120.0; v22...</td>\n",
       "      <td>ABPd 53.0  81.0 mmHg; ABPm 74.0  111.0 mmHg; A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39483</th>\n",
       "      <td>10840421</td>\n",
       "      <td>22617919</td>\n",
       "      <td>36870065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WHITE M 42 Blood Pressure 110/60</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ABPd 52.0  71.0 mmHg; ABPm 72.0  92.0 mmHg; AB...</td>\n",
       "      <td>...</td>\n",
       "      <td>l220228 13.0; l220545 40.0; l220546 13.4; l220...</td>\n",
       "      <td>Foley 670.0 ml</td>\n",
       "      <td>o226559 670.0</td>\n",
       "      <td>Hydromorphone (Dilaudid)</td>\n",
       "      <td>m221833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>v220045 66.0  88.0; v220050 109.0  132.0; v220...</td>\n",
       "      <td>ABPd 52.0  71.0 mmHg; ABPm 72.0  92.0 mmHg; AB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39484</th>\n",
       "      <td>10840421</td>\n",
       "      <td>22617919</td>\n",
       "      <td>36870065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>WHITE M 42 Blood Pressure 110/60</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HR 74.0  93.0 bpm; HR Alarm - High 120.0  120....</td>\n",
       "      <td>...</td>\n",
       "      <td>l220228 13.0; l220545 40.2; l220546 17.1; l220...</td>\n",
       "      <td>Void 1850.0 ml</td>\n",
       "      <td>o226560 1850.0</td>\n",
       "      <td>Heparin Sodium (Prophylaxis); Hydromorphone (D...</td>\n",
       "      <td>m225975 m221833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>v220045 74.0  93.0; v220046 120.0  120.0; v220...</td>\n",
       "      <td>HR 74.0  93.0 bpm; HR Alarm - High 120.0  120....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39485</th>\n",
       "      <td>10840421</td>\n",
       "      <td>22617919</td>\n",
       "      <td>36870065</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>WHITE M 42 Blood Pressure 110/60</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HR 62.0  89.0 bpm; HR Alarm - High 120.0  120....</td>\n",
       "      <td>...</td>\n",
       "      <td>l220228 13.0; l220545 40.2; l220546 17.1; l220...</td>\n",
       "      <td>Void 1600.0 ml</td>\n",
       "      <td>o226560 1600.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>v220045 62.0  89.0; v220046 120.0  120.0; v220...</td>\n",
       "      <td>HR 62.0  89.0 bpm; HR Alarm - High 120.0  120....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39486</th>\n",
       "      <td>10840421</td>\n",
       "      <td>22617919</td>\n",
       "      <td>36870065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>WHITE M 42 Blood Pressure 110/60</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HR 78.0  78.0 bpm; NBPd 73.0  73.0 mmHg; NBPm ...</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>v220045 78.0  78.0; v220179 139.0  139.0; v220...</td>\n",
       "      <td>HR 78.0  78.0 bpm; NBPd 73.0  73.0 mmHg; NBPm ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id   hadm_id   stay_id  day_id  icu_12h_window_id  icu_day_id  \\\n",
       "39482    10840421  22617919  36870065     NaN                0.0           0   \n",
       "39483    10840421  22617919  36870065     0.0                1.0           0   \n",
       "39484    10840421  22617919  36870065     NaN                2.0           1   \n",
       "39485    10840421  22617919  36870065     1.0                3.0           1   \n",
       "39486    10840421  22617919  36870065     NaN                4.0           2   \n",
       "\n",
       "                            demographics  \\\n",
       "39482  WHITE M 42 Blood Pressure 110/60    \n",
       "39483  WHITE M 42 Blood Pressure 110/60    \n",
       "39484  WHITE M 42 Blood Pressure 110/60    \n",
       "39485  WHITE M 42 Blood Pressure 110/60    \n",
       "39486  WHITE M 42 Blood Pressure 110/60    \n",
       "\n",
       "                                  previous_diags_codes  \\\n",
       "39482                                                    \n",
       "39483                                                    \n",
       "39484                                                    \n",
       "39485                                                    \n",
       "39486                                                    \n",
       "\n",
       "                                  previous_diags_names  \\\n",
       "39482                                                    \n",
       "39483                                                    \n",
       "39484                                                    \n",
       "39485                                                    \n",
       "39486                                                    \n",
       "\n",
       "                                            vitals_names  ...  \\\n",
       "39482  ABPd 53.0  81.0 mmHg; ABPm 74.0  111.0 mmHg; A...  ...   \n",
       "39483  ABPd 52.0  71.0 mmHg; ABPm 72.0  92.0 mmHg; AB...  ...   \n",
       "39484  HR 74.0  93.0 bpm; HR Alarm - High 120.0  120....  ...   \n",
       "39485  HR 62.0  89.0 bpm; HR Alarm - High 120.0  120....  ...   \n",
       "39486  HR 78.0  78.0 bpm; NBPd 73.0  73.0 mmHg; NBPm ...  ...   \n",
       "\n",
       "                                              labs_codes  \\\n",
       "39482  l220228 13.0; l220545 40.0; l220546 13.4; l220...   \n",
       "39483  l220228 13.0; l220545 40.0; l220546 13.4; l220...   \n",
       "39484  l220228 13.0; l220545 40.2; l220546 17.1; l220...   \n",
       "39485  l220228 13.0; l220545 40.2; l220546 17.1; l220...   \n",
       "39486                                                      \n",
       "\n",
       "                                           outputs_names  \\\n",
       "39482  Emesis 500.0 ml; Foley 1670.0 ml; OR EBL 100.0...   \n",
       "39483                                     Foley 670.0 ml   \n",
       "39484                                     Void 1850.0 ml   \n",
       "39485                                     Void 1600.0 ml   \n",
       "39486                                                      \n",
       "\n",
       "                                           outputs_codes  \\\n",
       "39482  o226559 1670.0; o226571 500.0; o226626 100.0; ...   \n",
       "39483                                      o226559 670.0   \n",
       "39484                                     o226560 1850.0   \n",
       "39485                                     o226560 1600.0   \n",
       "39486                                                      \n",
       "\n",
       "                                       medications_names  \\\n",
       "39482  Hydralazine; Morphine Sulfate; Hydromorphone (...   \n",
       "39483                           Hydromorphone (Dilaudid)   \n",
       "39484  Heparin Sodium (Prophylaxis); Hydromorphone (D...   \n",
       "39485                                                      \n",
       "39486                                                      \n",
       "\n",
       "                             medications_codes AKI_1 AKI_2  AKI_3  \\\n",
       "39482  m221828 m225154 m221833 m225907 m222042   0.0   0.0    0.0   \n",
       "39483                                  m221833   0.0   0.0    0.0   \n",
       "39484                          m225975 m221833   0.0   0.0    0.0   \n",
       "39485                                            0.0   0.0    0.0   \n",
       "39486                                            0.0   0.0    0.0   \n",
       "\n",
       "                                      icu_12h_info_codes  \\\n",
       "39482  v220045 49.0  106.0; v220046 120.0  120.0; v22...   \n",
       "39483  v220045 66.0  88.0; v220050 109.0  132.0; v220...   \n",
       "39484  v220045 74.0  93.0; v220046 120.0  120.0; v220...   \n",
       "39485  v220045 62.0  89.0; v220046 120.0  120.0; v220...   \n",
       "39486  v220045 78.0  78.0; v220179 139.0  139.0; v220...   \n",
       "\n",
       "                                      icu_12h_info_names  \n",
       "39482  ABPd 53.0  81.0 mmHg; ABPm 74.0  111.0 mmHg; A...  \n",
       "39483  ABPd 52.0  71.0 mmHg; ABPm 72.0  92.0 mmHg; AB...  \n",
       "39484  HR 74.0  93.0 bpm; HR Alarm - High 120.0  120....  \n",
       "39485  HR 62.0  89.0 bpm; HR Alarm - High 120.0  120....  \n",
       "39486  HR 78.0  78.0 bpm; NBPd 73.0  73.0 mmHg; NBPm ...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_train_dataset_icu[pid_train_dataset_icu.stay_id==36870065]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 1\n",
    "BATCH_SIZE = 8\n",
    "test_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/test_data/'\n",
    "train_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/train_data/'\n",
    "val_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/val_data/'\n",
    "\n",
    "test_dataset = MyDataset(data_path=test_data_path, labels_df=aki_stage_labels, tokenizer=tokenizer, max_length_dict=max_lengths_dict)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = MyDataset(data_path=val_data_path, labels_df=aki_stage_labels, tokenizer=tokenizer, max_length_dict=max_lengths_dict)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "train_dataset = MyDataset(data_path=train_data_path, labels_df=aki_stage_labels, tokenizer=tokenizer, max_length_dict=max_lengths_dict)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lengths_dict = {'demographics':30, 'previous_diags_codes':65, 'labs_codes':240, 'icu_12h_info_codes':120}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHR_MODEL(nn.Module):\n",
    "    def __init__(self, max_lengths_dict, vocab_size, pred_window=2, observing_window=2,  H=128, embedding_size=200, drop=0.6):\n",
    "        super(EHR_MODEL, self).__init__()\n",
    "\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.H = H\n",
    "        self.p = drop\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.max_length_demographics = max_lengths_dict['demographics']\n",
    "        self.max_length_previous_diags = max_lengths_dict['previous_diags_codes']\n",
    "        self.max_length_labs_24h = max_lengths_dict['labs_codes']\n",
    "        self.max_length_icu_12h = max_lengths_dict['icu_12h_info_codes']\n",
    "        # layers of the network\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.lstm_1 = nn.LSTM(input_size=self.embedding_size,\n",
    "                              hidden_size=self.H,\n",
    "                              num_layers=1,\n",
    "                              batch_first=True,\n",
    "                              bidirectional=False)\n",
    "\n",
    "        self.fc_1 = nn.Linear(self.embedding_size, 256)\n",
    "        self.fc_2 = nn.Linear(256* (self.max_length_demographics + self.max_length_previous_diags) + self.H * (self.max_length_labs_24h + self.max_length_icu_12h * 2) , 4096)\n",
    "        self.lstm_2 = nn.LSTM(input_size=4096,\n",
    "                              hidden_size=self.H,\n",
    "                              num_layers=1,\n",
    "                              batch_first=True,\n",
    "                              bidirectional=True)\n",
    "        self.fc_3 = nn.Linear(self.H*2, 4)\n",
    "        self.drop = nn.Dropout(p=self.p)\n",
    "\n",
    "    def forward(self, tensor_12h_info, tensor_24h_labs, tensor_diagnoses, tensor_demographics):\n",
    "\n",
    "        out_emb_demo = self.embedding(tensor_demographics) # batch_size x max_length_demographics x embedding_size\n",
    "        # print(f'out_emb_demo: ', out_emb_demo.size())\n",
    "        out_emb_diags = self.embedding(tensor_diagnoses) # batch_size x max_length_previous_diags x embedding_size\n",
    "        # print(f'out_emb_diags: ', out_emb_diags.size())\n",
    "        out_emb_24h = self.embedding(tensor_24h_labs.squeeze(1))    # batch_size x max_length_labs_24h x embedding_size\n",
    "        # print(f'out_emb_24h: ', out_emb_24h.size())\n",
    "        out_emb_12h_1 = self.embedding(tensor_12h_info[:,0,:])\n",
    "        out_emb_12h_2 = self.embedding(tensor_12h_info[:,1,:])\n",
    "        # print('out_emb_12h_1', out_emb_12h_1.size(), 'out_emb_12h_2', out_emb_12h_2.size())\n",
    "        out_emb_12h = torch.cat([ out_emb_12h_1, out_emb_12h_2], dim=1)\n",
    "        # print(f'out_emb_12h: ', out_emb_12h.size())\n",
    "\n",
    "        out_static = self.fc_1(torch.cat([out_emb_demo, out_emb_diags], dim=1))\n",
    "        # print(f'\\nout_static: ', out_static.size())\n",
    "        out_static = out_static.reshape(out_static.size(0), out_static.size(1)*out_static.size(2))\n",
    "        # print(f'out_static: ', out_static.size())\n",
    "\n",
    "        out_lstm_1_24h, (hn, cn) = self.lstm_1(out_emb_24h)         # batch_size x max_length_labs_24h x H\n",
    "        # print(f'\\nout_lstm_1_24h: ', out_lstm_1_24h.size())\n",
    "        out_lstm_1_24h = out_lstm_1_24h.reshape(out_lstm_1_24h.size(0), out_lstm_1_24h.size(1)*out_lstm_1_24h.size(2))   # batch_size x max_length_labs_24h * H\n",
    "        # print(f'out_lstm_1_24h: ', out_lstm_1_24h.size())\n",
    "\n",
    "        out_lstm_1_12h, (hn, cn) = self.lstm_1(out_emb_12h)\n",
    "        # print(f'\\nout_lstm_1_12h: ', out_lstm_1_12h.size())\n",
    "        out_lstm_1_12h = out_lstm_1_12h.reshape(out_lstm_1_12h.size(0), out_lstm_1_12h.size(1)*out_lstm_1_12h.size(2))\n",
    "        # print(f'out_lstm_1_12h: ', out_lstm_1_12h.size())\n",
    "        full_output = torch.cat([out_static, out_lstm_1_24h, out_lstm_1_12h], dim=1)\n",
    "        # print(f'\\nfull_output: ', full_output.size())\n",
    "        out_fc_2 = self.fc_2(full_output)\n",
    "        # print(f'out_fc_2: ', out_fc_2.size())\n",
    "        out_lstm_2, (hn, cn) = self.lstm_2(out_fc_2)\n",
    "        # print(f'out_lstm_2: ', out_lstm_2.size())\n",
    "        out_lstm_2 = self.drop(out_lstm_2)\n",
    "        # print(f'out_lstm_2: ', out_lstm_2.size())\n",
    "        out_fc_3 = torch.squeeze(self.fc_3(out_lstm_2), 1)\n",
    "        # print(f'out_fc_3: ', out_fc_3.size())\n",
    "\n",
    "        return out_fc_3\n",
    "\n",
    "model1 = EHR_MODEL(max_lengths_dict, vocab_size=tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHR_MODEL(nn.Module):\n",
    "    def __init__(self, max_length, vocab_size, pred_window=2, observing_window=2,  H=128, embedding_size=200, drop=0.6):\n",
    "        super(EHR_MODEL, self).__init__()\n",
    "\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.H = H\n",
    "        self.p = drop\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.max_length_demo_diags = 50\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # layers of the network\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_size,\n",
    "                              hidden_size=self.H,\n",
    "                              num_layers=1,\n",
    "                              batch_first=True,\n",
    "                              bidirectional=True)\n",
    "\n",
    "        self.fc_1 = nn.Linear(2 * self.H * (self.max_length_demo_diags + self.max_length) , 2048)\n",
    "        self.fc_2 = nn.Linear(2048, 3)\n",
    "        self.drop = nn.Dropout(p=self.p)\n",
    "\n",
    "    def forward(self, tensor_day_info, tensor_demo_diags):\n",
    "\n",
    "        out_emb_demo_diags = torch.squeeze(self.embedding(tensor_demo_diags)) # batch_size x max_length_demographics x embedding_size\n",
    "        out_emb_day_info = torch.squeeze(self.embedding(tensor_day_info)) # batch_size x max_length_previous_diags x embedding_size\n",
    "        # print('out_emb_demo_diags: ', out_emb_demo_diags.size())\n",
    "        # print('out_emb_day_info: ', out_emb_day_info.size())\n",
    "        embedded = torch.cat([out_emb_demo_diags, out_emb_day_info], dim=1)\n",
    "\n",
    "        out_lstm, _ = self.lstm(embedded)\n",
    "        out_lstm = out_lstm.reshape(out_lstm.size(0), out_lstm.size(1)*out_lstm.size(2))\n",
    "        # print('out_lstm', out_lstm.size())\n",
    "\n",
    "        output = self.fc_1(out_lstm)\n",
    "        output = self.drop(output)\n",
    "        output = self.fc_2(output)\n",
    "        output = torch.squeeze(output, 1)\n",
    "\n",
    "        return output\n",
    "# class EHR_MODEL(nn.Module):\n",
    "#     def __init__(self, max_length, vocab_size, pred_window=2, observing_window=2,  H=128, embedding_size=200, drop=0.6):\n",
    "#         super(EHR_MODEL, self).__init__()\n",
    "\n",
    "#         self.observing_window = observing_window\n",
    "#         self.pred_window = pred_window\n",
    "#         self.H = H\n",
    "#         self.p = drop\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.embedding_size = embedding_size\n",
    "#         self.max_length_demo_diags = 50\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#         # layers of the network\n",
    "#         self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "#         self.lstm = nn.LSTM(input_size=self.embedding_size,\n",
    "#                               hidden_size=self.H,\n",
    "#                               num_layers=1,\n",
    "#                               batch_first=True,\n",
    "#                               bidirectional=True)\n",
    "\n",
    "#         self.fc_1 = nn.Linear(2 * self.H * (self.max_length_demo_diags + self.max_length) , 2048)\n",
    "#         self.fc_2 = nn.Linear(2048, 4)\n",
    "#         self.drop = nn.Dropout(p=self.p)\n",
    "\n",
    "#     def forward(self, tensor_day_info, tensor_demo_diags):\n",
    "\n",
    "#         out_emb_demo_diags = self.embedding(tensor_demo_diags) # batch_size x max_length_demographics x embedding_size\n",
    "#         out_emb_day_info = torch.squeeze(self.embedding(tensor_day_info), 1) # batch_size x max_length_previous_diags x embedding_size\n",
    "#         embedded = torch.cat([out_emb_demo_diags, out_emb_day_info], dim=1)\n",
    "\n",
    "#         out_lstm, _ = self.lstm(embedded)\n",
    "#         out_lstm = out_lstm.reshape(out_lstm.size(0), out_lstm.size(1)*out_lstm.size(2))\n",
    "\n",
    "#         output = self.fc_1(out_lstm)\n",
    "#         output = self.drop(output)\n",
    "#         output = self.fc_2(output)\n",
    "#         output = torch.squeeze(output, 1)\n",
    "\n",
    "#         return output\n",
    "\n",
    "model2 = EHR_MODEL(max_length=350, vocab_size=tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 8\n",
    "# test_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/test_data/'\n",
    "# train_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/train_data/'\n",
    "# val_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/val_data/'\n",
    "\n",
    "# test_dataset = MyDataset(data_path=test_data_path, labels_df=aki_stage_labels, tokenizer=tokenizer, max_length=350)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# train_dataset = MyDataset(data_path=train_data_path, labels_df=aki_stage_labels,  tokenizer=tokenizer, max_length=350)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "test_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/test_data/'\n",
    "train_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/train_data/'\n",
    "val_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/val_data/'\n",
    "\n",
    "test_dataset = MyDataset(data_path=test_data_path, labels_df=None, tokenizer=tokenizer, max_length=350)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "train_dataset = MyDataset(data_path=train_data_path, labels_df=None, tokenizer=tokenizer, max_length=350)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "device = 'cpu'\n",
    "\n",
    "# tensor_12h_info, tensor_24h_labs, tensor_diagnoses, tensor_demographics, tensor_labels, cash = next(iter(train_loader))\n",
    "tensor_day_info, tensor_demo_diags, tensor_labels, cash = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 350])\n",
      "torch.Size([8, 50])\n",
      "torch.Size([8, 3])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_day_info.size())\n",
    "print(tensor_demo_diags.size())\n",
    "print(tensor_labels.size())\n",
    "# print(tensor_12h_info.size())\n",
    "# print(tensor_24h_labs.size())\n",
    "# print(tensor_diagnoses.size())\n",
    "# print(tensor_demographics.size())\n",
    "# print(tensor_labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation_fn = nn.Softmax(dim=1)\n",
    "activation_fn = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_emb_demo_diags:  torch.Size([8, 50, 200])\n",
      "out_emb_day_info:  torch.Size([8, 350, 200])\n",
      "out_lstm torch.Size([8, 102400])\n"
     ]
    }
   ],
   "source": [
    "# output = model1(tensor_12h_info, tensor_24h_labs, tensor_diagnoses, tensor_demographics)\n",
    "output = model2(tensor_day_info, tensor_demo_diags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, '')\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211,819,515\n"
     ]
    }
   ],
   "source": [
    "print(f'{count_parameters(model):n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357,583,867\n"
     ]
    }
   ],
   "source": [
    "print(f'{count_parameters(model1):n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5293, 0.4941, 0.5273, 0.5194],\n",
       "        [0.4968, 0.5552, 0.5036, 0.4822],\n",
       "        [0.5633, 0.5363, 0.4990, 0.5490],\n",
       "        [0.4940, 0.5246, 0.5194, 0.5008],\n",
       "        [0.5242, 0.4845, 0.5240, 0.5098],\n",
       "        [0.4833, 0.4792, 0.4905, 0.4945],\n",
       "        [0.5498, 0.4923, 0.4681, 0.5074],\n",
       "        [0.4885, 0.4704, 0.4921, 0.4686]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Sigmoid()(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2617, 0.2272, 0.2596, 0.2515],\n",
       "        [0.2362, 0.2985, 0.2426, 0.2227],\n",
       "        [0.2768, 0.2482, 0.2137, 0.2612],\n",
       "        [0.2345, 0.2650, 0.2596, 0.2409],\n",
       "        [0.2634, 0.2247, 0.2632, 0.2487],\n",
       "        [0.2464, 0.2423, 0.2536, 0.2577],\n",
       "        [0.2978, 0.2365, 0.2146, 0.2512],\n",
       "        [0.2585, 0.2405, 0.2623, 0.2387]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_fn(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, \n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        file_path,\n",
    "        device='cuda',\n",
    "        num_epochs=1,\n",
    "        criterion = 'BCELoss',\n",
    "        pos_weight = torch.tensor([]),\n",
    "        best_valid_loss = float(\"Inf\"),\n",
    "        dimension=128,\n",
    "        epoch_patience=15,\n",
    "        threshold=None,\n",
    "        scheduler=None):\n",
    "\n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    valid_running_acc = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    train_acc_list = []\n",
    "    valid_acc_list = []\n",
    "    global_steps_list = []\n",
    "    stop_training = 0\n",
    "\n",
    "    # activation_fn = nn.Softmax(dim=1)\n",
    "    activation_fn = nn.Sigmoid()\n",
    "\n",
    "    if criterion == 'BCEWithLogitsLoss':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        criterion.pos_weight = pos_weight.to(device)\n",
    "        use_sigmoid=False\n",
    "    else:\n",
    "        criterion = nn.BCELoss()\n",
    "        use_sigmoid = True\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):  \n",
    "\n",
    "        model.train()\n",
    "        for tensor_12h_info, tensor_24h_labs, tensor_diagnoses, tensor_demographics, tensor_labels, _  in train_loader:\n",
    "            # transferring everything to GPU\n",
    "            tensor_labels = tensor_labels.to(device)\n",
    "            tensor_12h_info = tensor_12h_info.to(device)\n",
    "            tensor_24h_labs = tensor_24h_labs.to(device)\n",
    "            tensor_diagnoses = tensor_diagnoses.to(device)\n",
    "            tensor_demographics = tensor_demographics.to(device)\n",
    "\n",
    "            output = model(tensor_12h_info, tensor_24h_labs, tensor_diagnoses, tensor_demographics)\n",
    "\n",
    "            if use_sigmoid:\n",
    "                loss = criterion(activation_fn(output), tensor_labels.type(torch.float32))\n",
    "            else:\n",
    "                loss = criterion(output, tensor_labels.type(torch.float32))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()            \n",
    "            global_step += 1\n",
    "            # wandb.log({'step_train_loss': loss.item(), 'global_step': global_step})\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            print(f'Learning rate is {get_lr(optimizer)}')\n",
    "\n",
    "        model.eval()\n",
    "        stacked_labels = torch.tensor([]).to(device)\n",
    "        stacked_probs = torch.tensor([]).to(device)\n",
    "        with torch.no_grad():\n",
    "            # validation loop\n",
    "            for tensor_12h_info, tensor_24h_labs, tensor_diagnoses, tensor_demographics, tensor_labels, _  in valid_loader:\n",
    "                tensor_labels = tensor_labels.to(device)\n",
    "                tensor_12h_info = tensor_12h_info.to(device)\n",
    "                tensor_24h_labs = tensor_24h_labs.to(device)\n",
    "                tensor_diagnoses = tensor_diagnoses.to(device)\n",
    "                tensor_demographics = tensor_demographics.to(device)\n",
    "\n",
    "                output = model(tensor_12h_info, tensor_24h_labs, tensor_diagnoses, tensor_demographics)\n",
    "\n",
    "                if use_sigmoid:\n",
    "                    loss = criterion(activation_fn(output), tensor_labels.type(torch.float32))\n",
    "                else:\n",
    "                    loss = criterion(output, tensor_labels.type(torch.float32))\n",
    "\n",
    "                valid_running_loss += loss.item()\n",
    "                probs = activation_fn(output)\n",
    "                # stacking labels and predictions\n",
    "                stacked_labels = torch.cat([stacked_labels, tensor_labels], dim=0)\n",
    "                stacked_probs = torch.cat([stacked_probs, probs], dim=0, )\n",
    "\n",
    "        # transfer to device\n",
    "        stacked_labels = stacked_labels.cpu().detach().numpy()\n",
    "        stacked_probs = stacked_probs.cpu().detach().numpy()\n",
    "        # valid loss\n",
    "        epoch_average_train_loss = running_loss / len(train_loader)  \n",
    "        epoch_average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "\n",
    "        train_loss_list.append(epoch_average_train_loss)\n",
    "        valid_loss_list.append(epoch_average_valid_loss)\n",
    "\n",
    "        stages = ['AKI_1', 'AKI_2', 'AKI_3']\n",
    "        for w in range(stacked_labels.ndim):\n",
    "            stage = stages[w]\n",
    "            precision, recall, thresholds = precision_recall_curve(stacked_labels.T[w], stacked_probs.T[w])\n",
    "            precision, recall, thresholds = np.round(precision, 2), np.round(recall,2), np.round(thresholds,2)\n",
    "            \n",
    "            # convert to f score\n",
    "            fscore = np.round((2 * precision * recall) / (precision + recall), 2)\n",
    "            # locate the index of the largest f score\n",
    "            ix = np.argmax(np.nan_to_num(fscore))\n",
    "            threshold = np.round(thresholds[ix], 2)\n",
    "            stacked_preds = (stacked_probs.T[w] > threshold).astype(int)\n",
    "            y_true = stacked_labels.T[0]\n",
    "            y_pred = stacked_preds\n",
    "            f1_score_ = np.round(f1_score(y_true, y_pred, pos_label=1, average='binary', zero_division=0), 2)\n",
    "            recall_score_ = np.round(recall_score(y_true, y_pred, pos_label=1, average='binary', zero_division=0), 2)\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "            specificity =  np.round(tn / (tn + fp), 2)\n",
    "            pr_auc = np.round(auc(recall, precision), 2)\n",
    "            # wandb.log({'val_f1_score_'+stage: f1_score_, 'val_recall_score_'+stage:recall_score_, \\\n",
    "            #             'val_specificity'+stage:specificity, 'val_pr_auc'+stage:pr_auc,\\\n",
    "            #                 'epoch': epoch+1})\n",
    "\n",
    "        global_steps_list.append(global_step)\n",
    "        # wandb.log({'epoch_average_train_loss': epoch_average_train_loss,\n",
    "        #             'epoch_average_valid_loss': epoch_average_valid_loss,\n",
    "        #             'epoch': epoch+1})\n",
    "\n",
    "        # resetting running values\n",
    "        running_loss = 0.0                \n",
    "        valid_running_loss = 0.0\n",
    "        \n",
    "        # print progress\n",
    "        print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                        epoch_average_train_loss, epoch_average_valid_loss))      \n",
    "\n",
    "        # checkpoint\n",
    "        if best_valid_loss > epoch_average_valid_loss:\n",
    "            best_valid_loss = epoch_average_valid_loss\n",
    "            save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)\n",
    "            stop_training = 0\n",
    "        else:\n",
    "            stop_training +=1\n",
    "        \n",
    "        if stop_training == epoch_patience:\n",
    "            break\n",
    "\n",
    "\n",
    "# save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Finished Training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "device = 'cuda'\n",
    "# model = EHR_MODEL(max_lengths_dict, vocab_size=tokenizer.get_vocab_size()).to(device)\n",
    "optimizer = optim.Adam(model2.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[  0, 145, 356,  ...,   2,   2,   2],\n",
       "          [  0, 145, 351,  ...,   2,   2,   2]],\n",
       " \n",
       "         [[  0, 145, 434,  ...,   2,   2,   2],\n",
       "          [  0, 145, 186,  ...,   2,   2,   2]],\n",
       " \n",
       "         [[  0, 145, 106,  ...,   2,   2,   2],\n",
       "          [  0, 145, 106,  ...,   2,   2,   2]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  0, 145, 239,  ...,   2,   2,   2],\n",
       "          [  0, 145, 291,  ...,   2,   2,   2]],\n",
       " \n",
       "         [[  0, 145, 108,  ...,   2,   2,   2],\n",
       "          [  0, 145, 108,  ...,   2,   2,   2]],\n",
       " \n",
       "         [[  0, 145, 352,  ...,   2,   2,   2],\n",
       "          [  0, 145,  89,  ...,   2,   2,   2]]]),\n",
       " tensor([[[   0,  541,  618,  ...,    2,    2,    2]],\n",
       " \n",
       "         [[   0,  289,   87,  ...,    2,    2,    2]],\n",
       " \n",
       "         [[   0,  541, 1298,  ...,    2,    2,    2]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[   0,  289,   67,  ...,    2,    2,    2]],\n",
       " \n",
       "         [[   0,  289,   67,  ...,    2,    2,    2]],\n",
       " \n",
       "         [[   0,  289,   87,  ...,    2,    2,    2]]]),\n",
       " tensor([[   0,    1,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2],\n",
       "         [   0,    1,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2],\n",
       "         [   0,    1,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2],\n",
       "         [   0, 1458, 1039,  829, 1576, 1590, 1418, 2713,  477, 2239, 5045, 2254,\n",
       "          3170,  581, 2242, 1074, 1247, 3848,  905,  863,  911, 1140, 2563, 2402,\n",
       "          1558, 1001,  579,    1,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2],\n",
       "         [   0, 2785,  767, 1280,  608, 1953, 2157, 3089,  477, 2554,  678, 1657,\n",
       "          2098, 1195, 1026,  690,  579, 1369, 3745,  978, 1009, 1233, 1205,  831,\n",
       "          1205, 6306, 1657,  690,  911, 1312,  678,  608, 2934,  100, 1009,  579,\n",
       "          1025,  767, 1806, 5335,  477,  829, 2785, 1953,  767,  477,  579, 1657,\n",
       "          1205, 1381,    1,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2],\n",
       "         [   0,    1,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2],\n",
       "         [   0,    1,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2],\n",
       "         [   0,    1,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2]]),\n",
       " tensor([[   0,  320,   36,  152,    1,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2],\n",
       "         [   0,  320,   29,  167,  318,  106,  188,  165,  390,   11,   95,  247,\n",
       "           169,   10,   21,  238,  737,    1,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2],\n",
       "         [   0,  320,   29,  317,    1,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2],\n",
       "         [   0,  320,   36,  185,  188,  165,  405,   11,  408,    1,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2],\n",
       "         [   0,  320,   36,   65,    1,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2],\n",
       "         [   0,    3,   37, 1037,   36,   86,    1,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2],\n",
       "         [   0,  320,   29,  317,    1,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2],\n",
       "         [   0,  320,   29,  317,    1,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2,    2,    2,    2,    2,    2]]),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 0.]], dtype=torch.float64),\n",
       " {'stay_id': tensor([31560342, 30408393, 32284425, 35614689, 35029333, 33392165, 30900977,\n",
       "          31718370]),\n",
       "  'day_id': [tensor([0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([1, 1, 1, 1, 1, 1, 1, 1])],\n",
       "  'wind_id': [tensor([0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "   tensor([1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "   tensor([2, 2, 2, 2, 2, 2, 2, 2]),\n",
       "   tensor([3, 3, 3, 3, 3, 3, 3, 3])]}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22940/3604715386.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/test_model'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         device='cuda')\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_22940/1776422837.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, valid_loader, file_path, device, num_epochs, criterion, pos_weight, best_valid_loss, dimension, epoch_patience, threshold, scheduler)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtensor_12h_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_24h_labs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_diagnoses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_demographics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0;31m# transferring everything to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mtensor_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 4)"
     ]
    }
   ],
   "source": [
    "train(model1.to(device), \n",
    "        optimizer,\n",
    "        test_loader,\n",
    "        val_loader,\n",
    "        file_path='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/test_model',\n",
    "        device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, \n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        file_path,\n",
    "        device='cuda',\n",
    "        num_epochs=1,\n",
    "        criterion = 'BCELoss',\n",
    "        pos_weight = torch.tensor([]),\n",
    "        best_valid_loss = float(\"Inf\"),\n",
    "        dimension=128,\n",
    "        epoch_patience=15,\n",
    "        threshold=None,\n",
    "        scheduler=None):\n",
    "\n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    valid_running_acc = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    train_acc_list = []\n",
    "    valid_acc_list = []\n",
    "    global_steps_list = []\n",
    "    stop_training = 0\n",
    "\n",
    "    activation_fn = nn.Sigmoid()\n",
    "    # activation_fn = nn.Softmax(dim=1)\n",
    "\n",
    "    if criterion == 'BCEWithLogitsLoss':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        criterion.pos_weight = pos_weight.to(device)\n",
    "        use_sigmoid=False\n",
    "    else:\n",
    "        criterion = nn.BCELoss()\n",
    "        use_sigmoid = True\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):  \n",
    "\n",
    "        model.train()\n",
    "        for tensor_day_info, tensor_demo_diags, tensor_labels, _  in train_loader:\n",
    "            # transferring everything to GPU\n",
    "            tensor_labels = tensor_labels.to(device)\n",
    "            tensor_day_info = tensor_day_info.to(device)\n",
    "            tensor_demo_diags = tensor_demo_diags.to(device)\n",
    "            if global_step % 10 == 0 :\n",
    "                print(f'Step {global_step}/{len(train_loader)}')\n",
    "\n",
    "            output = model(tensor_day_info, tensor_demo_diags)\n",
    "\n",
    "            if use_sigmoid:\n",
    "                loss = criterion(activation_fn(output), tensor_labels.type(torch.float32))\n",
    "            else:\n",
    "                loss = criterion(output, tensor_labels.type(torch.float32))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()            \n",
    "            global_step += 1\n",
    "\n",
    "            # wandb.log({'step_train_loss': loss.item(), 'global_step': global_step})\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            print(f'Learning rate is {get_lr(optimizer)}')\n",
    "\n",
    "        model.eval()\n",
    "        stacked_labels = torch.tensor([]).to(device)\n",
    "        stacked_probs = torch.tensor([]).to(device)\n",
    "        with torch.no_grad():\n",
    "            # validation loop\n",
    "            for tensor_day_info, tensor_demo_diags, tensor_labels,  _  in valid_loader:\n",
    "                tensor_labels = tensor_labels.to(device)\n",
    "                tensor_day_info = tensor_day_info.to(device)\n",
    "                tensor_demo_diags = tensor_demo_diags.to(device)\n",
    "\n",
    "                output = model(tensor_day_info, tensor_demo_diags)\n",
    "\n",
    "                if use_sigmoid:\n",
    "                    loss = criterion(activation_fn(output), tensor_labels.type(torch.float32))\n",
    "                else:\n",
    "                    loss = criterion(output, tensor_labels.type(torch.float32))\n",
    "\n",
    "                valid_running_loss += loss.item()\n",
    "                probs = activation_fn(output)\n",
    "                # stacking labels and predictions\n",
    "                stacked_labels = torch.cat([stacked_labels, tensor_labels], dim=0)\n",
    "                stacked_probs = torch.cat([stacked_probs, probs], dim=0, )\n",
    "\n",
    "        # transfer to device\n",
    "        stacked_labels = stacked_labels.cpu().detach().numpy()\n",
    "        stacked_probs = stacked_probs.cpu().detach().numpy()\n",
    "        # valid loss\n",
    "        epoch_average_train_loss = running_loss / len(train_loader)  \n",
    "        epoch_average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "\n",
    "        train_loss_list.append(epoch_average_train_loss)\n",
    "        valid_loss_list.append(epoch_average_valid_loss)\n",
    "\n",
    "        stages = ['ANY', '2|3', '3']\n",
    "        for w in range(len(stages)):\n",
    "            stage = stages[w]\n",
    "\n",
    "\n",
    "            precision, recall, thresholds = precision_recall_curve(stacked_labels.T[w], stacked_probs.T[w])\n",
    "            precision, recall, thresholds = np.round(precision, 2), np.round(recall,2), np.round(thresholds,2)\n",
    "            \n",
    "            # convert to f score\n",
    "            fscore = np.round((2 * precision * recall) / (precision + recall + 0.000001), 2)\n",
    "            # locate the index of the largest f score\n",
    "            ix = np.argmax(np.nan_to_num(fscore))\n",
    "            threshold = np.round(thresholds[ix], 2)\n",
    "\n",
    "            stacked_preds = (stacked_probs.T[w] > threshold).astype(int)\n",
    "            y_true = stacked_labels.T[w]\n",
    "            y_pred = stacked_preds\n",
    "\n",
    "            f1_score_ = np.round(f1_score(y_true, y_pred, pos_label=1, average='binary', zero_division=0), 2)\n",
    "            recall_score_ = np.round(recall_score(y_true, y_pred, pos_label=1, average='binary', zero_division=0), 2)\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "            specificity =  np.round(tn / (tn + fp), 2)\n",
    "            pr_auc = np.round(auc(recall, precision), 2)\n",
    "            # wandb.log({'val_f1_score_'+stage: f1_score_, 'val_recall_score_'+stage:recall_score_, \\\n",
    "            #             'val_specificity'+stage:specificity, 'val_pr_auc'+stage:pr_auc,\\\n",
    "            #                 'epoch': epoch+1})\n",
    "\n",
    "        global_steps_list.append(global_step)\n",
    "        # wandb.log({'epoch_average_train_loss': epoch_average_train_loss,\n",
    "        #             'epoch_average_valid_loss': epoch_average_valid_loss,\n",
    "        #             'epoch': epoch+1})\n",
    "\n",
    "        # resetting running values\n",
    "        running_loss = 0.0                \n",
    "        valid_running_loss = 0.0\n",
    "        \n",
    "        # print progress\n",
    "        print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                        epoch_average_train_loss, epoch_average_valid_loss))      \n",
    "\n",
    "        # checkpoint\n",
    "        if best_valid_loss > epoch_average_valid_loss:\n",
    "            best_valid_loss = epoch_average_valid_loss\n",
    "            save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)\n",
    "            stop_training = 0\n",
    "        else:\n",
    "            stop_training +=1\n",
    "        \n",
    "        if stop_training == epoch_patience:\n",
    "            break\n",
    "\n",
    "\n",
    "# save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Finished Training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "device = 'cuda'\n",
    "# model = \n",
    "optimizer = optim.Adam(model2.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/179\n",
      "Step 10/179\n",
      "Step 20/179\n",
      "Step 30/179\n",
      "Step 40/179\n",
      "Step 50/179\n",
      "Step 60/179\n",
      "Step 70/179\n",
      "Step 80/179\n",
      "Step 90/179\n",
      "Step 100/179\n",
      "Step 110/179\n",
      "Step 120/179\n",
      "Step 130/179\n",
      "Step 140/179\n",
      "Step 150/179\n",
      "Step 160/179\n",
      "Step 170/179\n",
      "Epoch [1/1], Step [179/179], Train Loss: 0.1473, Valid Loss: 0.1803\n",
      "Model saved to ==> /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/test_model/model.pt\n",
      "Finished Training!\n"
     ]
    }
   ],
   "source": [
    "train(model2.to(device), \n",
    "        optimizer,\n",
    "        test_loader,\n",
    "        val_loader,\n",
    "        file_path='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/test_model',\n",
    "        device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, threshold=None, log_res=True):\n",
    "    print('Evaluation..')\n",
    "    def find_nearest(array, value):\n",
    "        array = np.asarray(array)\n",
    "        idx = (np.abs(array - value)).argmin()\n",
    "        return idx\n",
    "\n",
    "    device = 'cpu'\n",
    "    model = model.to(device)\n",
    "    stacked_labels = torch.tensor([]).to(device)\n",
    "    stacked_probs = torch.tensor([]).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    step = 1\n",
    "    with torch.no_grad():\n",
    "        for  tensor_12h_info, tensor_24h_labs, tensor_diagnoses, tensor_demographics, tensor_labels, _  in test_loader:\n",
    "            if step % 100==0:\n",
    "                print(f'Step {step}/{len(test_loader)}' )\n",
    "\n",
    "            tensor_12h_info = tensor_12h_info.to(device)\n",
    "            tensor_24h_labs = tensor_24h_labs.to(device)\n",
    "            tensor_diagnoses = tensor_diagnoses.to(device)\n",
    "            tensor_demographics = tensor_demographics.to(device)\n",
    "            tensor_labels = tensor_labels.to(device)\n",
    "\n",
    "            probs = model(tensor_12h_info, tensor_24h_labs, tensor_diagnoses, tensor_demographics)\n",
    "            probs = nn.Sigmoid()(probs)\n",
    "            # output = (probs > threshold).int()\n",
    "\n",
    "            # stacking labels and predictions\n",
    "            stacked_labels = torch.cat([stacked_labels, tensor_labels], dim=0, )\n",
    "            # stacked_preds = torch.cat([stacked_preds, output], dim=0, )\n",
    "            stacked_probs = torch.cat([stacked_probs, probs], dim=0, )\n",
    "            step += 1\n",
    "            \n",
    "    # transfer to device\n",
    "    stacked_labels = stacked_labels.cpu().detach().numpy()\n",
    "    stacked_probs = stacked_probs.cpu().detach().numpy()\n",
    "    # for printing purposes\n",
    "    stages_names = ['1', '2', '3', 'NO_AKI']\n",
    "    if threshold==None:\n",
    "        for w in range(len(stages_names)):\n",
    "            print('------------- AKI stage ', stages_names[w], '------------- ')\n",
    "            labels = stacked_labels.T[w]\n",
    "            probs = stacked_probs.T[w]            \n",
    "\n",
    "            precision, recall, thresholds = precision_recall_curve(labels, probs)\n",
    "            precision, recall, thresholds = np.round(precision, 2), np.round(recall,2), np.round(thresholds,2)\n",
    "            \n",
    "            # convert to f score\n",
    "            fscore = np.round((2 * precision * recall) / (precision + recall), 2)\n",
    "\n",
    "            # locate the index of the largest f score\n",
    "            ix = np.argmax(np.nan_to_num(fscore))\n",
    "            threshold = np.round(thresholds[ix], 2)\n",
    "            print('Best Threshold=%.2f, F-Score=%.2f' % (threshold, fscore[ix]))\n",
    "\n",
    "            y_true = labels\n",
    "            y_pred = (probs > threshold).astype(int)\n",
    "\n",
    "            accuracy = np.round(accuracy_score(y_true, y_pred), 2)\n",
    "            print(f'Accuracy: {accuracy}')\n",
    "\n",
    "            f1_score_ = np.round(f1_score(y_true, y_pred, pos_label=1, average='binary', zero_division=0), 2)\n",
    "            print(f'F1: ', f1_score_)\n",
    "\n",
    "            recall_score_ = np.round(recall_score(y_true, y_pred, pos_label=1, average='binary', zero_division=0), 2)\n",
    "            print(f'Sensitivity: ', recall_score_)\n",
    "\n",
    "            precision_score_ = np.round(precision_score(y_true, y_pred, pos_label=1, average='binary', zero_division=0), 2)\n",
    "            print(f'Precision: ', precision_score_)\n",
    "\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "            specificity =  np.round(tn / (tn + fp), 2)\n",
    "            print(f'Specificity: ', specificity)\n",
    "\n",
    "            pr_auc = np.round(auc(recall, precision), 2) \n",
    "            print(f'PR AUC: ', pr_auc)\n",
    "\n",
    "            roc_auc = np.round(roc_auc_score(y_true, y_pred), 2)\n",
    "            print(f'ROC AUC: ', roc_auc)\n",
    "            # confusion matrix\n",
    "            print(f'Confusion matrix:\\n', confusion_matrix(y_true, y_pred))\n",
    "            # get classification metrics for all samples in the test set\n",
    "            classification_report_res = classification_report(y_true, y_pred, zero_division=0, output_dict=True)\n",
    "            print(classification_report(y_true, y_pred, zero_division=0, output_dict=False))\n",
    "            # operating points \n",
    "            precision_list = [0.2, 0.25, 0.33, 0.4, 0.5, 0.6, 0.75]\n",
    "            for p in precision_list:\n",
    "                idx = find_nearest(precision, p)\n",
    "                sensitivity = recall[idx]\n",
    "                print(f'Precision {np.round(precision[idx]*100, 1)}% , Sensitivity {sensitivity} ')\n",
    "\n",
    "            if log_res:\n",
    "                wandb.log({'test_accuracy'+stages_names[w] :accuracy, 'test_f1_score'+stages_names[w]:f1_score_, \\\n",
    "                            'test_recall_score'+stages_names[w]:recall_score_, 'test_precision_score'+stages_names[w]:precision_score_, \\\n",
    "                                'test_specificity'+stages_names[w]:specificity})\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27549/3366565675.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtensor_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tensor_labels' is not defined"
     ]
    }
   ],
   "source": [
    "tensor_labels.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation..\n",
      "Step 100/179\n",
      "------------- AKI stage  1 ------------- \n",
      "Best Threshold=0.50, F-Score=0.14\n",
      "Accuracy: 0.39\n",
      "F1:  0.13\n",
      "Sensitivity:  0.64\n",
      "Precision:  0.07\n",
      "Specificity:  0.37\n",
      "PR AUC:  0.07\n",
      "ROC AUC:  0.51\n",
      "Confusion matrix:\n",
      " [[493 832]\n",
      " [ 37  66]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.37      0.53      1325\n",
      "         1.0       0.07      0.64      0.13       103\n",
      "\n",
      "    accuracy                           0.39      1428\n",
      "   macro avg       0.50      0.51      0.33      1428\n",
      "weighted avg       0.87      0.39      0.50      1428\n",
      "\n",
      "Precision 13.0% , Sensitivity 0.02 \n",
      "Precision 13.0% , Sensitivity 0.02 \n",
      "Precision 13.0% , Sensitivity 0.02 \n",
      "Precision 13.0% , Sensitivity 0.02 \n",
      "Precision 13.0% , Sensitivity 0.02 \n",
      "Precision 100.0% , Sensitivity 0.0 \n",
      "Precision 100.0% , Sensitivity 0.0 \n",
      "------------- AKI stage  2 ------------- \n",
      "Best Threshold=0.55, F-Score=0.11\n",
      "Accuracy: 0.88\n",
      "F1:  0.1\n",
      "Sensitivity:  0.21\n",
      "Precision:  0.06\n",
      "Specificity:  0.9\n",
      "PR AUC:  0.04\n",
      "ROC AUC:  0.56\n",
      "Confusion matrix:\n",
      " [[1248  138]\n",
      " [  33    9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.90      0.94      1386\n",
      "         1.0       0.06      0.21      0.10        42\n",
      "\n",
      "    accuracy                           0.88      1428\n",
      "   macro avg       0.52      0.56      0.52      1428\n",
      "weighted avg       0.95      0.88      0.91      1428\n",
      "\n",
      "Precision 11.0% , Sensitivity 0.02 \n",
      "Precision 11.0% , Sensitivity 0.02 \n",
      "Precision 11.0% , Sensitivity 0.02 \n",
      "Precision 11.0% , Sensitivity 0.02 \n",
      "Precision 11.0% , Sensitivity 0.02 \n",
      "Precision 100.0% , Sensitivity 0.0 \n",
      "Precision 100.0% , Sensitivity 0.0 \n",
      "------------- AKI stage  3 ------------- \n",
      "Best Threshold=0.49, F-Score=0.02\n",
      "Accuracy: 0.02\n",
      "F1:  0.02\n",
      "Sensitivity:  0.93\n",
      "Precision:  0.01\n",
      "Specificity:  0.01\n",
      "PR AUC:  0.01\n",
      "ROC AUC:  0.47\n",
      "Confusion matrix:\n",
      " [[   9 1404]\n",
      " [   1   14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.01      0.01      1413\n",
      "         1.0       0.01      0.93      0.02        15\n",
      "\n",
      "    accuracy                           0.02      1428\n",
      "   macro avg       0.45      0.47      0.02      1428\n",
      "weighted avg       0.89      0.02      0.01      1428\n",
      "\n",
      "Precision 1.0% , Sensitivity 1.0 \n",
      "Precision 1.0% , Sensitivity 1.0 \n",
      "Precision 1.0% , Sensitivity 1.0 \n",
      "Precision 1.0% , Sensitivity 1.0 \n",
      "Precision 1.0% , Sensitivity 1.0 \n",
      "Precision 100.0% , Sensitivity 0.0 \n",
      "Precision 100.0% , Sensitivity 0.0 \n",
      "------------- AKI stage  NO_AKI ------------- \n",
      "Best Threshold=0.43, F-Score=0.91\n",
      "Accuracy: 0.83\n",
      "F1:  0.91\n",
      "Sensitivity:  1.0\n",
      "Precision:  0.83\n",
      "Specificity:  0.0\n",
      "PR AUC:  0.83\n",
      "ROC AUC:  0.5\n",
      "Confusion matrix:\n",
      " [[   0  240]\n",
      " [   0 1188]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       240\n",
      "         1.0       0.83      1.00      0.91      1188\n",
      "\n",
      "    accuracy                           0.83      1428\n",
      "   macro avg       0.42      0.50      0.45      1428\n",
      "weighted avg       0.69      0.83      0.76      1428\n",
      "\n",
      "Precision 25.0% , Sensitivity 0.0 \n",
      "Precision 25.0% , Sensitivity 0.0 \n",
      "Precision 33.0% , Sensitivity 0.0 \n",
      "Precision 40.0% , Sensitivity 0.0 \n",
      "Precision 50.0% , Sensitivity 0.0 \n",
      "Precision 60.0% , Sensitivity 0.01 \n",
      "Precision 75.0% , Sensitivity 0.04 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svetlanamaslenkova/anaconda_envs/aki_env/lib/python3.7/site-packages/ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/svetlanamaslenkova/anaconda_envs/aki_env/lib/python3.7/site-packages/ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/svetlanamaslenkova/anaconda_envs/aki_env/lib/python3.7/site-packages/ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/svetlanamaslenkova/anaconda_envs/aki_env/lib/python3.7/site-packages/ipykernel_launcher.py:51: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "evaluate(model1, test_loader, log_res=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, threshold=None, log_res=True, activation_fn = nn.Sigmoid()):\n",
    "    print('Evaluation..')\n",
    "    def find_nearest(array, value):\n",
    "        array = np.asarray(array)\n",
    "        idx = (np.abs(array - value)).argmin()\n",
    "        return idx\n",
    "\n",
    "    device = 'cpu'\n",
    "    model = model.to(device)\n",
    "    stacked_labels = torch.tensor([]).to(device)\n",
    "    stacked_probs = torch.tensor([]).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    step = 1\n",
    "    with torch.no_grad():\n",
    "        for  tensor_day_info, tensor_demo_diags,  tensor_labels, _  in test_loader:\n",
    "            if step % 100==0:\n",
    "                print(f'Step {step}/{len(test_loader)}' )\n",
    "\n",
    "            tensor_day_info = tensor_day_info.to(device)\n",
    "            tensor_demo_diags = tensor_demo_diags.to(device)\n",
    "            tensor_labels = tensor_labels.to(device)\n",
    "\n",
    "            probs = model(tensor_day_info, tensor_demo_diags)\n",
    "            probs = activation_fn(probs)\n",
    "            # output = (probs > threshold).int()\n",
    "\n",
    "            # stacking labels and predictions\n",
    "            stacked_labels = torch.cat([stacked_labels, tensor_labels], dim=0, )\n",
    "            # stacked_preds = torch.cat([stacked_preds, output], dim=0, )\n",
    "            stacked_probs = torch.cat([stacked_probs, probs], dim=0, )\n",
    "            step += 1\n",
    "            # if step > 5:break\n",
    "    # transfer to device\n",
    "    stacked_labels = stacked_labels.cpu().detach().numpy()\n",
    "    stacked_probs = stacked_probs.cpu().detach().numpy()\n",
    "    # for printing purposes\n",
    "    stages_names = ['ANY', '2|3', '3']\n",
    "    if threshold is None:\n",
    "        for w in range(len(stages_names)):\n",
    "            print('------------- Stage ', stages_names[w], '------------- ')\n",
    "\n",
    "            precision, recall, thresholds = precision_recall_curve(stacked_labels.T[w], stacked_probs.T[w])\n",
    "            precision, recall, thresholds = np.round(precision, 2), np.round(recall,2), np.round(thresholds,2)\n",
    "            \n",
    "            # convert to f score\n",
    "            fscore = np.round((2 * precision * recall) / (precision + recall + 0.000001), 2)\n",
    "\n",
    "            # locate the index of the largest f score\n",
    "            ix = np.argmax(np.nan_to_num(fscore))\n",
    "            threshold = np.round(thresholds[ix], 2)\n",
    "            print('Best Threshold=%.2f, F-Score=%.2f' % (threshold, fscore[ix]))\n",
    "\n",
    "            stacked_preds = (stacked_probs.T[w] > threshold).astype(int)\n",
    "            y_true = stacked_labels.T[w]\n",
    "            y_pred = stacked_preds\n",
    "\n",
    "            accuracy = np.round(accuracy_score(y_true, y_pred), 2)\n",
    "            print(f'Accuracy: {accuracy}')\n",
    "\n",
    "            f1_score_ = np.round(f1_score(y_true, y_pred, pos_label=1, average='binary', zero_division=0), 2)\n",
    "            print(f'F1: ', f1_score_)\n",
    "\n",
    "            recall_score_ = np.round(recall_score(y_true, y_pred, pos_label=1, average='binary', zero_division=0), 2)\n",
    "            print(f'Sensitivity: ', recall_score_)\n",
    "\n",
    "            precision_score_ = np.round(precision_score(y_true, y_pred, pos_label=1, average='binary', zero_division=0), 2)\n",
    "            print(f'Precision: ', precision_score_)\n",
    "\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "            specificity =  np.round(tn / (tn + fp), 2)\n",
    "            print(f'Specificity: ', specificity)\n",
    "\n",
    "            pr_auc = np.round(auc(recall, precision), 2) \n",
    "            print(f'PR AUC: ', pr_auc)\n",
    "\n",
    "            roc_auc = np.round(roc_auc_score(y_true, y_pred), 2)\n",
    "            print(f'ROC AUC: ', roc_auc)\n",
    "            # confusion matrix\n",
    "            print(f'Confusion matrix:\\n', confusion_matrix(y_true, y_pred))\n",
    "            # get classification metrics for all samples in the test set\n",
    "            classification_report_res = classification_report(y_true, y_pred, zero_division=0, output_dict=True)\n",
    "            print(classification_report(y_true, y_pred, zero_division=0, output_dict=False))\n",
    "            # operating points \n",
    "            precision_list = [0.2, 0.25, 0.33, 0.4, 0.5, 0.6, 0.75]\n",
    "            for p in precision_list:\n",
    "                idx = find_nearest(precision, p)\n",
    "                sensitivity = recall[idx]\n",
    "                print(f'Precision {np.round(precision[idx]*100, 1)}% , Sensitivity {sensitivity} ')\n",
    "\n",
    "            if log_res:\n",
    "                wandb.log({'test_accuracy'+stages_names[w] :accuracy, 'test_f1_score'+stages_names[w]:f1_score_, \\\n",
    "                            'test_recall_score'+stages_names[w]:recall_score_, 'test_precision_score'+stages_names[w]:precision_score_, \\\n",
    "                                'test_specificity'+stages_names[w]:specificity})\n",
    "\n",
    "    return stacked_labels, stacked_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation..\n",
      "Step 100/179\n",
      "------------- Stage  ANY ------------- \n",
      "Best Threshold=0.46, F-Score=0.99\n",
      "Accuracy: 1.0\n",
      "F1:  0.99\n",
      "Sensitivity:  0.99\n",
      "Precision:  0.99\n",
      "Specificity:  1.0\n",
      "PR AUC:  0.99\n",
      "ROC AUC:  0.99\n",
      "Confusion matrix:\n",
      " [[1290    1]\n",
      " [   2  135]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      1291\n",
      "         1.0       0.99      0.99      0.99       137\n",
      "\n",
      "    accuracy                           1.00      1428\n",
      "   macro avg       1.00      0.99      0.99      1428\n",
      "weighted avg       1.00      1.00      1.00      1428\n",
      "\n",
      "Precision 39.0% , Sensitivity 0.99 \n",
      "Precision 39.0% , Sensitivity 0.99 \n",
      "Precision 39.0% , Sensitivity 0.99 \n",
      "Precision 40.0% , Sensitivity 1.0 \n",
      "Precision 50.0% , Sensitivity 0.99 \n",
      "Precision 60.0% , Sensitivity 0.99 \n",
      "Precision 75.0% , Sensitivity 0.99 \n",
      "------------- Stage  2|3 ------------- \n",
      "Best Threshold=0.24, F-Score=0.97\n",
      "Accuracy: 1.0\n",
      "F1:  0.97\n",
      "Sensitivity:  0.96\n",
      "Precision:  0.98\n",
      "Specificity:  1.0\n",
      "PR AUC:  0.99\n",
      "ROC AUC:  0.98\n",
      "Confusion matrix:\n",
      " [[1381    1]\n",
      " [   2   44]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      1382\n",
      "         1.0       0.98      0.96      0.97        46\n",
      "\n",
      "    accuracy                           1.00      1428\n",
      "   macro avg       0.99      0.98      0.98      1428\n",
      "weighted avg       1.00      1.00      1.00      1428\n",
      "\n",
      "Precision 45.0% , Sensitivity 1.0 \n",
      "Precision 45.0% , Sensitivity 1.0 \n",
      "Precision 45.0% , Sensitivity 1.0 \n",
      "Precision 45.0% , Sensitivity 1.0 \n",
      "Precision 50.0% , Sensitivity 0.98 \n",
      "Precision 60.0% , Sensitivity 0.98 \n",
      "Precision 75.0% , Sensitivity 0.98 \n",
      "------------- Stage  3 ------------- \n",
      "Best Threshold=0.78, F-Score=1.00\n",
      "Accuracy: 1.0\n",
      "F1:  1.0\n",
      "Sensitivity:  1.0\n",
      "Precision:  1.0\n",
      "Specificity:  1.0\n",
      "PR AUC:  1.0\n",
      "ROC AUC:  1.0\n",
      "Confusion matrix:\n",
      " [[1417    0]\n",
      " [   0   11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      1417\n",
      "         1.0       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00      1428\n",
      "   macro avg       1.00      1.00      1.00      1428\n",
      "weighted avg       1.00      1.00      1.00      1428\n",
      "\n",
      "Precision 100.0% , Sensitivity 1.0 \n",
      "Precision 100.0% , Sensitivity 1.0 \n",
      "Precision 100.0% , Sensitivity 1.0 \n",
      "Precision 100.0% , Sensitivity 1.0 \n",
      "Precision 100.0% , Sensitivity 1.0 \n",
      "Precision 100.0% , Sensitivity 1.0 \n",
      "Precision 100.0% , Sensitivity 1.0 \n"
     ]
    }
   ],
   "source": [
    "stacked_labels, stacked_probs = evaluate(model2, test_loader, threshold=None, log_res=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.29066033e-04, 1.07276705e-04, 1.10628468e-03],\n",
       "       [4.41620834e-02, 9.28481948e-03, 1.75114605e-04],\n",
       "       [9.95434701e-01, 9.99836564e-01, 8.12603813e-03],\n",
       "       [1.33489845e-02, 5.22824703e-04, 8.22989212e-04],\n",
       "       [2.59149037e-02, 2.90323771e-03, 2.41676625e-02]], dtype=float32)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_probs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57 0.56 0.57 0.57 0.57 0.57 0.57 0.57 0.57 0.57 0.57 0.58 0.58 0.58\n",
      " 0.58 0.58 0.58 0.58 0.58 0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.6\n",
      " 0.6  0.6  0.6  0.6  0.6  0.6  0.61 0.61 0.61 0.61 0.61 0.61 0.61 0.62\n",
      " 0.62 0.62 0.62 0.62 0.62 0.62 0.63 0.63 0.63 0.63 0.63 0.63 0.64 0.64\n",
      " 0.64 0.64 0.64 0.64 0.65 0.65 0.65 0.65 0.65 0.65 0.66 0.66 0.66 0.66\n",
      " 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.67 0.67 0.67 0.67 0.67 0.68 0.68\n",
      " 0.68 0.68 0.68 0.69 0.69 0.69 0.69 0.69 0.7  0.7  0.7  0.7  0.7  0.71\n",
      " 0.71 0.71 0.71 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.72 0.73 0.73\n",
      " 0.73 0.73 0.74 0.74 0.74 0.74 0.75 0.75 0.75 0.75 0.75 0.75 0.75 0.76\n",
      " 0.76 0.76 0.76 0.77 0.77 0.77 0.78 0.78 0.78 0.78 0.78 0.78 0.78 0.79\n",
      " 0.79 0.79 0.8  0.8  0.8  0.81 0.81 0.81 0.81 0.81 0.81 0.82 0.82 0.82\n",
      " 0.83 0.83 0.83 0.83 0.83 0.84 0.84 0.85 0.85 0.85 0.85 0.85 0.86 0.86\n",
      " 0.86 0.87 0.87 0.87 0.87 0.88 0.88 0.88 0.88 0.88 0.89 0.89 0.9  0.9\n",
      " 0.9  0.9  0.9  0.91 0.91 0.91 0.91 0.92 0.93 0.93 0.93 0.93 0.94 0.94\n",
      " 0.94 0.95 0.95 0.95 0.95 0.96 0.96 0.96 0.97 0.97 0.97 0.98 0.98 0.99\n",
      " 0.99 0.99 0.99 0.98 0.98 0.98 0.97 0.97 0.96 0.96 0.96 0.95 0.95 0.95\n",
      " 0.94 0.94 0.94 0.93 0.92 0.92 0.92 0.91 0.91 0.9  0.9  0.9  0.89 0.89\n",
      " 0.88 0.88 0.87 0.87 0.86 0.86 0.85 0.85 0.84 0.84 0.84 0.83 0.82 0.82\n",
      " 0.82 0.81 0.8  0.8  0.8  0.79 0.78 0.78 0.77 0.77 0.76 0.76 0.75 0.74\n",
      " 0.73 0.73 0.73 0.72 0.71 0.71 0.7  0.69 0.69 0.68 0.68 0.67 0.67 0.66\n",
      " 0.65 0.64 0.64 0.63 0.62 0.62 0.61 0.6  0.59 0.59 0.58 0.57 0.56 0.56\n",
      " 0.55 0.54 0.53 0.53 0.52 0.51 0.51 0.5  0.48 0.47 0.47 0.46 0.45 0.44\n",
      " 0.44 0.43 0.41 0.41 0.4  0.39 0.37 0.37 0.36 0.35 0.33 0.33 0.32 0.31\n",
      " 0.31 0.29 0.28 0.26 0.26 0.25 0.23 0.21 0.21 0.2  0.18 0.17 0.17 0.15\n",
      " 0.13 0.13 0.11 0.1  0.08 0.08 0.06 0.04 0.02 0.02 0.  ]\n"
     ]
    }
   ],
   "source": [
    "w = 0\n",
    "precision, recall, thresholds = precision_recall_curve(stacked_labels.T[w], stacked_probs.T[w])\n",
    "precision, recall, thresholds = np.round(precision, 2), np.round(recall,2), np.round(thresholds,2)\n",
    "\n",
    "# convert to f score\n",
    "fscore = np.round((2 * precision * recall) / (precision + recall + 0.000001), 2)\n",
    "print(fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3bc7ed3890>]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOzElEQVR4nO3df6jdd33H8eeribET+8MtV3RN2tuxlJk5we5SRUHLdJLmj4TpJskmWukMDOvG7ISKo0plDBUUZHEubkUttLH2D7nDuCBaKTjjcku1mNSWu1jbVLfc1trJSo2x7/1xjtvZ7b33fJN77j33fnw+4MI53++Hc95fzs2Tc7/n3nxTVUiS1r/zxj2AJGk0DLokNcKgS1IjDLokNcKgS1IjNo7riTdv3lyTk5PjenpJWpfuueeex6pqYqF9Ywv65OQkMzMz43p6SVqXknx/sX2ecpGkRhh0SWqEQZekRhh0SWqEQZekRgwNepJbkpxK8p1F9ifJx5PMJrkvyZWjH1OSNEyXd+ifBnYssf8aYFv/ax/w98sfS5J0toYGvaruBn60xJLdwGer5whwcZIXj2rAYZ55pvjX2cf48L98d7WeUpLWpFH8YdElwCMD90/2t/1w/sIk++i9i+fSSy9d1pPO/eSn3HnPST539GEeevwpLjx/I9e+apIXXnj+sh5XktarVf1L0ao6ABwAmJqaOqcra5z6r6d5//Qxvnz8PznzTO8hfv2i83nDb7+IW77+0MhmlaSVsGlDeOurJtn8/OeO/LFHEfRHga0D97f0t62I7/7HT7j7wTnOOy9sOi8APPbfp7nt3x5eqaeUpNEoOP3zZ3jRRb/CH79ieWcpFjKKoE8D1yc5CLwCeLKqnnW6ZVRec8UEx25e6jNaSVqbTv3kaa76m6/wzApd+nNo0JPcDlwNbE5yEng/8ByAqvokcAjYCcwCTwFvX5FJJUlLGhr0qto7ZH8B7xzZRJKkc+JfikpSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIwy6JDXCoEtSIzoFPcmOJA8kmU1y4wL7L01yV5J7k9yXZOfoR5UkLWVo0JNsAPYD1wDbgb1Jts9b9tfAHVX1cmAP8IlRDypJWlqXd+hXAbNVdaKqTgMHgd3z1hRwYf/2RcAPRjeiJKmLLkG/BHhk4P7J/rZBHwDekuQkcAh410IPlGRfkpkkM3Nzc+cwriRpMaP6UHQv8Omq2gLsBG5N8qzHrqoDVTVVVVMTExMjempJEnQL+qPA1oH7W/rbBl0H3AFQVd8Azgc2j2JASVI3XYJ+FNiW5PIkm+h96Dk9b83DwOsAkryEXtA9pyJJq2ho0KvqDHA9cBi4n95vsxxLcnOSXf1lNwDvSPJt4Hbg2qqqlRpakvRsG7ssqqpD9D7sHNx208Dt48CrRzuaJOls+JeiktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjegU9CQ7kjyQZDbJjYuseXOS40mOJblttGNKkobZOGxBkg3AfuD3gZPA0STTVXV8YM024L3Aq6vqiSQvXKmBJUkL6/IO/SpgtqpOVNVp4CCwe96adwD7q+oJgKo6NdoxJUnDdAn6JcAjA/dP9rcNugK4IsnXkxxJsmOhB0qyL8lMkpm5ublzm1iStKBRfSi6EdgGXA3sBT6V5OL5i6rqQFVNVdXUxMTEiJ5akgTdgv4osHXg/pb+tkEngemq+llVfQ94kF7gJUmrpEvQjwLbklyeZBOwB5iet+YL9N6dk2QzvVMwJ0Y3piRpmKFBr6ozwPXAYeB+4I6qOpbk5iS7+ssOA48nOQ7cBbynqh5fqaElSc829NcWAarqEHBo3rabBm4X8O7+lyRpDPxLUUlqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqRKegJ9mR5IEks0luXGLdm5JUkqnRjShJ6mJo0JNsAPYD1wDbgb1Jti+w7gLgL4BvjnpISdJwXd6hXwXMVtWJqjoNHAR2L7Dug8CHgKdHOJ8kqaMuQb8EeGTg/sn+tv+V5Epga1V9cakHSrIvyUySmbm5ubMeVpK0uGV/KJrkPOCjwA3D1lbVgaqaqqqpiYmJ5T61JGlAl6A/CmwduL+lv+0XLgBeCnwtyUPAK4FpPxiVpNXVJehHgW1JLk+yCdgDTP9iZ1U9WVWbq2qyqiaBI8CuqppZkYklSQsaGvSqOgNcDxwG7gfuqKpjSW5OsmulB5QkdbOxy6KqOgQcmrftpkXWXr38sSRJZ8u/FJWkRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWpEp6An2ZHkgSSzSW5cYP+7kxxPcl+SryS5bPSjSpKWMjToSTYA+4FrgO3A3iTb5y27F5iqqpcBdwIfHvWgkqSldXmHfhUwW1Unquo0cBDYPbigqu6qqqf6d48AW0Y7piRpmC5BvwR4ZOD+yf62xVwHfGmhHUn2JZlJMjM3N9d9SknSUCP9UDTJW4Ap4CML7a+qA1U1VVVTExMTo3xqSfqlt7HDmkeBrQP3t/S3/T9JXg+8D3htVf10NONJkrrq8g79KLAtyeVJNgF7gOnBBUleDvwDsKuqTo1+TEnSMEODXlVngOuBw8D9wB1VdSzJzUl29Zd9BHg+8Pkk30oyvcjDSZJWSJdTLlTVIeDQvG03Ddx+/YjnkiSdJf9SVJIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIa0SnoSXYkeSDJbJIbF9j/3CSf6+//ZpLJkU8qSVrS0KAn2QDsB64BtgN7k2yft+w64Imq+k3gY8CHRj2oJGlpXd6hXwXMVtWJqjoNHAR2z1uzG/hM//adwOuSZHRjSpKG6RL0S4BHBu6f7G9bcE1VnQGeBH5t/gMl2ZdkJsnM3NzcuU0sSevUczdsYOfvvIhLf/V5K/L4G1fkURdRVQeAAwBTU1O1ms8tSeN20fOewyf+5HdX7PG7vEN/FNg6cH9Lf9uCa5JsBC4CHh/FgJKkbroE/SiwLcnlSTYBe4DpeWumgbf1b/8h8NWq8h24JK2ioadcqupMkuuBw8AG4JaqOpbkZmCmqqaBfwJuTTIL/Ihe9CVJq6jTOfSqOgQcmrftpoHbTwN/NNrRJElnw78UlaRGGHRJaoRBl6RGGHRJakTG9duFSeaA74/wITcDj43w8cbF41hbPI61xeOAy6pqYqEdYwv6qCWZqaqpcc+xXB7H2uJxrC0ex9I85SJJjTDoktSIloJ+YNwDjIjHsbZ4HGuLx7GEZs6hS9Ivu5beoUvSLzWDLkmNWHdBH3bB6oF1b0pSSdbkrzh1uPD2tUnmknyr//Wn45hzmC6vR5I3Jzme5FiS21Z7xi46vB4fG3gtHkzy4zGMOVSH47g0yV1J7k1yX5Kd45hzKR2O4bIkX+nP/7UkW8Yx5zBJbklyKsl3FtmfJB/vH+d9Sa5c9pNW1br5ovff9/478BvAJuDbwPYF1l0A3A0cAabGPfe5HAdwLfB34551BMexDbgXeEH//gvHPfe5fl8NrH8Xvf9Geuyzn8PrcQD4s/7t7cBD4577HI7h88Db+rd/D7h13HMvciyvAa4EvrPI/p3Al4AArwS+udznXG/v0LtcsBrgg8CHgKdXc7iz0PU41roux/EOYH9VPQFQVadWecYuzvb12AvcviqTnZ0ux1HAhf3bFwE/WMX5uuhyDNuBr/Zv37XA/jWhqu6md32IxewGPls9R4CLk7x4Oc+53oI+9ILV/R9btlbVF1dzsLPU5cLbAG/q/yh2Z5KtC+wfty7HcQVwRZKvJzmSZMeqTddd19eDJJcBl/N/QVlLuhzHB4C3JDlJ7xoH71qd0TrrcgzfBt7Yv/0HwAVJnnVR+nWg8/ddV+st6EtKch7wUeCGcc8yAv8MTFbVy4AvA58Z8zznaiO90y5X03tn+6kkF49zoGXaA9xZVT8f9yDnaC/w6araQu9H/lv7/27Wk78CXpvkXuC19K5pvF5fj5Faby/ksAtWXwC8FPhakofonZeaXoMfjA698HZVPV5VP+3f/Udg5S4Vfu66XED8JDBdVT+rqu8BD9IL/FrS5Th+YQ9r83QLdDuO64A7AKrqG8D59P6jqLWiy7+NH1TVG6vq5cD7+tt+vGoTjs7ZfN91st6CvuQFq6vqyaraXFWTVTVJ70PRXVU1M55xFzX0wtvzzqXtAu5fxfm66nIB8S/Qe3dOks30TsGcWMUZu+hyHCT5LeAFwDdWeb6uuhzHw8DrAJK8hF7Q51Z1yqV1+bexeeCnivcCt6zyjKMyDby1/9surwSerKofLucBO11TdK2obhesXvM6HsefJ9kFnKH3wcq1Yxt4ER2P4zDwhiTH6f1Y/J6qenx8Uz/bWXxf7QEOVv9XFNaajsdxA73TXn9J7wPSa9fS8XQ8hquBv01S9H6b7Z1jG3gJSW6nN+vm/mcW7weeA1BVn6T3GcZOYBZ4Cnj7sp9zDb2WkqRlWG+nXCRJizDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjfgfqOLGDUIuRbMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(saving_folder_name=None, additional_name='', criterion='BCELoss', \\\n",
    "    use_gpu=True, project_name='test', experiment='test', oversampling=False,\\\n",
    "            pred_window=2, observing_window=2, BATCH_SIZE=128, LR=0.0001, min_frequency=1, hidden_size=128,\\\n",
    "                drop=0.6, weight_decay=0, num_epochs=1, wandb_mode='disabled', PRETRAINED_PATH=None, run_id=None):\n",
    "\n",
    "    # define the device\n",
    "    if use_gpu:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    else:\n",
    "        device='cpu'\n",
    "    print(f'Device: {device}')         \n",
    "\n",
    "    CURR_PATH = '/home/svetlanamaslenkova/Documents/AKI_deep/LSTM'\n",
    "    DF_PATH = CURR_PATH +'/dataframes_2/'\n",
    "    destination_folder = CURR_PATH + '/training/'\n",
    "    TXT_FILES_CODES_PATH = '/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/txt_files/icu_train'\n",
    "    TOKENIZER_CODES_PATH = '/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/aki_prediction/tokenizer_icu_codes.json'\n",
    "    test_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/test_data/'\n",
    "    train_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/train_data/'\n",
    "    val_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/val_data/'\n",
    "\n",
    "    # Training the tokenizer\n",
    "    if exists(TOKENIZER_CODES_PATH):\n",
    "        tokenizer = Tokenizer.from_file(TOKENIZER_CODES_PATH)\n",
    "        print(f'Tokenizer is loaded from ==> {TOKENIZER_CODES_PATH}/tokenizer.json. Vocab size is {tokenizer.get_vocab_size()}')\n",
    "\n",
    "    max_lengths_dict = {'demographics':30, 'previous_diags_codes':65, 'labs_codes':240, 'icu_12h_info_codes':120}\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    embedding_size = 200\n",
    "    dimension = 128\n",
    "\n",
    "    test_dataset = MyDataset(data_path=test_data_path, tokenizer=tokenizer, max_length_dict=max_lengths_dict)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    val_dataset = MyDataset(data_path=val_data_path, tokenizer=tokenizer, max_length_dict=max_lengths_dict)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    train_dataset = MyDataset(data_path=train_data_path, tokenizer=tokenizer, max_length_dict=max_lengths_dict)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    if oversampling:\n",
    "        i = 0\n",
    "        for _, _, _, _, tensor_labels, _  in train_loader:\n",
    "            if i == 0:\n",
    "                stacked_labels = tensor_labels\n",
    "            else:\n",
    "                stacked_labels = np.concatenate([stacked_labels, tensor_labels])\n",
    "            i += 1\n",
    "        y_train = stacked_labels.T[0]\n",
    "        count=Counter(y_train)\n",
    "        class_count=np.array([count[0], count[1]])\n",
    "        weight=1./class_count\n",
    "        print('weights for oversampling: ', weight)\n",
    "\n",
    "        samples_weight = np.array([weight[int(t)] for t in y_train])\n",
    "        samples_weight=torch.from_numpy(samples_weight)\n",
    "        sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, num_workers=1, sampler=sampler)\n",
    "\n",
    "    model = EHR_MODEL(max_lengths_dict, vocab_size=tokenizer.get_vocab_size()).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    if PRETRAINED_PATH is not None:\n",
    "        load_checkpoint(PRETRAINED_PATH, model, optimizer, device)\n",
    "\n",
    "    exp_lr_scheduler = None\n",
    "\n",
    "    train_params = {\n",
    "                'model':model,\n",
    "                'device':device,\n",
    "                'optimizer':optimizer,\n",
    "                'criterion':criterion,\n",
    "                'train_loader':train_loader,\n",
    "                'valid_loader':val_loader,\n",
    "                'num_epochs':num_epochs, \n",
    "                'file_path':destination_folder,\n",
    "                'best_valid_loss':float(\"Inf\"),\n",
    "                'dimension':128,\n",
    "                'epoch_patience':15,\n",
    "                'threshold':None,\n",
    "                'scheduler':exp_lr_scheduler\n",
    "            }\n",
    "\n",
    "    weights = ''\n",
    "    # path for the model\n",
    "    if saving_folder_name is None:\n",
    "        saving_folder_name = additional_name + 'ICU_' + experiment + '_' + str(len(train_dataset) // 1000) + 'k_'  \\\n",
    "            + 'lr' + str(LR) + '_h'+ str(hidden_size) + '_pw' + str(pred_window) + '_ow' + str(observing_window) \\\n",
    "                + '_wd' + str(weight_decay) + '_'+ weights + '_drop' + str(drop)\n",
    "\n",
    "    \n",
    "    file_path = destination_folder + saving_folder_name\n",
    "    train_params['file_path'] = file_path\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "\n",
    "    # wandb setup\n",
    "    os.environ['WANDB_API_KEY'] = '8e859a0fc58f296096842a367ca532717d3b4059'\n",
    "    run_name = saving_folder_name\n",
    "    if run_id is None:    \n",
    "        run_id = wandb.util.generate_id()  \n",
    "        resume = 'allow' \n",
    "    else:\n",
    "        resume = 'must'\n",
    "\n",
    "    args = {'optimizer':'Adam', 'criterion':'BCELoss', 'LR':LR, 'min_frequency':min_frequency, 'hidden_size':hidden_size, \\\n",
    "            'pred_window':pred_window, 'experiment':experiment,'weight_decay':weight_decay, 'drop':drop}\n",
    "    wandb.init(project=project_name, name=run_name, mode=wandb_mode, config=args, id=run_id, resume=resume)\n",
    "    print('Run id is: ', run_id)\n",
    "    print('Run name: ', run_name)\n",
    "    print(f'\\n\\nMODEL PATH: {file_path}')\n",
    "    # training\n",
    "    print('Training started..')\n",
    "    train(**train_params)\n",
    "\n",
    "    # testing\n",
    "    print('\\nTesting the model...')\n",
    "    load_checkpoint(file_path + '/model.pt', model, optimizer, device=device)\n",
    "    evaluate(model, test_loader, threshold=None, log_res=True)\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Tokenizer is loaded from ==> /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/aki_prediction/tokenizer_icu_codes.json/tokenizer.json. Vocab size is 8791\n",
      "Run id is:  7d4me1po\n",
      "Run name:  ICU_test_11k_lr0.0001_h128_pw2_ow2_wd0__drop0.6\n",
      "\n",
      "\n",
      "MODEL PATH: /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/ICU_test_11k_lr0.0001_h128_pw2_ow2_wd0__drop0.6\n",
      "Training started..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svetlanamaslenkova/anaconda_envs/aki_env/lib/python3.7/site-packages/ipykernel_launcher.py:112: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/svetlanamaslenkova/anaconda_envs/aki_env/lib/python3.7/site-packages/ipykernel_launcher.py:112: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [90/90], Train Loss: 0.2002, Valid Loss: 0.1640\n",
      "Model saved to ==> /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/ICU_test_11k_lr0.0001_h128_pw2_ow2_wd0__drop0.6/model.pt\n",
      "Finished Training!\n",
      "\n",
      "Testing the model...\n",
      "Model loaded from <== /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/training/ICU_test_11k_lr0.0001_h128_pw2_ow2_wd0__drop0.6/model.pt\n",
      "Evaluation..\n",
      "------------- AKI stage  1 ------------- \n",
      "Best Threshold=0.09, F-Score=0.26\n",
      "Accuracy: 0.88\n",
      "F1:  0.1\n",
      "Sensitivity:  0.07\n",
      "Precision:  0.18\n",
      "Specificity:  0.97\n",
      "PR AUC:  0.15\n",
      "ROC AUC:  0.52\n",
      "Confusion matrix:\n",
      " [[1246   45]\n",
      " [ 127   10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.97      0.94      1291\n",
      "         1.0       0.18      0.07      0.10       137\n",
      "\n",
      "    accuracy                           0.88      1428\n",
      "   macro avg       0.54      0.52      0.52      1428\n",
      "weighted avg       0.84      0.88      0.86      1428\n",
      "\n",
      "Precision 20.0% , Sensitivity 0.21 \n",
      "Precision 25.0% , Sensitivity 0.01 \n",
      "Precision 33.0% , Sensitivity 0.01 \n",
      "Precision 33.0% , Sensitivity 0.01 \n",
      "Precision 50.0% , Sensitivity 0.01 \n",
      "Precision 50.0% , Sensitivity 0.01 \n",
      "Precision 50.0% , Sensitivity 0.01 \n",
      "------------- AKI stage  2 ------------- \n",
      "Best Threshold=0.05, F-Score=0.08\n",
      "Accuracy: 0.93\n",
      "F1:  0.05\n",
      "Sensitivity:  0.07\n",
      "Precision:  0.04\n",
      "Specificity:  0.95\n",
      "PR AUC:  0.03\n",
      "ROC AUC:  0.51\n",
      "Confusion matrix:\n",
      " [[1318   64]\n",
      " [  43    3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.95      0.96      1382\n",
      "         1.0       0.04      0.07      0.05        46\n",
      "\n",
      "    accuracy                           0.93      1428\n",
      "   macro avg       0.51      0.51      0.51      1428\n",
      "weighted avg       0.94      0.93      0.93      1428\n",
      "\n",
      "Precision 14.0% , Sensitivity 0.04 \n",
      "Precision 14.0% , Sensitivity 0.04 \n",
      "Precision 14.0% , Sensitivity 0.04 \n",
      "Precision 14.0% , Sensitivity 0.04 \n",
      "Precision 14.0% , Sensitivity 0.04 \n",
      "Precision 100.0% , Sensitivity 0.0 \n",
      "Precision 100.0% , Sensitivity 0.0 \n",
      "------------- AKI stage  3 ------------- \n",
      "Best Threshold=0.06, F-Score=0.09\n",
      "Accuracy: 0.99\n",
      "F1:  0.0\n",
      "Sensitivity:  0.0\n",
      "Precision:  0.0\n",
      "Specificity:  1.0\n",
      "PR AUC:  0.01\n",
      "ROC AUC:  0.5\n",
      "Confusion matrix:\n",
      " [[1417    0]\n",
      " [  11    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00      1417\n",
      "         1.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.99      1428\n",
      "   macro avg       0.50      0.50      0.50      1428\n",
      "weighted avg       0.98      0.99      0.99      1428\n",
      "\n",
      "Precision 9.0% , Sensitivity 0.09 \n",
      "Precision 9.0% , Sensitivity 0.09 \n",
      "Precision 9.0% , Sensitivity 0.09 \n",
      "Precision 9.0% , Sensitivity 0.09 \n",
      "Precision 9.0% , Sensitivity 0.09 \n",
      "Precision 100.0% , Sensitivity 0.0 \n",
      "Precision 100.0% , Sensitivity 0.0 \n",
      "------------- AKI stage  ANY ------------- \n",
      "Best Threshold=0.09, F-Score=0.26\n",
      "Accuracy: 0.88\n",
      "F1:  0.1\n",
      "Sensitivity:  0.07\n",
      "Precision:  0.18\n",
      "Specificity:  0.97\n",
      "PR AUC:  0.15\n",
      "ROC AUC:  0.52\n",
      "Confusion matrix:\n",
      " [[1246   45]\n",
      " [ 127   10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      1291\n",
      "           1       0.18      0.07      0.10       137\n",
      "\n",
      "    accuracy                           0.88      1428\n",
      "   macro avg       0.54      0.52      0.52      1428\n",
      "weighted avg       0.84      0.88      0.86      1428\n",
      "\n",
      "Precision 20.0% , Sensitivity 0.21 \n",
      "Precision 25.0% , Sensitivity 0.01 \n",
      "Precision 33.0% , Sensitivity 0.01 \n",
      "Precision 33.0% , Sensitivity 0.01 \n",
      "Precision 50.0% , Sensitivity 0.01 \n",
      "Precision 50.0% , Sensitivity 0.01 \n",
      "Precision 50.0% , Sensitivity 0.01 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svetlanamaslenkova/anaconda_envs/aki_env/lib/python3.7/site-packages/ipykernel_launcher.py:55: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/svetlanamaslenkova/anaconda_envs/aki_env/lib/python3.7/site-packages/ipykernel_launcher.py:55: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/svetlanamaslenkova/anaconda_envs/aki_env/lib/python3.7/site-packages/ipykernel_launcher.py:55: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/svetlanamaslenkova/anaconda_envs/aki_env/lib/python3.7/site-packages/ipykernel_launcher.py:55: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "main(saving_folder_name=None, additional_name='', criterion='BCELoss', \\\n",
    "    use_gpu=True, project_name='test', experiment='test', oversampling=False, \n",
    "            pred_window=2, observing_window=2, BATCH_SIZE=128, LR=0.0001, min_frequency=5, hidden_size=128,\\\n",
    "                drop=0.6, weight_decay=0, num_epochs=1, wandb_mode='disabled', PRETRAINED_PATH=None, run_id=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(saving_folder_name=None, additional_name='', criterion='BCELoss', \\\n",
    "    use_gpu=True, project_name='test', experiment='test', oversampling=False,\\\n",
    "            pred_window=2, observing_window=2, BATCH_SIZE=128, LR=0.0001, min_frequency=1, hidden_size=128,\\\n",
    "                drop=0.6, weight_decay=0, num_epochs=1, embedding_size=200, dimension = 128, wandb_mode='disabled', PRETRAINED_PATH=None, run_id=None):\n",
    "\n",
    "    # define the device\n",
    "    if use_gpu:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    else:\n",
    "        device='cpu'\n",
    "    print(f'Device: {device}')         \n",
    "\n",
    "    CURR_PATH = '/home/svetlanamaslenkova/Documents/AKI_deep/LSTM'\n",
    "    DF_PATH = CURR_PATH +'/dataframes_2/'\n",
    "    destination_folder = CURR_PATH + '/icu_training/'\n",
    "    TXT_FILES_CODES_PATH = '/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/txt_files/icu_train'\n",
    "    TOKENIZER_CODES_PATH = '/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/aki_prediction/tokenizer_icu_codes.json'\n",
    "    test_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/test_data/'\n",
    "    train_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/train_data/'\n",
    "    val_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/val_data/'\n",
    "    # LABELS_PATH = '/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/pickles_2/aki_stage_labels.pkl'\n",
    "\n",
    "    # with open(LABELS_PATH, 'rb') as f:\n",
    "    #     aki_stage_labels = pickle.load(f)\n",
    "\n",
    "    # CURR_PATH = os.getcwd()\n",
    "    # DF_PATH = CURR_PATH +'icu_data/dataframes_2/'\n",
    "    # destination_folder = '/l/users/svetlana.maslenkova/models' + '/icu_models/no_pretraining/'\n",
    "    # TXT_FILES_CODES_PATH = CURR_PATH + '/aki_prediction/txt_files/icu_train'\n",
    "    # TOKENIZER_CODES_PATH = CURR_PATH + '/aki_prediction/tokenizer_icu_codes.json'\n",
    "    # test_data_path = CURR_PATH + '/icu_data/dataframes_2/test_data/'\n",
    "    # train_data_path = CURR_PATH + '/icu_data/dataframes_2/train_data/'\n",
    "    # val_data_path = CURR_PATH + '/icu_data/dataframes_2/val_data/'\n",
    "    \n",
    "    # Training the tokenizer\n",
    "    print(f'Current directory: {CURR_PATH}')\n",
    "    if exists(TOKENIZER_CODES_PATH):\n",
    "        tokenizer = Tokenizer.from_file(TOKENIZER_CODES_PATH)\n",
    "        print(f'Tokenizer is loaded from ==> {TOKENIZER_CODES_PATH}/tokenizer.json. Vocab size is {tokenizer.get_vocab_size()}')\n",
    "\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    max_length = 350\n",
    "\n",
    "    train_dataset = MyDataset(data_path=train_data_path, labels_df=None, tokenizer=tokenizer, max_length=max_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    test_dataset = MyDataset(data_path=test_data_path, labels_df=None, tokenizer=tokenizer, max_length=max_length)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    val_dataset = MyDataset(data_path=val_data_path, labels_df=None, tokenizer=tokenizer, max_length=max_length)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    if oversampling:\n",
    "        i = 0\n",
    "        for _, _, _, _, tensor_labels, _  in train_loader:\n",
    "            if i == 0:\n",
    "                stacked_labels = tensor_labels\n",
    "            else:\n",
    "                stacked_labels = np.concatenate([stacked_labels, tensor_labels])\n",
    "            i += 1\n",
    "        y_train = stacked_labels.T[0]\n",
    "        count=Counter(y_train)\n",
    "        class_count=np.array([count[0], count[1]])\n",
    "        weight=1./class_count\n",
    "        print('weights for oversampling: ', weight)\n",
    "\n",
    "        samples_weight = np.array([weight[int(t)] for t in y_train])\n",
    "        samples_weight=torch.from_numpy(samples_weight)\n",
    "        sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, num_workers=1, sampler=sampler)\n",
    "\n",
    "    model = EHR_MODEL(max_length=max_length, vocab_size=vocab_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    if PRETRAINED_PATH is not None:\n",
    "        load_checkpoint(PRETRAINED_PATH, model, optimizer, device)\n",
    "\n",
    "    exp_lr_scheduler = None\n",
    "\n",
    "    train_params = {\n",
    "                'model':model,\n",
    "                'device':device,\n",
    "                'optimizer':optimizer,\n",
    "                'criterion':criterion,\n",
    "                'train_loader':train_loader,\n",
    "                'valid_loader':val_loader,\n",
    "                'num_epochs':num_epochs, \n",
    "                'file_path':destination_folder,\n",
    "                'best_valid_loss':float(\"Inf\"),\n",
    "                'dimension':128,\n",
    "                'epoch_patience':15,\n",
    "                'threshold':None,\n",
    "                'scheduler':exp_lr_scheduler\n",
    "            }\n",
    "\n",
    "    weights = ''\n",
    "    # path for the model\n",
    "    if saving_folder_name is None:\n",
    "        saving_folder_name = additional_name + '_M2_' + 'ICU_' + experiment + '_' + str(len(train_dataset) // 1000) + 'k_'  \\\n",
    "            + 'lr' + str(LR) + '_h'+ str(hidden_size) + '_pw' + str(pred_window) + '_ow' + str(observing_window) \\\n",
    "                + '_wd' + str(weight_decay) + '_'+ weights + '_drop' + str(drop) + '_emb' + str(embedding_size) + '_vocab' + str(vocab_size)\n",
    "                \n",
    "\n",
    "    \n",
    "    file_path = destination_folder + saving_folder_name\n",
    "    train_params['file_path'] = file_path\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "\n",
    "    # wandb setup\n",
    "    os.environ['WANDB_API_KEY'] = '8e859a0fc58f296096842a367ca532717d3b4059'\n",
    "    run_name = saving_folder_name\n",
    "    if run_id is None:    \n",
    "        run_id = wandb.util.generate_id()  \n",
    "        resume = 'allow' \n",
    "    else:\n",
    "        resume = 'must'\n",
    "\n",
    "    args = {'optimizer':'Adam', 'criterion':'BCELoss', 'LR':LR, 'min_frequency':min_frequency, 'hidden_size':hidden_size, \\\n",
    "            'pred_window':pred_window, 'experiment':experiment,'weight_decay':weight_decay, 'drop':drop, 'embedding_size':embedding_size, \\\n",
    "                'vocab_size':vocab_size}\n",
    "\n",
    "    wandb.init(project=project_name, name=run_name, mode=wandb_mode, config=args, id=run_id, resume=resume)\n",
    "    print('Run id is: ', run_id)\n",
    "    print('Run name: ', run_name)\n",
    "    print(f'\\n\\nMODEL PATH: {file_path}')\n",
    "    # training\n",
    "    print('Training started..')\n",
    "    train(**train_params)\n",
    "\n",
    "    # testing\n",
    "    print('\\nTesting the model...')\n",
    "    load_checkpoint(file_path + '/model.pt', model, optimizer, device=device)\n",
    "    evaluate(model, test_loader, threshold=None, log_res=True)\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current directory: /home/svetlanamaslenkova/Documents/AKI_deep/LSTM\n",
      "Tokenizer is loaded from ==> /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/aki_prediction/tokenizer_icu_codes.json/tokenizer.json. Vocab size is 8791\n",
      "Run id is:  1oxyasa4\n",
      "Run name:  test_model\n",
      "\n",
      "\n",
      "MODEL PATH: /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/icu_training/test_model\n",
      "Training started..\n",
      "Step 0/23\n",
      "Epoch [1/1], Step [23/23], Train Loss: 3.8169, Valid Loss: 2.0321\n",
      "Model saved to ==> /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/icu_training/test_model/model.pt\n",
      "Finished Training!\n",
      "\n",
      "Testing the model...\n",
      "Model loaded from <== /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/icu_training/test_model/model.pt\n",
      "Evaluation..\n",
      "------------- Stage  ANY ------------- \n",
      "Best Threshold=1.00, F-Score=0.18\n",
      "Accuracy: 0.9\n",
      "F1:  0.0\n",
      "Sensitivity:  0.0\n",
      "Precision:  0.0\n",
      "Specificity:  1.0\n",
      "PR AUC:  0.55\n",
      "ROC AUC:  0.5\n",
      "Confusion matrix:\n",
      " [[1291    0]\n",
      " [ 137    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      1.00      0.95      1291\n",
      "         1.0       0.00      0.00      0.00       137\n",
      "\n",
      "    accuracy                           0.90      1428\n",
      "   macro avg       0.45      0.50      0.47      1428\n",
      "weighted avg       0.82      0.90      0.86      1428\n",
      "\n",
      "Precision 10.0% , Sensitivity 1.0 \n",
      "Precision 10.0% , Sensitivity 1.0 \n",
      "Precision 10.0% , Sensitivity 1.0 \n",
      "Precision 10.0% , Sensitivity 1.0 \n",
      "Precision 10.0% , Sensitivity 1.0 \n",
      "Precision 100.0% , Sensitivity 0.0 \n",
      "Precision 100.0% , Sensitivity 0.0 \n",
      "------------- Stage  2|3 ------------- \n",
      "Best Threshold=1.00, F-Score=0.06\n",
      "Accuracy: 0.97\n",
      "F1:  0.0\n",
      "Sensitivity:  0.0\n",
      "Precision:  0.0\n",
      "Specificity:  1.0\n",
      "PR AUC:  0.52\n",
      "ROC AUC:  0.5\n",
      "Confusion matrix:\n",
      " [[1382    0]\n",
      " [  46    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.98      1382\n",
      "         1.0       0.00      0.00      0.00        46\n",
      "\n",
      "    accuracy                           0.97      1428\n",
      "   macro avg       0.48      0.50      0.49      1428\n",
      "weighted avg       0.94      0.97      0.95      1428\n",
      "\n",
      "Precision 3.0% , Sensitivity 1.0 \n",
      "Precision 3.0% , Sensitivity 1.0 \n",
      "Precision 3.0% , Sensitivity 1.0 \n",
      "Precision 3.0% , Sensitivity 1.0 \n",
      "Precision 3.0% , Sensitivity 1.0 \n",
      "Precision 100.0% , Sensitivity 0.0 \n",
      "Precision 100.0% , Sensitivity 0.0 \n",
      "------------- Stage  3 ------------- \n",
      "Best Threshold=0.04, F-Score=0.02\n",
      "Accuracy: 0.01\n",
      "F1:  0.01\n",
      "Sensitivity:  0.91\n",
      "Precision:  0.01\n",
      "Specificity:  0.0\n",
      "PR AUC:  0.01\n",
      "ROC AUC:  0.46\n",
      "Confusion matrix:\n",
      " [[   4 1413]\n",
      " [   1   10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.00      0.01      1417\n",
      "         1.0       0.01      0.91      0.01        11\n",
      "\n",
      "    accuracy                           0.01      1428\n",
      "   macro avg       0.40      0.46      0.01      1428\n",
      "weighted avg       0.79      0.01      0.01      1428\n",
      "\n",
      "Precision 1.0% , Sensitivity 1.0 \n",
      "Precision 1.0% , Sensitivity 1.0 \n",
      "Precision 1.0% , Sensitivity 1.0 \n",
      "Precision 1.0% , Sensitivity 1.0 \n",
      "Precision 1.0% , Sensitivity 1.0 \n",
      "Precision 100.0% , Sensitivity 0.0 \n",
      "Precision 100.0% , Sensitivity 0.0 \n"
     ]
    }
   ],
   "source": [
    "main(saving_folder_name='test_model', additional_name='', criterion='BCELoss', \\\n",
    "    use_gpu=True, project_name='test', experiment='test', oversampling=False,\\\n",
    "            pred_window=2, observing_window=2, BATCH_SIZE=512, LR=0.0001, min_frequency=1, hidden_size=128,\\\n",
    "                drop=0.6, weight_decay=0, num_epochs=1, embedding_size=200, dimension = 128, wandb_mode='disabled', \\\n",
    "                    PRETRAINED_PATH=None, run_id=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path,  tokenizer, labels_df=None, max_length=300, names=True, pred_window=2, observing_window=2):\n",
    "        # pred_window: number of 12h windows to predict AKI onset\n",
    "        # observing_window: number of 12h windows to observe\n",
    "        self.data_path = data_path\n",
    "        file_list = glob.glob(self.data_path + '*')\n",
    "        self.data = []\n",
    "        for sample in file_list:\n",
    "            self.data.append(sample)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.max_length = max_length\n",
    "        self.labels_df = labels_df[labels_df.icu_day_id==1]\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.make_matrices(idx)\n",
    "    \n",
    "    def tokenize(self, text, max_length):\n",
    "        \n",
    "        try:\n",
    "            output = self.tokenizer.encode(text)\n",
    "        except:\n",
    "            print( type(text), text, max_length, self.stay_id)\n",
    "            output = self.tokenizer.encode(text[0])\n",
    "\n",
    "        # padding and truncation\n",
    "        if len(output.ids) < max_length:\n",
    "            len_missing_token = max_length - len(output.ids)\n",
    "            padding_vec = [self.tokenizer.token_to_id('PAD') for _ in range(len_missing_token)]\n",
    "            token_output = [*output.ids, *padding_vec]\n",
    "        elif len(output.ids) > max_length:\n",
    "            token_output = output.ids[:max_length]\n",
    "        else:\n",
    "            token_output = output.ids\n",
    "        \n",
    "        return token_output\n",
    "\n",
    "    def preproccess(self, df):\n",
    "        df['demographics'] = df['demographics'].fillna('')\n",
    "        df['previous_diags_codes'] = df['previous_diags_codes'].fillna('')\n",
    "        df['previous_diags_names'] = df['previous_diags_names'].fillna('')\n",
    "        df['vitals_names'] = df['vitals_names'].fillna('')\n",
    "        df['vitals_codes'] = df['vitals_codes'].fillna('')\n",
    "        df['labs_names'] = df['labs_names'].fillna('')\n",
    "        df['labs_codes'] = df['labs_codes'].fillna('')\n",
    "        df['outputs_names'] = df['outputs_names'].fillna('')\n",
    "        df['outputs_codes'] = df['outputs_codes'].fillna('')\n",
    "        df['medications_names'] = df['medications_names'].fillna('')\n",
    "        df['medications_codes'] = df['medications_codes'].fillna('')\n",
    "        df = df.replace(r';', '', regex=True)\n",
    "        df['AKI_1'] = df['AKI_1'].fillna(0)\n",
    "        df['AKI_2'] = df['AKI_2'].fillna(0)\n",
    "        df['AKI_3'] = df['AKI_3'].fillna(0)\n",
    "\n",
    "        df = df[(df.icu_12h_window_id.isin(np.arange(self.min_wind, self.min_wind + self.observing_window +  self.pred_window))) | (df.icu_day_id.isin(np.arange(self.min_day, self.observing_window//2 +  self.pred_window//2)))]\n",
    "        return df\n",
    "\n",
    "    def make_matrices(self, idx):\n",
    "        # load csv file\n",
    "        sample_path = self.data[idx]\n",
    "        df = pd.read_csv(sample_path)\n",
    "        # print('Loaded from ', sample_path)\n",
    "        \n",
    "        windows_12h = df.icu_12h_window_id.values\n",
    "        days = df.icu_day_id.values\n",
    "        self.min_wind = int( np.max([np.min(windows_12h[~np.isnan(windows_12h)]),0] ) )       \n",
    "        self.min_day = int( np.max( [np.min(days[~np.isnan(days)]), 0] ))  \n",
    "        self.df = self.preproccess(df)\n",
    "        self.stay_id = self.df.stay_id.values[0]  \n",
    "        # print(stay_id)\n",
    "        sort = np.argsort(self.df.icu_12h_window_id.values)\n",
    "        windows_12h = self.df.icu_12h_window_id.values[sort]\n",
    "        days = self.df.icu_day_id.values[sort]\n",
    "\n",
    "        info_12h = self.df.icu_12h_info_codes.values[sort]\n",
    "        info_24h_labs = self.df.labs_codes.values[sort]\n",
    "        info_demo = self.df.demographics.values[0]\n",
    "        info_diagnoses = self.df.previous_diags_codes.values[0]\n",
    "\n",
    "        AKI_1_status = self.df.AKI_1.values[sort]\n",
    "        AKI_2_status = self.df.AKI_2.values[sort]\n",
    "        AKI_3_status = self.df.AKI_3.values[sort]\n",
    "\n",
    "        AKI_1_labels_l = []\n",
    "        AKI_2_labels_l = []\n",
    "        AKI_3_labels_l = []\n",
    "        info_12h_list = []\n",
    "        info_24h_list = []\n",
    "        day_l = []\n",
    "        info_l = []\n",
    "        used_day_id_l = []\n",
    "        used_wind_id_l = []\n",
    "\n",
    "        wind_12h_pairs = [(i, i+1) for i in range(0, 2*(self.min_day + self.observing_window//2 +  self.pred_window//2), 2)]\n",
    "\n",
    "        for day in range(self.min_day, self.min_day + self.observing_window//2 +  self.pred_window//2):\n",
    "            for wind in wind_12h_pairs[day]:\n",
    "                if wind not in windows_12h:\n",
    "                    if day not in days:\n",
    "                    # print('not in days')\n",
    "                        AKI_1_labels_l.append(0)\n",
    "                        AKI_2_labels_l.append(0)\n",
    "                        AKI_3_labels_l.append(0)\n",
    "                        day_l.append('')\n",
    "                        if day not in used_day_id_l:\n",
    "                            day_l.append('')\n",
    "                            used_day_id_l.append(day)\n",
    "                    else:\n",
    "                        AKI_1_labels_l.append(self.df[self.df.icu_day_id==day].AKI_1.values[0])\n",
    "                        AKI_2_labels_l.append(self.df[self.df.icu_day_id==day].AKI_2.values[0])\n",
    "                        AKI_3_labels_l.append(self.df[self.df.icu_day_id==day].AKI_3.values[0])\n",
    "                        day_l.append(self.df[self.df.icu_day_id==day].icu_12h_info_codes.values[0])\n",
    "                        if day not in used_day_id_l:\n",
    "                            day_l.append(self.df[self.df.icu_day_id==day].labs_codes.values[0])\n",
    "                            used_day_id_l.append(day)\n",
    "                else:\n",
    "                    i = list(windows_12h).index(wind)\n",
    "\n",
    "                    AKI_1_labels_l.append(AKI_1_status[i])\n",
    "                    AKI_2_labels_l.append(AKI_2_status[i])\n",
    "                    AKI_3_labels_l.append(AKI_3_status[i])\n",
    "                    day_l.append(info_12h[i])\n",
    "                    if day not in used_day_id_l:\n",
    "                        day_l.append(info_24h_labs[i])\n",
    "                        used_day_id_l.append(day)\n",
    "                used_wind_id_l.append(wind)\n",
    "            day_info = ' '.join(day_l)\n",
    "            info_l.append(self.tokenize(day_info, self.max_length))\n",
    "\n",
    "        demographics = info_demo\n",
    "        diagnoses = info_diagnoses\n",
    "        demo_diags = ' '.join([demographics, diagnoses])\n",
    "        demo_diags = self.tokenize(demo_diags, 50)\n",
    "\n",
    "        #make tensors\n",
    "        tensor_day_info = torch.tensor(info_l[:self.observing_window//2], dtype=torch.int64)\n",
    "        tensor_demo_diags = torch.tensor(demo_diags, dtype=torch.int64)\n",
    "\n",
    "        if self.labels_df is None:\n",
    "            # making 24h labels from 12h\n",
    "            AKI_1_labels = [int(bool(np.sum(AKI_1_labels_l[i:i+2]))) for i in np.arange(0, self.observing_window + self.pred_window, 2)]\n",
    "            AKI_2_labels = [int(bool(np.sum(AKI_2_labels_l[i:i+2]))) for i in np.arange(0, self.observing_window + self.pred_window, 2)]\n",
    "            AKI_3_labels = [int(bool(np.sum(AKI_3_labels_l[i:i+2]))) for i in np.arange(0, self.observing_window + self.pred_window, 2)]\n",
    "            \n",
    "            tensor_labels = torch.tensor([*AKI_1_labels[self.observing_window//2:self.observing_window//2 + self.pred_window//2],\\\n",
    "                                        *AKI_2_labels[self.observing_window//2:self.observing_window//2 + self.pred_window//2],\\\n",
    "                                        *AKI_3_labels[self.observing_window//2:self.observing_window//2 + self.pred_window//2] ]\\\n",
    "                                            , dtype=torch.float64)\n",
    "        else:\n",
    "            AKI_1_label = (np.sum(self.labels_df[self.labels_df.stay_id==self.stay_id].AKI_1.values) > 0).astype(int)\n",
    "            AKI_2_label = (np.sum(self.labels_df[self.labels_df.stay_id==self.stay_id].AKI_2.values) > 0).astype(int)\n",
    "            AKI_3_label = (np.sum(self.labels_df[self.labels_df.stay_id==self.stay_id].AKI_3.values) > 0).astype(int)\n",
    "            NO_AKI_label = (np.sum([AKI_1_label, AKI_2_label, AKI_3_label]) == 0).astype(int)\n",
    "            tensor_labels = torch.tensor([AKI_1_label, AKI_2_label, AKI_3_label, NO_AKI_label])\n",
    "\n",
    "        return tensor_day_info, tensor_demo_diags, tensor_labels, {'stay_id':self.stay_id, 'day_id':used_day_id_l, 'wind_id':used_wind_id_l}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 1\n",
    "BATCH_SIZE = 2\n",
    "test_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/test_data/'\n",
    "train_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/train_data/'\n",
    "val_data_path ='/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/val_data/'\n",
    "\n",
    "test_dataset = MyDataset(data_path=test_data_path, tokenizer=tokenizer, labels_df=aki_stage_labels, max_length=350)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_PATH = '/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/pickles_2/aki_stage_labels.pkl'\n",
    "\n",
    "with open(LABELS_PATH, 'rb') as f:\n",
    "    labels_lstm = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>specimen_id</th>\n",
       "      <th>day_id</th>\n",
       "      <th>icu_day_id</th>\n",
       "      <th>icu_12h_window_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>intime</th>\n",
       "      <th>AKI_1_scr</th>\n",
       "      <th>AKI_2_scr</th>\n",
       "      <th>AKI_3_scr</th>\n",
       "      <th>AKI_1_urine</th>\n",
       "      <th>AKI_2_urine</th>\n",
       "      <th>AKI_3_urine</th>\n",
       "      <th>AKI_1</th>\n",
       "      <th>AKI_2</th>\n",
       "      <th>AKI_3</th>\n",
       "      <th>ANY_AKI</th>\n",
       "      <th>NO_AKI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16788749</td>\n",
       "      <td>20000808</td>\n",
       "      <td>30610654</td>\n",
       "      <td>63232011.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2180-01-18 01:54:00</td>\n",
       "      <td>2180-01-17 19:32:10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16788749</td>\n",
       "      <td>20000808</td>\n",
       "      <td>30610654</td>\n",
       "      <td>72894600.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2180-01-19 03:36:00</td>\n",
       "      <td>2180-01-17 19:32:10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16003661</td>\n",
       "      <td>20001305</td>\n",
       "      <td>36916968</td>\n",
       "      <td>90355490.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2178-03-25 13:45:00</td>\n",
       "      <td>2178-03-25 02:59:09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16003661</td>\n",
       "      <td>20001305</td>\n",
       "      <td>36916968</td>\n",
       "      <td>92318652.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2178-03-27 02:12:00</td>\n",
       "      <td>2178-03-25 02:59:09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14577567</td>\n",
       "      <td>20001361</td>\n",
       "      <td>33475095</td>\n",
       "      <td>96321141.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2143-05-05 04:27:00</td>\n",
       "      <td>2143-05-04 16:52:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223351</th>\n",
       "      <td>10355856</td>\n",
       "      <td>29999625</td>\n",
       "      <td>36975675</td>\n",
       "      <td>93541889.0</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>2157-11-25 09:48:00</td>\n",
       "      <td>2157-11-07 11:51:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223352</th>\n",
       "      <td>10355856</td>\n",
       "      <td>29999625</td>\n",
       "      <td>36975675</td>\n",
       "      <td>43309251.0</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>37</td>\n",
       "      <td>2157-11-26 07:05:00</td>\n",
       "      <td>2157-11-07 11:51:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223353</th>\n",
       "      <td>10355856</td>\n",
       "      <td>29999625</td>\n",
       "      <td>36975675</td>\n",
       "      <td>73696493.0</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>39</td>\n",
       "      <td>2157-11-27 06:15:00</td>\n",
       "      <td>2157-11-07 11:51:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223354</th>\n",
       "      <td>10355856</td>\n",
       "      <td>29999625</td>\n",
       "      <td>36975675</td>\n",
       "      <td>77141158.0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>41</td>\n",
       "      <td>2157-11-28 06:38:00</td>\n",
       "      <td>2157-11-07 11:51:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223355</th>\n",
       "      <td>10355856</td>\n",
       "      <td>29999625</td>\n",
       "      <td>36975675</td>\n",
       "      <td>45463834.0</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>43</td>\n",
       "      <td>2157-11-29 06:15:00</td>\n",
       "      <td>2157-11-07 11:51:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223356 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        subject_id   hadm_id   stay_id  specimen_id  day_id  icu_day_id  \\\n",
       "0         16788749  20000808  30610654   63232011.0       5           0   \n",
       "1         16788749  20000808  30610654   72894600.0       6           1   \n",
       "2         16003661  20001305  36916968   90355490.0       0           0   \n",
       "3         16003661  20001305  36916968   92318652.0       1           1   \n",
       "4         14577567  20001361  33475095   96321141.0       0           0   \n",
       "...            ...       ...       ...          ...     ...         ...   \n",
       "223351    10355856  29999625  36975675   93541889.0      17          17   \n",
       "223352    10355856  29999625  36975675   43309251.0      18          18   \n",
       "223353    10355856  29999625  36975675   73696493.0      19          19   \n",
       "223354    10355856  29999625  36975675   77141158.0      20          20   \n",
       "223355    10355856  29999625  36975675   45463834.0      21          21   \n",
       "\n",
       "        icu_12h_window_id           charttime              intime  AKI_1_scr  \\\n",
       "0                       0 2180-01-18 01:54:00 2180-01-17 19:32:10          0   \n",
       "1                       2 2180-01-19 03:36:00 2180-01-17 19:32:10          0   \n",
       "2                       0 2178-03-25 13:45:00 2178-03-25 02:59:09          0   \n",
       "3                       3 2178-03-27 02:12:00 2178-03-25 02:59:09          1   \n",
       "4                       0 2143-05-05 04:27:00 2143-05-04 16:52:00          1   \n",
       "...                   ...                 ...                 ...        ...   \n",
       "223351                 35 2157-11-25 09:48:00 2157-11-07 11:51:00          0   \n",
       "223352                 37 2157-11-26 07:05:00 2157-11-07 11:51:00          0   \n",
       "223353                 39 2157-11-27 06:15:00 2157-11-07 11:51:00          0   \n",
       "223354                 41 2157-11-28 06:38:00 2157-11-07 11:51:00          0   \n",
       "223355                 43 2157-11-29 06:15:00 2157-11-07 11:51:00          0   \n",
       "\n",
       "        AKI_2_scr  AKI_3_scr  AKI_1_urine  AKI_2_urine  AKI_3_urine  AKI_1  \\\n",
       "0               0          0          0.0          0.0          0.0      0   \n",
       "1               0          0          1.0          0.0          0.0      1   \n",
       "2               0          0          1.0          1.0          1.0      0   \n",
       "3               0          0          1.0          1.0          1.0      0   \n",
       "4               0          0          0.0          0.0          0.0      1   \n",
       "...           ...        ...          ...          ...          ...    ...   \n",
       "223351          0          0          0.0          0.0          0.0      0   \n",
       "223352          0          0          0.0          0.0          0.0      0   \n",
       "223353          0          0          0.0          0.0          0.0      0   \n",
       "223354          0          0          0.0          0.0          0.0      0   \n",
       "223355          0          0          0.0          0.0          0.0      0   \n",
       "\n",
       "        AKI_2  AKI_3  ANY_AKI  NO_AKI  \n",
       "0           0      0        0       1  \n",
       "1           0      0        1       0  \n",
       "2           0      1        1       0  \n",
       "3           0      1        1       0  \n",
       "4           0      0        1       0  \n",
       "...       ...    ...      ...     ...  \n",
       "223351      0      0        0       1  \n",
       "223352      0      0        0       1  \n",
       "223353      0      0        0       1  \n",
       "223354      0      0        0       1  \n",
       "223355      0      0        0       1  \n",
       "\n",
       "[223356 rows x 20 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_fn = nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = EHR_MODEL(max_length=350, vocab_size=tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stay_id': tensor([35918219, 32151512]),\n",
       " 'day_id': [tensor([0, 0]), tensor([1, 1])],\n",
       " 'wind_id': [tensor([0, 0]), tensor([1, 1]), tensor([2, 2]), tensor([3, 3])]}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>day_id</th>\n",
       "      <th>icu_12h_window_id</th>\n",
       "      <th>icu_day_id</th>\n",
       "      <th>demographics</th>\n",
       "      <th>previous_diags_codes</th>\n",
       "      <th>previous_diags_names</th>\n",
       "      <th>vitals_names</th>\n",
       "      <th>...</th>\n",
       "      <th>labs_codes</th>\n",
       "      <th>outputs_names</th>\n",
       "      <th>outputs_codes</th>\n",
       "      <th>medications_names</th>\n",
       "      <th>medications_codes</th>\n",
       "      <th>AKI_1</th>\n",
       "      <th>AKI_2</th>\n",
       "      <th>AKI_3</th>\n",
       "      <th>icu_12h_info_codes</th>\n",
       "      <th>icu_12h_info_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16230329</td>\n",
       "      <td>23715751</td>\n",
       "      <td>34711573</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WHITE F {54}</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HR {103.0}  {123.0} bpm; HR Alarm - High {140....</td>\n",
       "      <td>...</td>\n",
       "      <td>l220228 {13.8}; l220274 {7.53}; l220545 {41.2}...</td>\n",
       "      <td>Foley {600.0} ml; Pre-Admission {1100.0} ml; V...</td>\n",
       "      <td>o226559 {600.0}; o226560 {475.0}; o226633 {110...</td>\n",
       "      <td>Heparin Sodium; Dopamine</td>\n",
       "      <td>m225152 m221662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>v220045 {103.0}  {123.0}; v220046 {140.0}  {14...</td>\n",
       "      <td>HR {103.0}  {123.0} bpm; HR Alarm - High {140....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16230329</td>\n",
       "      <td>23715751</td>\n",
       "      <td>34711573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>WHITE F {54}</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HR {95.0}  {115.0} bpm; HR Alarm - High {130.0...</td>\n",
       "      <td>...</td>\n",
       "      <td>l220228 {13.8}; l220274 {7.53}; l220545 {41.2}...</td>\n",
       "      <td>Void {975.0} ml</td>\n",
       "      <td>o226560 {975.0}</td>\n",
       "      <td>Heparin Sodium</td>\n",
       "      <td>m225152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>v220045 {95.0}  {115.0}; v220046 {130.0}  {130...</td>\n",
       "      <td>HR {95.0}  {115.0} bpm; HR Alarm - High {130.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id   stay_id  day_id  icu_12h_window_id  icu_day_id  \\\n",
       "1    16230329  23715751  34711573     NaN                0.0           0   \n",
       "2    16230329  23715751  34711573     0.0                1.0           0   \n",
       "\n",
       "    demographics                             previous_diags_codes  \\\n",
       "1  WHITE F {54}                                                     \n",
       "2  WHITE F {54}                                                     \n",
       "\n",
       "                              previous_diags_names  \\\n",
       "1                                                    \n",
       "2                                                    \n",
       "\n",
       "                                        vitals_names  ...  \\\n",
       "1  HR {103.0}  {123.0} bpm; HR Alarm - High {140....  ...   \n",
       "2  HR {95.0}  {115.0} bpm; HR Alarm - High {130.0...  ...   \n",
       "\n",
       "                                          labs_codes  \\\n",
       "1  l220228 {13.8}; l220274 {7.53}; l220545 {41.2}...   \n",
       "2  l220228 {13.8}; l220274 {7.53}; l220545 {41.2}...   \n",
       "\n",
       "                                       outputs_names  \\\n",
       "1  Foley {600.0} ml; Pre-Admission {1100.0} ml; V...   \n",
       "2                                    Void {975.0} ml   \n",
       "\n",
       "                                       outputs_codes  \\\n",
       "1  o226559 {600.0}; o226560 {475.0}; o226633 {110...   \n",
       "2                                    o226560 {975.0}   \n",
       "\n",
       "          medications_names medications_codes AKI_1 AKI_2  AKI_3  \\\n",
       "1  Heparin Sodium; Dopamine   m225152 m221662   0.0   0.0    0.0   \n",
       "2            Heparin Sodium           m225152   0.0   0.0    0.0   \n",
       "\n",
       "                                  icu_12h_info_codes  \\\n",
       "1  v220045 {103.0}  {123.0}; v220046 {140.0}  {14...   \n",
       "2  v220045 {95.0}  {115.0}; v220046 {130.0}  {130...   \n",
       "\n",
       "                                  icu_12h_info_names  \n",
       "1  HR {103.0}  {123.0} bpm; HR Alarm - High {140....  \n",
       "2  HR {95.0}  {115.0} bpm; HR Alarm - High {130.0...  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/test_data/icu_stay_31141989.csv'\n",
    "data_to_test = df = pd.read_csv('/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes_2/test_data/icu_stay_34711573.csv')\n",
    "data_to_test = data_to_test[data_to_test.icu_day_id==0]\n",
    "data_to_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 300])\n",
      "torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "tensor_day_info, tensor_demo_diags, tensor_labels, cash = MyDataset(data_path, tokenizer=tokenizer, labels_df=aki_stage_labels)[0]\n",
    "print(tensor_day_info.size())\n",
    "print(tensor_demo_diags.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(model, data_path):\n",
    "    input_ = MyDataset(data_path, tokenizer=tokenizer, labels_df=aki_stage_labels)[0]\n",
    "    tensor_day_info, tensor_demo_diags, tensor_labels, cash = input_\n",
    "    output = model(tensor_day_info, tensor_demo_diags)\n",
    "    proba = activation_fn(output)\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_emb_demo_diags:  torch.Size([50, 200])\n",
      "out_emb_day_info:  torch.Size([300, 200])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7268/352799226.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_7268/3330822396.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(model, data_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maki_stage_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtensor_day_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_demo_diags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_day_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_demo_diags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda_envs/aki_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7268/3095871079.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor_day_info, tensor_demo_diags)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mout_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mout_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'out_lstm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "predict_proba(model2, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "class_names=['no_aki','aki_1', 'aki_2', 'aki_3']\n",
    "explainer= LimeTextExplainer(class_names=class_names)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb5fdc613a097f32ece1e000bc60f17f3e33b8b9e39d8b8753abf3e45200e5d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
