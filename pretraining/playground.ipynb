{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tokenizers import  Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import glob\n",
    "from os.path import exists\n",
    "import os\n",
    "\n",
    "import pickle5 as pickle\n",
    "import wandb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, batch_size, device, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.register_buffer(\"temperature\", torch.tensor(temperature))\n",
    "        self.register_buffer(\"negatives_mask\", (~torch.eye(batch_size * 2, batch_size * 2, dtype=bool)).float())\n",
    "            \n",
    "    def forward(self, emb_i, emb_j):\n",
    "        \"\"\"\n",
    "        emb_i and emb_j are batches of embeddings, where corresponding indices are pairs\n",
    "        z_i, z_j as per SimCLR paper\n",
    "        \"\"\"\n",
    "        z_i = F.normalize(emb_i, dim=1)\n",
    "        z_j = F.normalize(emb_j, dim=1)\n",
    "\n",
    "        representations = torch.cat([z_i, z_j], dim=0)\n",
    "        \n",
    "        similarity_matrix = F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=2)\n",
    "        \n",
    "        sim_ij = torch.diag(similarity_matrix, self.batch_size)\n",
    "        sim_ji = torch.diag(similarity_matrix, -self.batch_size)\n",
    "        positives = torch.cat([sim_ij, sim_ji], dim=0)\n",
    "        \n",
    "        nominator = torch.exp(positives / self.temperature)\n",
    "        denominator = self.negatives_mask.to(self.device) * torch.exp(similarity_matrix / self.temperature)\n",
    "    \n",
    "        loss_partial = -torch.log(nominator / torch.sum(denominator, dim=1))\n",
    "        loss = torch.sum(loss_partial) / (2 * self.batch_size)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = {'demographics':5, 'lab_tests':400, 'vitals':31, 'medications':255}\n",
    "# max_length = {'demographics':5, 'lab_tests':400, 'vitals':200, 'medications':255}\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_length, pred_window=2, observing_window=3):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.max_length = max_length\n",
    "        self.max_length_diags = 35\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.make_matrices(idx)\n",
    "    \n",
    "    def tokenize(self, text, max_length): \n",
    "        \n",
    "        max_length = max_length + 2\n",
    "        tokenizer.enable_truncation(max_length=max_length)\n",
    "\n",
    "        output = self.tokenizer.encode(text)\n",
    "\n",
    "        # padding and truncation\n",
    "        if len(output.ids) < max_length:\n",
    "            len_missing_token = max_length - len(output.ids)\n",
    "            padding_vec = [self.tokenizer.token_to_id('PAD') for _ in range(len_missing_token)]\n",
    "            token_output = [*output.ids, *padding_vec]\n",
    "        elif len(output.ids) > max_length:\n",
    "            token_output = output.ids[:max_length]\n",
    "        else:\n",
    "            token_output = output.ids\n",
    "        \n",
    "        return token_output\n",
    "\n",
    "    def make_matrices(self, idx):\n",
    "        \n",
    "        hadm_id = self.df.hadm_id.values[idx]\n",
    "        diagnoses_info = self.df.previous_diagnoses.values[idx]\n",
    "        demo_info = self.df.demographics_in_visit.values[idx][0]\n",
    "        lab_info = self.df.lab_tests_in_visit.values[idx]\n",
    "        med_info = self.df.medications_in_visit.values[idx]\n",
    "        vitals_info = self.df.vitals_in_visit.values[idx]\n",
    "        \n",
    "        # aki_status = self.df.aki_status_in_visit.values[idx]\n",
    "        days = self.df.days.values[idx]\n",
    "        # print(idx)\n",
    "\n",
    "        lab_info_list = []\n",
    "        med_info_list = []\n",
    "        vitals_info_list = []\n",
    "        label = None\n",
    "\n",
    "        for day in range(days[0], days[0] + self.observing_window + self.pred_window):\n",
    "            # print('day', day)\n",
    "            if day not in days:\n",
    "                vitals_info_list.append(self.tokenize('', self.max_length['vitals']))\n",
    "                lab_info_list.append(self.tokenize('', self.max_length['lab_tests']))\n",
    "                med_info_list.append(self.tokenize('', self.max_length['medications']))\n",
    "\n",
    "            else:\n",
    "                i = days.index(day)\n",
    "                \n",
    "                # vitals\n",
    "                if (str(vitals_info[i]) == 'nan') or (vitals_info[i] == np.nan):\n",
    "                    vitals_info_list.append(self.tokenize('PAD', self.max_length['vitals']))\n",
    "                else:\n",
    "                    vitals_info_list.append(self.tokenize(vitals_info[i], self.max_length['vitals']))\n",
    "\n",
    "                # lab results\n",
    "                if (str(lab_info[i]) == 'nan') or (lab_info[i] == np.nan):\n",
    "                    lab_info_list.append(self.tokenize('PAD', self.max_length['lab_tests']))\n",
    "                else:\n",
    "                    lab_info_list.append(self.tokenize(lab_info[i], self.max_length['lab_tests']))\n",
    "                \n",
    "                # medications\n",
    "                if (str(med_info[i]) == 'nan') or (med_info[i] == np.nan):\n",
    "                    med_info_list.append(self.tokenize('PAD', self.max_length['medications']))\n",
    "                else:\n",
    "                    med_info_list.append(self.tokenize(med_info[i], self.max_length['medications']))\n",
    "\n",
    "        # diagnoses\n",
    "        if (str(diagnoses_info) == 'nan') or (diagnoses_info == np.nan):\n",
    "            diagnoses_info = self.tokenize('PAD', self.max_length_diags)\n",
    "        else:\n",
    "            diagnoses_info = self.tokenize(diagnoses_info, self.max_length_diags)\n",
    "\n",
    "        # demographics\n",
    "        if (str(demo_info) == 'nan') or (demo_info == np.nan):\n",
    "            demo_info = self.tokenize('PAD', self.max_length_diags)\n",
    "        else:\n",
    "            demo_info = self.tokenize(demo_info, self.max_length['demographics'])\n",
    "\n",
    "        #make tensors\n",
    "        tensor_demo = torch.tensor(demo_info, dtype=torch.int64)\n",
    "        tensor_diags = torch.tensor(diagnoses_info, dtype=torch.int64)\n",
    "        tensor_vitals = torch.tensor(vitals_info_list, dtype=torch.int64)\n",
    "        tensor_labs = torch.tensor(lab_info_list, dtype=torch.int64)\n",
    "        tensor_meds = torch.tensor(med_info_list, dtype=torch.int64)\n",
    "        # tensor_labels = torch.tensor(label, dtype=torch.float64)\n",
    "    \n",
    "        return tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds, hadm_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHR_PRETRAINING(nn.Module):\n",
    "    def __init__(self, max_length, vocab_size, device, pred_window=2, observing_window=3,  H=128, embedding_size=200, drop=0.6):\n",
    "        super(EHR_PRETRAINING, self).__init__()\n",
    "\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.H = H\n",
    "        self.max_length = max_length\n",
    "        self.max_length_diags = 30\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device\n",
    "        self.drop = drop\n",
    "\n",
    "        # self.embedding = pretrained_model\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "\n",
    "        self.lstm_day = nn.LSTM(input_size=embedding_size,\n",
    "                            hidden_size=self.H,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "\n",
    "        self.fc_day = nn.Linear(self.max_length * 2 * self.H, 2048)\n",
    "\n",
    "        self.fc_adm = nn.Linear(2048*self.observing_window +  self.max_length_diags * 2 * self.H, 2048)\n",
    "\n",
    "        self.lstm_adm = nn.LSTM(input_size=2048,\n",
    "                            hidden_size=self.H,\n",
    "                            num_layers=2,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=False)\n",
    "\n",
    "        self.drop = nn.Dropout(p=drop)\n",
    "        self.inner_drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        # self.fc_2 = nn.Linear(self.H*2, 2)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=self.H, out_features=256)\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor_day, tensor_diagnoses):\n",
    "\n",
    "        batch_size = tensor_day.size()[0]\n",
    "\n",
    "        full_output = torch.tensor([]).to(device=self.device)\n",
    "        out_emb_diags = self.embedding(tensor_diagnoses.squeeze(1))\n",
    "        out_lstm_diags, _ = self.lstm_day(out_emb_diags)\n",
    "        full_output = out_lstm_diags.reshape(batch_size, self.max_length_diags * 2 * self.H)\n",
    "        \n",
    "\n",
    "        for d in range(self.observing_window):\n",
    "            # embedding layer applied to all tensors [16,400,200]\n",
    "            out_emb = self.embedding(tensor_day[:, d, :].squeeze(1))\n",
    "            # print('out_emb', out_emb.size())\n",
    "\n",
    "            # lstm layer applied to embedded tensors\n",
    "            output_lstm_day= self.inner_drop(self.fc_day(\\\n",
    "                                    self.lstm_day(out_emb)[0]\\\n",
    "                                        .reshape(batch_size, self.max_length * 2 * self.H)))\n",
    "\n",
    "            # print('output_lstm_day', output_lstm_day.size())                   \n",
    "            # concatenate for all * days\n",
    "            full_output = torch.cat([full_output, output_lstm_day], dim=1) # [16, 768]\n",
    "\n",
    "        # print('full_output size: ', full_output.size(), '\\n')\n",
    "        output = self.fc_adm(full_output)\n",
    "        # print('output after fc_adm size: ', output.size(), '\\n')\n",
    "        output_vector, _ = self.lstm_adm(output)\n",
    "\n",
    "        # the fisrt transformation\n",
    "        output_vector_X = self.drop(output_vector)\n",
    "        projection_X = self.projection(output_vector_X)\n",
    "        # the second transformation\n",
    "        output_vector_Y = self.drop(output_vector)\n",
    "        projection_Y = self.projection(output_vector_Y)\n",
    "\n",
    "        return output_vector_X, projection_X, output_vector_Y, projection_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "CURR_PATH = os.getcwd()\n",
    "PKL_PATH = CURR_PATH+'/pickles/'\n",
    "DF_PATH = CURR_PATH +'/dataframes/'\n",
    "TXT_DIR_TRAIN = CURR_PATH + '/txt_files/train'\n",
    "destination_folder = '/l/users/svetlana.maslenkova/models' + '/pretraining/fc1_fixed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer.from_file('/home/svetlana.maslenkova/LSTM/aki_prediction/tokenizer.json')\n",
    "# print(f' Vocab size is {tokenizer.get_vocab_size()}')\n",
    "\n",
    "with open('/home/svetlana.maslenkova/LSTM/dataframes/pid_train_df_pretraining.pkl', 'rb') as f:\n",
    "    pid_train_df = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>demographics_in_visit</th>\n",
       "      <th>lab_tests_in_visit</th>\n",
       "      <th>medications_in_visit</th>\n",
       "      <th>vitals_in_visit</th>\n",
       "      <th>days_in_visit</th>\n",
       "      <th>aki_status_in_visit</th>\n",
       "      <th>previous_diagnoses</th>\n",
       "      <th>days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16679562</td>\n",
       "      <td>20001395</td>\n",
       "      <td>[hispanic latino m 73, hispanic latino m 73, h...</td>\n",
       "      <td>[hematology blood hematocrit  51.2  %; hematol...</td>\n",
       "      <td>[influenza vaccine quadrivalent  0.5  ml ; bis...</td>\n",
       "      <td>[temp    heartrate  80.0  resprate  16.0  o2sa...</td>\n",
       "      <td>[hispanic latino m 73$temp    heartrate  80.0 ...</td>\n",
       "      <td>[0, 0, 0, 1, 1, 0, 0, 1, 0]</td>\n",
       "      <td></td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id                              demographics_in_visit  \\\n",
       "9    16679562  20001395  [hispanic latino m 73, hispanic latino m 73, h...   \n",
       "\n",
       "                                  lab_tests_in_visit  \\\n",
       "9  [hematology blood hematocrit  51.2  %; hematol...   \n",
       "\n",
       "                                medications_in_visit  \\\n",
       "9  [influenza vaccine quadrivalent  0.5  ml ; bis...   \n",
       "\n",
       "                                     vitals_in_visit  \\\n",
       "9  [temp    heartrate  80.0  resprate  16.0  o2sa...   \n",
       "\n",
       "                                       days_in_visit  \\\n",
       "9  [hispanic latino m 73$temp    heartrate  80.0 ...   \n",
       "\n",
       "           aki_status_in_visit previous_diagnoses                         days  \n",
       "9  [0, 0, 0, 1, 1, 0, 0, 1, 0]                     [0, 1, 2, 3, 4, 5, 6, 7, 8]  "
      ]
     },
     "execution_count": 818,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2534868/2745813000.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpid_train_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedications_in_visit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mday\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for row in pid_train_df.medications_in_visit:\n",
    "    for day in row.values:\n",
    "        print(day)    \n",
    "        i+= 1\n",
    "        if i>0:break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "LIST_HADMS = []\n",
    "list_diags = []\n",
    "# for _, row in pid_val_df.iterrows():\n",
    "#     # try:\n",
    "#     #     list_diags.append(row.previous_diagnoses[0].replace('PAD ', '').replace('PAD', ''))\n",
    "#     #     LIST_HADMS.append(row.hadm_id)\n",
    "#     # except:\n",
    "#     #     print(row.previous_diagnoses[0])\n",
    "#     #     print(np.isnan(row.previous_diagnoses[0]))\n",
    "#     #     break\n",
    "#     try:\n",
    "#         if isinstance(row.previous_diagnoses[0], float):\n",
    "#             list_diags.append('')      \n",
    "#         else:\n",
    "#             list_diags.append(row.previous_diagnoses[0].replace('PAD ', '').replace('PAD', ''))  \n",
    "#         LIST_HADMS.append(row.hadm_id) \n",
    "#     except:\n",
    "#         print(row.previous_diagnoses[0])\n",
    "#         print(type(row.previous_diagnoses[0]))\n",
    "#         raise\n",
    "#         break\n",
    "\n",
    "#     i+=1\n",
    "#     # if i>0:break\n",
    "\n",
    "pid_train_df['previous_diags'] = [val.replace('PAD ', '').replace('PAD', '') if not isinstance(val, float) else '' for val in pid_train_df['previous_diags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer...\n",
      "\n",
      "\n",
      "\n",
      "Vocab size is 22569\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import Digits, Punctuation, Whitespace\n",
    "from tokenizers.normalizers import Lowercase, Replace\n",
    "from tokenizers import pre_tokenizers, normalizers\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "print('Training tokenizer...')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"UNK\"))\n",
    "tokenizer.normalizer = normalizers.Sequence([Lowercase()])\n",
    "# tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=False), Punctuation( behavior = 'removed')])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation(behavior = 'isolated')])\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=[\"<s>\", \"</s>\", \"PAD\", \"UNK\", \"$\"], min_frequency=10)\n",
    "\n",
    "files = glob.glob('/home/svetlana.maslenkova/LSTM/aki_prediction/txt_files/train'+'/*')\n",
    "tokenizer.train(files, trainer)\n",
    "tokenizer.post_processor = BertProcessing(\n",
    "        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "        (\"<s>\", tokenizer.token_to_id(\"<s>\")), \n",
    "        )\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(f'Vocab size is {tokenizer.get_vocab_size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22569\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(vocab_size)\n",
    "device='cpu'\n",
    "frac=1\n",
    "BATCH_SIZE=16\n",
    "LR=0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(pid_train_df.sample(frac=frac), tokenizer=tokenizer, max_length=max_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EHR_PRETRAINING(max_length=400, vocab_size=vocab_size, device=device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22553884, 25509314, 22279579, 28187167, 24420446, 25905565, 20764576,\n",
      "        29770896, 23424563, 24709146, 27972173, 28551210, 26626469, 26969230,\n",
      "        29925932, 25792248])\n"
     ]
    }
   ],
   "source": [
    "tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds, idx = next(iter(train_loader))\n",
    "print(idx)\n",
    "# for idx, (tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds, idx) in enumerate(train_dataset):\n",
    "#     print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp 97 . 7 heartrate 53 . 0 resprate 18 . 0 o2sat 10 . 0 sbp 129 . 0 dbp 99 . 0 rhythm pain'"
      ]
     },
     "execution_count": 811,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "tokenizer.decode(tensor_vitals[i][0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "influenza vaccine quadrivalent 0 . 5 ml ; bisacodyl 10 mg ; senna 8 . 6 mg ; heparin 5000 unit ; ipratropium - albuterol neb 1 neb ; albuterol 0 . 083 % neb soln 1 neb ; folic acid 1 mg ; multivitamins 1 tab ; nicotine patch 7 mg day ; thiamine 100 mg ; vitamin d 1000 unit ; potassium chloride replacement ( critical care and oncology ) 40 meq ; potassium chloride replacement ( critical care and oncology ) 60 meq ; potassium chloride replacement ( critical care and oncology ) 80 meq ; bag 1 bag ; magnesium sulfate 4 gm ; 0 . 9 % sodium chloride 100 ml ; calcium gluconate 2 g ; 0 . 9 % sodium chloride 250 ml ; calcium gluconate 4 g ; amlodipine 5 mg ; aspirin 81 mg ; chlorthalidone 25 mg ; nortriptyline 10 mg ; omeprazole 20 mg ; tamsulosin 0 . 4 mg ; azithromycin 500 mg ; prednisone 60 mg ; phytonadione 5 mg ; sodium chloride 0 . 9 % flush 3 - 10 ml ;\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "tokenizer.enable_truncation(200)\n",
    "out = tokenizer.encode('influenza vaccine quadrivalent  0.5  ml ; bisacodyl  10  mg ; senna  8.6  mg ; heparin  5000  unit ; ipratropium-albuterol neb  1  neb ; albuterol 0.083% neb soln  1  neb ; folic acid  1  mg ; multivitamins  1  tab ; nicotine patch  7  mg day ; thiamine  100  mg ; vitamin d  1000  unit ; potassium chloride replacement (critical care and oncology)   40  meq ; potassium chloride replacement (critical care and oncology)   60  meq ; potassium chloride replacement (critical care and oncology)   80  meq ; bag  1  bag ; magnesium sulfate  4  gm ; 0.9% sodium chloride  100  ml ; calcium gluconate  2  g ; 0.9% sodium chloride  250  ml ; calcium gluconate  4  g ; amlodipine  5  mg ; aspirin  81  mg ; chlorthalidone  25  mg ; nortriptyline  10  mg ; omeprazole  20  mg ; tamsulosin  0.4  mg ; azithromycin  500  mg ; prednisone  60  mg ; phytonadione  5  mg ; sodium chloride 0.9%  flush  3-10  ml ; ')\n",
    "print(tokenizer.decode(out.ids))\n",
    "print(len(out.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 478 565  17  26 487 960  17  19 489 280  17  19 488 117  17  19 485\n",
      " 988  17  19 483 326  17  19 475 479   1   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2] ===> temp 97 . 7 heartrate 53 . 0 resprate 18 . 0 o2sat 10 . 0 sbp 129 . 0 dbp 99 . 0 rhythm pain\n",
      "[0 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] ===> \n",
      "[0 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] ===> \n",
      "[0 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] ===> \n",
      "[0 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] ===> \n"
     ]
    }
   ],
   "source": [
    "for p in tensor_vitals[i]:\n",
    "    print(p.cpu().detach().numpy(), \"===>\", tokenizer.decode(p.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day = pid_train_df[pid_train_df.hadm_id==idx[i].item()].previous_diagnoses.values[0]\n",
    "day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de119 db20 de785 di10 dr338'"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[0, 4277, 849, 3387, 828, 3507, 1]\n"
     ]
    }
   ],
   "source": [
    "print(len(output.ids))\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <s>\n",
      "4277 d42843\n",
      "849 d2449\n",
      "3387 d5939\n",
      "828 d4280\n",
      "3507 d4239\n",
      "1 </s>\n"
     ]
    }
   ],
   "source": [
    "for id_ in output.ids:\n",
    "    print(id_, tokenizer.id_to_token(id_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(tensor_vitals[i].cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1091699/3265823591.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_vitals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_to_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "for id_ in tensor_vitals[i].cpu().detach().numpy():\n",
    "    print(id_, tokenizer.id_to_token(id_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7'"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.id_to_token(24)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f41d6bb73514239eeae9d84db1aabfbecf43feae35a358a827a9ce792198f6fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
