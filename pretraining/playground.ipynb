{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tokenizers import  Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Punctuation, Whitespace\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers import pre_tokenizers, normalizers\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "import glob\n",
    "from os.path import exists\n",
    "import os\n",
    "\n",
    "import pickle5 as pickle\n",
    "import wandb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(load_path, model, optimizer, device):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, batch_size, device, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.register_buffer(\"temperature\", torch.tensor(temperature))\n",
    "        self.register_buffer(\"negatives_mask\", (~torch.eye(batch_size * 2, batch_size * 2, dtype=bool)).float())\n",
    "            \n",
    "    def forward(self, emb_i, emb_j):\n",
    "        \"\"\"\n",
    "        emb_i and emb_j are batches of embeddings, where corresponding indices are pairs\n",
    "        z_i, z_j as per SimCLR paper\n",
    "        \"\"\"\n",
    "        z_i = F.normalize(emb_i, dim=1)\n",
    "        z_j = F.normalize(emb_j, dim=1)\n",
    "\n",
    "        representations = torch.cat([z_i, z_j], dim=0)\n",
    "        \n",
    "        similarity_matrix = F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=2)\n",
    "        \n",
    "        sim_ij = torch.diag(similarity_matrix, self.batch_size)\n",
    "        sim_ji = torch.diag(similarity_matrix, -self.batch_size)\n",
    "        positives = torch.cat([sim_ij, sim_ji], dim=0)\n",
    "        \n",
    "        nominator = torch.exp(positives / self.temperature)\n",
    "        denominator = self.negatives_mask.to(self.device) * torch.exp(similarity_matrix / self.temperature)\n",
    "    \n",
    "        loss_partial = -torch.log(nominator / torch.sum(denominator, dim=1))\n",
    "        loss = torch.sum(loss_partial) / (2 * self.batch_size)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset for three stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        batch_size,\n",
    "        file_path,\n",
    "        embedding_size,\n",
    "        device='cpu',\n",
    "        num_epochs=2,\n",
    "        epoch_patience=10,\n",
    "        best_valid_loss = float(\"Inf\"),\n",
    "        dimension=128,\n",
    "        save_model=True,\n",
    "        temperature=0.1):\n",
    "\n",
    "    \n",
    "    train_running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    total_train_steps = len(train_loader)\n",
    "    total_val_steps = len(valid_loader)\n",
    "    stop_training = 0\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_step = 1\n",
    "            print(f'Epoch {epoch+1}/{num_epochs} training..')\n",
    "            criterion = ContrastiveLoss(batch_size=batch_size, device=device, temperature=temperature)\n",
    "\n",
    "            for  tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds, idx in train_loader:\n",
    "                if train_step % 100==0:\n",
    "                    print(f'Step {train_step}/{total_train_steps}')\n",
    "                # print(f'Step {train_step}/{total_train_steps}')\n",
    "\n",
    "                vector_X, projectionX, vector_Y, projectionY = model(tensor_demo.to(device), tensor_diags.to(device),\\\n",
    "                                                                    tensor_meds.to(device), tensor_vitals.to(device),\\\n",
    "                                                                    tensor_labs.to(device))\n",
    "                                        \n",
    "                if train_step >= total_train_steps:\n",
    "                        new_batch_size = projectionX.size()[0]\n",
    "                        criterion = ContrastiveLoss(batch_size=new_batch_size, device=device, temperature=temperature)\n",
    "                \n",
    "                loss = criterion(projectionX.type(torch.float32), projectionY.type(torch.float32))\n",
    "                #   print(loss.item())\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_running_loss += loss.item()\n",
    "                train_step += 1\n",
    "\n",
    "                wandb.log({'step_train_loss': loss.item(), 'global_step': train_step})\n",
    "\n",
    "            epoch_average_train_loss = train_running_loss / len(train_loader)  \n",
    "\n",
    "            model.eval()\n",
    "            val_step = 1\n",
    "            print(f\"Validation started..\")\n",
    "            criterion = ContrastiveLoss(batch_size=batch_size, device=device, temperature=temperature)\n",
    "            with torch.no_grad():\n",
    "                for tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds, idx in valid_loader:\n",
    "                        vector_X, projectionX, vector_Y, projectionY = model(tensor_demo.to(device), tensor_diags.to(device),\\\n",
    "                                                                    tensor_meds.to(device), tensor_vitals.to(device),\\\n",
    "                                                                    tensor_labs.to(device))\n",
    "\n",
    "                        if val_step >= total_val_steps:\n",
    "                            new_batch_size = projectionX.size()[0]\n",
    "                            criterion = ContrastiveLoss(batch_size=new_batch_size, device=device, temperature=temperature)\n",
    "                                            \n",
    "                        loss = criterion(projectionX.type(torch.float32), projectionY.type(torch.float32))\n",
    "\n",
    "                        valid_running_loss += loss.item()\n",
    "                        val_step += 1\n",
    "                        \n",
    "\n",
    "            epoch_average_val_loss = valid_running_loss / len(valid_loader)\n",
    "\n",
    "            train_running_loss = 0.0\n",
    "            valid_running_loss = 0.0\n",
    "\n",
    "            print(f'Train loss {epoch_average_train_loss}, Validation loss {epoch_average_val_loss}')\n",
    "\n",
    "            wandb.log({'epoch_average_train_loss':epoch_average_train_loss, 'epoch_average_val_loss':epoch_average_val_loss, 'epoch':epoch+1})\n",
    "            \n",
    "            # checkpoint\n",
    "            if best_valid_loss > epoch_average_val_loss and save_model:\n",
    "                print(f'Validation loss decreased {best_valid_loss}==>{epoch_average_val_loss}')\n",
    "                best_valid_loss = epoch_average_val_loss\n",
    "                save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)\n",
    "                wandb.save(file_path + '/model.pt')\n",
    "                stop_training = 0\n",
    "            else:\n",
    "                stop_training +=1\n",
    "            \n",
    "            if stop_training == epoch_patience:\n",
    "                break\n",
    "\n",
    "    print('Finished training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fixed_model = True\n",
    "fixed_model_with_diags = False\n",
    "cont_model = False\n",
    "\n",
    "def main(project_name, num_epochs, pred_window, max_day, PRETRAINED_PATH=None, drop=0.1, \\\n",
    "    temperature=0.5, embedding_size=200, min_frequency=1, BATCH_SIZE=16, small_dataset=True, \\\n",
    "        LR=0.000005, save_model=False, use_gpu=True, saving_folder_name=None, wandb_mode = 'online', \\\n",
    "            run_id=None):\n",
    "    \n",
    "    if use_gpu:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    else:\n",
    "        device='cpu'\n",
    "    print('device: ', device)\n",
    "    \n",
    "    #paths\n",
    "    CURR_PATH = os.getcwd()\n",
    "    PKL_PATH = CURR_PATH+'/pickles/'\n",
    "    DF_PATH = '/home/svetlana.maslenkova/LSTM/dataframes/'\n",
    "    TXT_DIR_TRAIN = CURR_PATH + '/txt_files/train'\n",
    "    destination_folder = '/l/users/svetlana.maslenkova/models' + '/pretraining/fc1_fixed/'\n",
    "\n",
    "\n",
    "    # Training the tokenizer\n",
    "    if exists(CURR_PATH + '/tokenizer.json'):\n",
    "        tokenizer = Tokenizer.from_file(CURR_PATH + '/tokenizer.json')\n",
    "        print(f'Tokenizer is loaded from ==> {CURR_PATH}/tokenizer.json. Vocab size is {tokenizer.get_vocab_size()}')\n",
    "    else:\n",
    "        print('Training tokenizer...')\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"UNK\"))\n",
    "        tokenizer.normalizer = normalizers.Sequence([Lowercase()])\n",
    "        # tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=False), Punctuation( behavior = 'removed')])\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation(behavior = 'isolated')])\n",
    "\n",
    "        trainer = BpeTrainer(special_tokens=[\"<s>\", \"</s>\", \"PAD\", \"UNK\", \"$\"], min_frequency=10)\n",
    "\n",
    "        files = glob.glob('/home/svetlana.maslenkova/LSTM/aki_prediction/txt_files/train'+'/*')\n",
    "        tokenizer.train(files, trainer)\n",
    "        tokenizer.post_processor = BertProcessing(\n",
    "                (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "                (\"<s>\", tokenizer.token_to_id(\"<s>\")), \n",
    "                )\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    # variables for classes\n",
    "    # max_length = {'demographics':5, 'lab_tests':400, 'vitals':200, 'medications':255}\n",
    "    max_length = {'demographics':5+2, 'diagnoses':35+2, 'lab_tests':300+2, 'vitals':31+2, 'medications':256+2}\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    print(f'Vocab size: {vocab_size}')\n",
    "\n",
    "    with open(DF_PATH + 'pid_train_df_pretraining.pkl', 'rb') as f:\n",
    "        pid_train_df = pickle.load(f)\n",
    "\n",
    "    with open(DF_PATH + 'pid_val_df_pretraining.pkl', 'rb') as f:\n",
    "        pid_val_df = pickle.load(f)\n",
    "\n",
    "    if small_dataset: frac=0.0001 \n",
    "    else: frac=1\n",
    "    \n",
    "    pid_train_df_small = pid_train_df.sample(frac=frac)\n",
    "    pid_val_df_small = pid_val_df.sample(frac=frac)\n",
    "\n",
    "    if fixed_model_with_diags:\n",
    "        train_dataset = MyDataset(pid_train_df.sample(frac=frac), tokenizer=tokenizer, max_length_day=400)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        val_dataset = MyDataset(pid_val_df.sample(frac=frac), tokenizer=tokenizer, max_length_day=400)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    elif cont_model:\n",
    "        train_dataset = MyDataset(pid_train_df_small, tokenizer=tokenizer, max_length=max_length)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        val_dataset = MyDataset(pid_val_df_small, tokenizer=tokenizer, max_length=max_length)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    elif new_fixed_model:\n",
    "        train_dataset = MyDataset(pid_train_df.sample(frac=frac), tokenizer=tokenizer, max_length=max_length)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        val_dataset = MyDataset(pid_val_df.sample(frac=frac), tokenizer=tokenizer, max_length=max_length)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print('DATA SHAPES: ')\n",
    "    print('train data shape: ', len(train_loader)*BATCH_SIZE)\n",
    "    print('val data shape: ', len(val_loader)*BATCH_SIZE)\n",
    "\n",
    "    if fixed_model_with_diags:\n",
    "        model = EHR_PRETRAINING(max_length=400, vocab_size=vocab_size, device=device).to(device)\n",
    "    elif cont_model:\n",
    "        model = EHR_model(embedding_size=embedding_size, vocab_size=vocab_size, max_length=max_length, pred_window=pred_window, max_day=max_day, drop=0.1).to(device)\n",
    "    elif new_fixed_model:\n",
    "        model = EHR_PRETRAINING(max_length=max_length, vocab_size=vocab_size, device=device, pred_window=2, observing_window=3,  H=128, embedding_size=200, drop=0.6)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    if PRETRAINED_PATH is not None:\n",
    "        load_checkpoint(PRETRAINED_PATH, model, optimizer, device=device)\n",
    "    \n",
    "    train_params = {'model':model,\n",
    "                    'optimizer':optimizer,\n",
    "                    'train_loader':train_loader,\n",
    "                    'valid_loader':val_loader,\n",
    "                    'batch_size':BATCH_SIZE,\n",
    "                    'embedding_size':embedding_size,\n",
    "                    'file_path':destination_folder,\n",
    "                    'num_epochs':num_epochs,\n",
    "                    'device':device,\n",
    "                    'save_model':save_model,\n",
    "                    'temperature':temperature\n",
    "    }\n",
    "\n",
    "    num_samples = (len(train_loader)+len(val_loader))*BATCH_SIZE // 1000\n",
    "\n",
    "    if saving_folder_name is None:\n",
    "        saving_folder_name = 'CL_WHOLE_FX_ND_' + '_bs' + str(BATCH_SIZE) +'_' + str(num_samples) + 'k' + '_lr'+ str(LR) + '_Adam' + '_temp' + str(temperature) + '_drop' + str(drop)\n",
    "    file_path = destination_folder + saving_folder_name\n",
    "    train_params['file_path'] = file_path\n",
    "\n",
    "    print(f'\\n\\nMODEL PATH: {file_path}')\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "        \n",
    "    run_name = saving_folder_name\n",
    "    print('Run name: ', run_name)\n",
    "    args = {'optimizer':'Adam', 'LR':LR, 'min_frequency':min_frequency, 'dropout':drop, \\\n",
    "        'vocab_size':vocab_size, 'embedding_size':embedding_size, 'pretrained':'FC1', \\\n",
    "            'temperature':temperature, 'batch_size':BATCH_SIZE,  'experiment':'FX_DIAGS_ND', \\\n",
    "                'pretrained':'whole'}\n",
    "\n",
    "    if run_id is None:    \n",
    "        run_id = wandb.util.generate_id()  \n",
    "        resume = 'allow' \n",
    "    else:\n",
    "        resume = 'must'\n",
    "\n",
    "    print('Run id is: ', run_id)\n",
    "    wandb.init(project=project_name, name=run_name, mode=wandb_mode, config=args, id=run_id, resume=resume)\n",
    "    train(**train_params)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n",
      "Training tokenizer...\n",
      "\n",
      "\n",
      "\n",
      "Vocab size: 22569\n",
      "DATA SHAPES: \n",
      "train data shape:  48\n",
      "val data shape:  16\n",
      "\n",
      "\n",
      "MODEL PATH: /l/users/svetlana.maslenkova/models/pretraining/fc1_fixed/test\n",
      "Run name:  test\n",
      "Run id is:  14bnw4hp\n",
      "Epoch 1/1 training..\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[16, 9472]' is invalid for input of size 143360",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2540000/711473782.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_frequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmall_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mLR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_folder_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwandb_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'disabled'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             run_id=None)\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2540000/2615113654.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(project_name, num_epochs, pred_window, max_day, PRETRAINED_PATH, drop, temperature, embedding_size, min_frequency, BATCH_SIZE, small_dataset, LR, save_model, use_gpu, saving_folder_name, wandb_mode, run_id)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Run id is: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwandb_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2540000/1115800015.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, valid_loader, batch_size, file_path, embedding_size, device, num_epochs, epoch_patience, best_valid_loss, dimension, save_model, temperature)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 vector_X, projectionX, vector_Y, projectionY = model(tensor_demo.to(device), tensor_diags.to(device),\\\n\u001b[1;32m     38\u001b[0m                                                                     \u001b[0mtensor_meds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_vitals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                                                                     tensor_labs.to(device))\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtrain_step\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mtotal_train_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda_envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2540000/2208218639.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor_demo, tensor_diags, tensor_med, tensor_vitals, tensor_labs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# reshape and concat demographics and diags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mout_lstm_diags_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_lstm_diags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length_diags\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mout_lstm_demo_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_lstm_demo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length_demo\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[16, 9472]' is invalid for input of size 143360"
     ]
    }
   ],
   "source": [
    "main(project_name='test', num_epochs=1, pred_window=None, max_day=None, PRETRAINED_PATH=None, drop=0.1, \\\n",
    "    temperature=0.1, embedding_size=200, min_frequency=5, BATCH_SIZE=16, small_dataset=True, \\\n",
    "        LR=0.00001, save_model=True, use_gpu=False, saving_folder_name='test', wandb_mode = 'disabled', \\\n",
    "            run_id=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer is loaded from ==> /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/aki_prediction/pretraining/tokenizer.json. Vocab size is 22569\n"
     ]
    }
   ],
   "source": [
    "#paths\n",
    "CURR_PATH = os.getcwd()\n",
    "PKL_PATH = CURR_PATH+'/pickles/'\n",
    "DF_PATH = '/home/svetlana.maslenkova/LSTM/dataframes/'\n",
    "TXT_DIR_TRAIN = CURR_PATH + '/txt_files/train'\n",
    "destination_folder = '/l/users/svetlana.maslenkova/models' + '/pretraining/fc1_fixed/'\n",
    "\n",
    "# Training the tokenizer\n",
    "if exists(CURR_PATH + '/tokenizer.json'):\n",
    "    tokenizer = Tokenizer.from_file(CURR_PATH + '/tokenizer.json')\n",
    "    print(f'Tokenizer is loaded from ==> {CURR_PATH}/tokenizer.json. Vocab size is {tokenizer.get_vocab_size()}')\n",
    "else:\n",
    "    print('Training tokenizer...')\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"UNK\"))\n",
    "    tokenizer.normalizer = normalizers.Sequence([Lowercase()])\n",
    "    # tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=False), Punctuation( behavior = 'removed')])\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation(behavior = 'isolated')])\n",
    "\n",
    "    trainer = BpeTrainer(special_tokens=[\"<s>\", \"</s>\", \"PAD\", \"UNK\", \"$\"], min_frequency=10)\n",
    "\n",
    "    files = glob.glob('/home/svetlana.maslenkova/LSTM/aki_prediction/txt_files/train'+'/*')\n",
    "    tokenizer.train(files, trainer)\n",
    "    tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")), \n",
    "            )\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    tokenizer.save(CURR_PATH + '/tokenizer.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 22569\n"
     ]
    }
   ],
   "source": [
    "device='cpu'\n",
    "max_length = {'demographics':5+2, 'diagnoses':35+2, 'lab_tests':300+2, 'vitals':31+2, 'medications':256+2}\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f'Vocab size: {vocab_size}')\n",
    "\n",
    "small_dataset = True\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_PATH = '/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes/'\n",
    "\n",
    "with open(DF_PATH + 'pid_train_df_pretraining.pkl', 'rb') as f:\n",
    "    pid_train_df = pickle.load(f)\n",
    "\n",
    "with open(DF_PATH + 'pid_val_df_pretraining.pkl', 'rb') as f:\n",
    "    pid_val_df = pickle.load(f)\n",
    "\n",
    "if small_dataset: frac=0.0001 \n",
    "else: frac=1\n",
    "\n",
    "# pid_train_df_small = pid_train_df.sample(frac=frac)\n",
    "# pid_val_df_small = pid_val_df.sample(frac=frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_fixed_model\n"
     ]
    }
   ],
   "source": [
    "if fixed_model_with_diags:\n",
    "    train_dataset = MyDataset(pid_train_df.sample(frac=frac), tokenizer=tokenizer, max_length_day=400)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    val_dataset = MyDataset(pid_val_df.sample(frac=frac), tokenizer=tokenizer, max_length_day=400)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "elif cont_model:\n",
    "    train_dataset = MyDataset(pid_train_df_small, tokenizer=tokenizer, max_length=max_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    val_dataset = MyDataset(pid_val_df_small, tokenizer=tokenizer, max_length=max_length)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "elif new_fixed_model:\n",
    "    print(f'new_fixed_model')\n",
    "    train_dataset = MyDataset(pid_train_df.sample(frac=frac), tokenizer=tokenizer, max_length=max_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    val_dataset = MyDataset(pid_val_df.sample(frac=frac), tokenizer=tokenizer, max_length=max_length)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "if fixed_model_with_diags:\n",
    "    model = EHR_PRETRAINING(max_length=400, vocab_size=vocab_size, device=device).to(device)\n",
    "elif cont_model:\n",
    "    model = EHR_model(embedding_size=embedding_size, vocab_size=vocab_size, max_length=max_length, pred_window=pred_window, max_day=max_day, drop=0.1).to(device)\n",
    "elif new_fixed_model:\n",
    "    pretrained_model = EHR_PRETRAINING(max_length=max_length, vocab_size=vocab_size, device=device, pred_window=2, observing_window=3,  H=128, embedding_size=200, drop=0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 200\n",
    "num_epochs = 1\n",
    "save_model = False\n",
    "temperature = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "train_params = {'model':model,\n",
    "                'optimizer':optimizer,\n",
    "                'train_loader':train_loader,\n",
    "                'valid_loader':val_loader,\n",
    "                'batch_size':BATCH_SIZE,\n",
    "                'embedding_size':embedding_size,\n",
    "                'file_path':destination_folder,\n",
    "                'num_epochs':num_epochs,\n",
    "                'device':device,\n",
    "                'save_model':save_model,\n",
    "                'temperature':temperature\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 training..\n",
      "out_emb_diags:  torch.Size([36, 37, 200])\n",
      "out_emb_demo:  torch.Size([36, 7, 200])\n",
      "out_lstm_diags:  torch.Size([36, 37, 256])\n",
      "out_lstm_demo:  torch.Size([36, 7, 256])\n",
      "out_lstm_diags_reshaped torch.Size([36, 9472])\n",
      "out_lstm_demo_reshaped torch.Size([36, 1792])\n",
      "full_output torch.Size([36, 11264])\n",
      "out_med_emb torch.Size([36, 258, 200])\n",
      "out_vitals_emb torch.Size([36, 33, 200])\n",
      "out_labs_emb torch.Size([36, 302, 200])\n",
      "output_lstm_med torch.Size([36, 2048])\n",
      "output_lstm_vitals torch.Size([36, 2048])\n",
      "output_lstm_labs torch.Size([36, 2048])\n",
      "--------------\n",
      "out_med_emb torch.Size([36, 258, 200])\n",
      "out_vitals_emb torch.Size([36, 33, 200])\n",
      "out_labs_emb torch.Size([36, 302, 200])\n",
      "output_lstm_med torch.Size([36, 2048])\n",
      "output_lstm_vitals torch.Size([36, 2048])\n",
      "output_lstm_labs torch.Size([36, 2048])\n",
      "--------------\n",
      "out_med_emb torch.Size([36, 258, 200])\n",
      "out_vitals_emb torch.Size([36, 33, 200])\n",
      "out_labs_emb torch.Size([36, 302, 200])\n",
      "output_lstm_med torch.Size([36, 2048])\n",
      "output_lstm_vitals torch.Size([36, 2048])\n",
      "output_lstm_labs torch.Size([36, 2048])\n",
      "--------------\n",
      "full_output size:  torch.Size([36, 29696]) \n",
      "\n",
      "fc_adm:  torch.Size([36, 2048]) \n",
      "\n",
      "output_vector:  torch.Size([36, 128]) \n",
      "\n",
      "Validation started..\n",
      "out_emb_diags:  torch.Size([2, 37, 200])\n",
      "out_emb_demo:  torch.Size([2, 7, 200])\n",
      "out_lstm_diags:  torch.Size([2, 37, 256])\n",
      "out_lstm_demo:  torch.Size([2, 7, 256])\n",
      "out_lstm_diags_reshaped torch.Size([2, 9472])\n",
      "out_lstm_demo_reshaped torch.Size([2, 1792])\n",
      "full_output torch.Size([2, 11264])\n",
      "out_med_emb torch.Size([2, 258, 200])\n",
      "out_vitals_emb torch.Size([2, 33, 200])\n",
      "out_labs_emb torch.Size([2, 302, 200])\n",
      "output_lstm_med torch.Size([2, 2048])\n",
      "output_lstm_vitals torch.Size([2, 2048])\n",
      "output_lstm_labs torch.Size([2, 2048])\n",
      "--------------\n",
      "out_med_emb torch.Size([2, 258, 200])\n",
      "out_vitals_emb torch.Size([2, 33, 200])\n",
      "out_labs_emb torch.Size([2, 302, 200])\n",
      "output_lstm_med torch.Size([2, 2048])\n",
      "output_lstm_vitals torch.Size([2, 2048])\n",
      "output_lstm_labs torch.Size([2, 2048])\n",
      "--------------\n",
      "out_med_emb torch.Size([2, 258, 200])\n",
      "out_vitals_emb torch.Size([2, 33, 200])\n",
      "out_labs_emb torch.Size([2, 302, 200])\n",
      "output_lstm_med torch.Size([2, 2048])\n",
      "output_lstm_vitals torch.Size([2, 2048])\n",
      "output_lstm_labs torch.Size([2, 2048])\n",
      "--------------\n",
      "full_output size:  torch.Size([2, 29696]) \n",
      "\n",
      "fc_adm:  torch.Size([2, 2048]) \n",
      "\n",
      "output_vector:  torch.Size([2, 128]) \n",
      "\n",
      "Train loss 4.495946884155273, Validation loss 1.0028973817825317\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "train(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_emb_diags:  torch.Size([36, 37, 200])\n",
      "out_emb_demo:  torch.Size([36, 7, 200])\n",
      "out_lstm_diags:  torch.Size([36, 37, 256])\n",
      "out_lstm_demo:  torch.Size([36, 7, 256])\n",
      "out_lstm_diags_reshaped torch.Size([36, 9472])\n",
      "out_lstm_demo_reshaped torch.Size([36, 1792])\n",
      "full_output torch.Size([36, 11264])\n",
      "--------------\n",
      "out_med_emb torch.Size([36, 258, 200])\n",
      "out_vitals_emb torch.Size([36, 33, 200])\n",
      "out_labs_emb torch.Size([36, 302, 200])\n",
      "\n",
      "lstm_day(out_med_emb)  torch.Size([36, 258, 256]) ==> [36, 66048]\n",
      "output_lstm_med torch.Size([36, 2048])\n",
      "\n",
      "lstm_day(out_vitals_emb)  torch.Size([36, 33, 256]) ==> [36, 8448]\n",
      "output_lstm_vitals torch.Size([36, 2048])\n",
      "\n",
      "lstm_day(out_labs_emb)  torch.Size([36, 302, 256]) ==> [36, 77312]\n",
      "output_lstm_labs torch.Size([36, 2048])\n",
      "--------------\n",
      "out_med_emb torch.Size([36, 258, 200])\n",
      "out_vitals_emb torch.Size([36, 33, 200])\n",
      "out_labs_emb torch.Size([36, 302, 200])\n",
      "\n",
      "lstm_day(out_med_emb)  torch.Size([36, 258, 256]) ==> [36, 66048]\n",
      "output_lstm_med torch.Size([36, 2048])\n",
      "\n",
      "lstm_day(out_vitals_emb)  torch.Size([36, 33, 256]) ==> [36, 8448]\n",
      "output_lstm_vitals torch.Size([36, 2048])\n",
      "\n",
      "lstm_day(out_labs_emb)  torch.Size([36, 302, 256]) ==> [36, 77312]\n",
      "output_lstm_labs torch.Size([36, 2048])\n",
      "--------------\n",
      "out_med_emb torch.Size([36, 258, 200])\n",
      "out_vitals_emb torch.Size([36, 33, 200])\n",
      "out_labs_emb torch.Size([36, 302, 200])\n",
      "\n",
      "lstm_day(out_med_emb)  torch.Size([36, 258, 256]) ==> [36, 66048]\n",
      "output_lstm_med torch.Size([36, 2048])\n",
      "\n",
      "lstm_day(out_vitals_emb)  torch.Size([36, 33, 256]) ==> [36, 8448]\n",
      "output_lstm_vitals torch.Size([36, 2048])\n",
      "\n",
      "lstm_day(out_labs_emb)  torch.Size([36, 302, 256]) ==> [36, 77312]\n",
      "output_lstm_labs torch.Size([36, 2048])\n",
      "--------------\n",
      "full_output size:  torch.Size([36, 29696]) \n",
      "\n",
      "fc_adm:  torch.Size([36, 2048]) \n",
      "\n",
      "output_vector:  torch.Size([36, 128]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds, idx = next(iter(train_loader))\n",
    "\n",
    "vector_X, projectionX, vector_Y, projectionY = model(tensor_demo.to(device), tensor_diags.to(device),\\\n",
    "                                                    tensor_meds.to(device), tensor_vitals.to(device),\\\n",
    "                                                    tensor_labs.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = {'demographics':5+2, 'diagnoses':35+2, 'lab_tests':300+2, 'vitals':31+2, 'medications':256+2}\n",
    "tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds, idx = next(iter(train_loader))\n",
    "model = EHR_PRETRAINING(max_length=max_length, vocab_size=vocab_size, device=device, pred_window=2, observing_window=3,  H=128, embedding_size=200, drop=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 35])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_diags.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "CURR_PATH = os.getcwd()\n",
    "PKL_PATH = CURR_PATH+'/pickles/'\n",
    "DF_PATH = CURR_PATH +'/dataframes/'\n",
    "TXT_DIR_TRAIN = CURR_PATH + '/txt_files/train'\n",
    "destination_folder = '/l/users/svetlana.maslenkova/models' + '/pretraining/fc1_fixed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer.from_file('/home/svetlana.maslenkova/LSTM/aki_prediction/tokenizer.json')\n",
    "# print(f' Vocab size is {tokenizer.get_vocab_size()}')\n",
    "\n",
    "with open('/home/svetlana.maslenkova/LSTM/dataframes/pid_train_df_pretraining.pkl', 'rb') as f:\n",
    "    pid_train_df = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>demographics_in_visit</th>\n",
       "      <th>lab_tests_in_visit</th>\n",
       "      <th>medications_in_visit</th>\n",
       "      <th>vitals_in_visit</th>\n",
       "      <th>days_in_visit</th>\n",
       "      <th>previous_diagnoses</th>\n",
       "      <th>days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10467237</td>\n",
       "      <td>20000019</td>\n",
       "      <td>[hispanic latino f 76  , hispanic latino f 76 ...</td>\n",
       "      <td>[hematology blood hematocrit  26.5  %; hematol...</td>\n",
       "      <td>[pneumococcal 23-valent polysaccharide vaccine...</td>\n",
       "      <td>[temp  98.0  heartrate  65.0  resprate  16.0  ...</td>\n",
       "      <td>[hispanic latino f 76  $temp  98.0  heartrate ...</td>\n",
       "      <td></td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id                              demographics_in_visit  \\\n",
       "0    10467237  20000019  [hispanic latino f 76  , hispanic latino f 76 ...   \n",
       "\n",
       "                                  lab_tests_in_visit  \\\n",
       "0  [hematology blood hematocrit  26.5  %; hematol...   \n",
       "\n",
       "                                medications_in_visit  \\\n",
       "0  [pneumococcal 23-valent polysaccharide vaccine...   \n",
       "\n",
       "                                     vitals_in_visit  \\\n",
       "0  [temp  98.0  heartrate  65.0  resprate  16.0  ...   \n",
       "\n",
       "                                       days_in_visit previous_diagnoses  \\\n",
       "0  [hispanic latino f 76  $temp  98.0  heartrate ...                      \n",
       "\n",
       "        days  \n",
       "0  [0, 1, 2]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "LIST_HADMS = []\n",
    "list_diags = []\n",
    "# for _, row in pid_val_df.iterrows():\n",
    "#     # try:\n",
    "#     #     list_diags.append(row.previous_diagnoses[0].replace('PAD ', '').replace('PAD', ''))\n",
    "#     #     LIST_HADMS.append(row.hadm_id)\n",
    "#     # except:\n",
    "#     #     print(row.previous_diagnoses[0])\n",
    "#     #     print(np.isnan(row.previous_diagnoses[0]))\n",
    "#     #     break\n",
    "#     try:\n",
    "#         if isinstance(row.previous_diagnoses[0], float):\n",
    "#             list_diags.append('')      \n",
    "#         else:\n",
    "#             list_diags.append(row.previous_diagnoses[0].replace('PAD ', '').replace('PAD', ''))  \n",
    "#         LIST_HADMS.append(row.hadm_id) \n",
    "#     except:\n",
    "#         print(row.previous_diagnoses[0])\n",
    "#         print(type(row.previous_diagnoses[0]))\n",
    "#         raise\n",
    "#         break\n",
    "\n",
    "#     i+=1\n",
    "#     # if i>0:break\n",
    "\n",
    "pid_train_df['previous_diags'] = [val.replace('PAD ', '').replace('PAD', '') if not isinstance(val, float) else '' for val in pid_train_df['previous_diags']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer is loaded from ==> /home/svetlanamaslenkova/Documents/AKI_deep/LSTM/aki_prediction/pretraining/tokenizer.json/tokenizer.json. Vocab size is 22569\n"
     ]
    }
   ],
   "source": [
    "#paths\n",
    "diagnoses = 'icd'\n",
    "min_frequency = 10\n",
    "CURR_PATH = os.getcwd()\n",
    "PKL_PATH = CURR_PATH+'/pickles/'\n",
    "DF_PATH = CURR_PATH +'/dataframes/'\n",
    "destination_folder = '/l/users/svetlana.maslenkova/models' + '/pretraining/three_stages/'\n",
    "# destination_folder = '/home/svetlanamaslenkova/Documents/AKI_deep/training/'\n",
    "\n",
    "if diagnoses=='icd':\n",
    "        TOKENIZER_PATH = CURR_PATH  + '/tokenizer.json'\n",
    "        TXT_DIR_TRAIN = CURR_PATH + '/txt_files/train'\n",
    "elif diagnoses=='titles':\n",
    "        TOKENIZER_PATH = CURR_PATH + '/tokenizer_titles.json'\n",
    "        TXT_DIR_TRAIN = CURR_PATH + '/txt_files/titles_diags'\n",
    "\n",
    "# Training the tokenizer\n",
    "if exists(TOKENIZER_PATH):\n",
    "        tokenizer = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "        print(f'Tokenizer is loaded from ==> {TOKENIZER_PATH}/tokenizer.json. Vocab size is {tokenizer.get_vocab_size()}')\n",
    "else:\n",
    "        print('Training tokenizer...')\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"UNK\"))\n",
    "        tokenizer.normalizer = normalizers.Sequence([Lowercase()])\n",
    "        # tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=False), Punctuation( behavior = 'removed')])\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation(behavior = 'isolated')])\n",
    "\n",
    "        trainer = BpeTrainer(special_tokens=[\"<s>\", \"</s>\", \"PAD\", \"UNK\", \"$\"], min_frequency=min_frequency)\n",
    "\n",
    "        files = glob.glob(TXT_DIR_TRAIN+'/*')\n",
    "        tokenizer.train(files, trainer)\n",
    "        tokenizer.post_processor = BertProcessing(\n",
    "                (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "                (\"<s>\", tokenizer.token_to_id(\"<s>\")), \n",
    "                )\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "        print(f'Vocab size is {tokenizer.get_vocab_size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22569\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(vocab_size)\n",
    "device='cpu'\n",
    "frac=1\n",
    "BATCH_SIZE=1024\n",
    "LR=0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(pid_train_df.sample(frac=frac), tokenizer=tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EHR_PRETRAINING' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11379/3832573699.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEHR_PRETRAINING\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EHR_PRETRAINING' is not defined"
     ]
    }
   ],
   "source": [
    "model = EHR_PRETRAINING(max_length=400, vocab_size=vocab_size, device=device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([25608940, 26676014, 25229043, 22517940, 20539214, 28675100, 21815284,\n",
      "        22431587, 24259871, 29837158, 23507274, 27676611, 21343164, 20376368,\n",
      "        28143288, 27172224])\n"
     ]
    }
   ],
   "source": [
    "tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds, idx = next(iter(train_loader))\n",
    "print(idx)\n",
    "# for idx, (tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds, idx) in enumerate(train_dataset):\n",
    "#     print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp 98 . 8 heartrate 96 . 0 resprate 18 . 0 o2sat 97 . 0 sbp 100 . 0 dbp 59 . 0 rhythm pain 8'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "tokenizer.decode(tensor_vitals[i][0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0,0,0,0,1,0])\n",
    "idx = np.where(a==1)[0][0]\n",
    "a[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens_meds = []\n",
    "tokenizer.enable_truncation(500)\n",
    "for tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds, idx in train_loader:\n",
    "    for sample in tensor_meds.cpu().detach():\n",
    "        for day in sample:\n",
    "            # print(day)\n",
    "            idx = np.where(day==1)[0][0]\n",
    "            # print(len(day[:idx]))\n",
    "            lens_meds.append(len(day[:idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Num of tensors')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(lens, density=False, bins=150)\n",
    "plt.ylim(0, 200000)\n",
    "plt.grid()\n",
    "plt.title('Distribution of number of tokens in laboratory results tensors')\n",
    "plt.xlabel('Num of tokens')\n",
    "plt.ylabel('Num of tensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Num of tensors')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "print(np.max(lens_meds))\n",
    "plt.hist(lens_meds, density=False, bins=150)\n",
    "plt.ylim(0, 60000)\n",
    "plt.grid()\n",
    "plt.title('Distribution of number of tokens in medications tensors')\n",
    "plt.xlabel('Num of tokens')\n",
    "plt.ylabel('Num of tensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 478 565  17  26 487 960  17  19 489 280  17  19 488 117  17  19 485\n",
      " 988  17  19 483 326  17  19 475 479   1   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2] ===> temp 97 . 7 heartrate 53 . 0 resprate 18 . 0 o2sat 10 . 0 sbp 129 . 0 dbp 99 . 0 rhythm pain\n",
      "[0 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] ===> \n",
      "[0 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] ===> \n",
      "[0 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] ===> \n",
      "[0 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] ===> \n"
     ]
    }
   ],
   "source": [
    "for p in tensor_vitals[i]:\n",
    "    print(p.cpu().detach().numpy(), \"===>\", tokenizer.decode(p.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day = pid_train_df[pid_train_df.hadm_id==idx[i].item()].previous_diagnoses.values[0]\n",
    "day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de119 db20 de785 di10 dr338'"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[0, 4277, 849, 3387, 828, 3507, 1]\n"
     ]
    }
   ],
   "source": [
    "print(len(output.ids))\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <s>\n",
      "4277 d42843\n",
      "849 d2449\n",
      "3387 d5939\n",
      "828 d4280\n",
      "3507 d4239\n",
      "1 </s>\n"
     ]
    }
   ],
   "source": [
    "for id_ in output.ids:\n",
    "    print(id_, tokenizer.id_to_token(id_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(tensor_vitals[i].cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1091699/3265823591.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_vitals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_to_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "for id_ in tensor_vitals[i].cpu().detach().numpy():\n",
    "    print(id_, tokenizer.id_to_token(id_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7'"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.id_to_token(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Functions\n",
    "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(load_path, model, optimizer, device):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_metrics(load_path):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']\n",
    "\n",
    "def calculate_class_weights(data_loader):\n",
    "    labels = np.array([])\n",
    "    for tensor_day, tensor_diags, tensor_labels, idx in data_loader:\n",
    "        labels = np.concatenate([labels, tensor_labels], axis=0) if labels.size else tensor_labels\n",
    "    n_pos = np.sum(labels==1, axis=0)\n",
    "    n_neg = np.sum(labels==0, axis=0)\n",
    "    pos_weight = np.round(n_neg / n_pos, 2)\n",
    "    \n",
    "    return pos_weight\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHR_FINETUNING(nn.Module):\n",
    "    def __init__(self, pretrained_model, max_length, vocab_size, device, pred_window=2, observing_window=3,  H=128, embedding_size=200, drop=0.6):\n",
    "        super(EHR_FINETUNING, self).__init__()\n",
    "\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.H = H\n",
    "        self.max_length_demo = max_length['demographics']\n",
    "        self.max_length_diags = max_length['diagnoses']\n",
    "        self.max_length_meds = max_length['medications']\n",
    "        self.max_length_vitals = max_length['vitals']\n",
    "        self.max_length_lab = max_length['lab_tests']\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device\n",
    "        self.drop = drop\n",
    "\n",
    "        # self.embedding = pretrained_model\n",
    "        self.embedding = pretrained_model.embedding\n",
    "\n",
    "        self.lstm_day = pretrained_model.lstm_day\n",
    "\n",
    "        self.fc_med = pretrained_model.fc_med\n",
    "        self.fc_vitals = pretrained_model.fc_vitals\n",
    "        self.fc_lab = pretrained_model.fc_lab\n",
    "\n",
    "        self.fc_adm = pretrained_model.fc_adm\n",
    "\n",
    "        self.lstm_adm = pretrained_model.lstm_adm\n",
    "\n",
    "        self.drop = nn.Dropout(p=drop)\n",
    "        self.inner_drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc_2 = nn.Linear(pretrained_model.lstm_adm.hidden_size, 2)\n",
    "\n",
    "    def forward(self, tensor_demo, tensor_diags, tensor_med, tensor_vitals, tensor_labs):\n",
    "\n",
    "        batch_size = tensor_demo.size()[0]\n",
    "\n",
    "        # embeddings\n",
    "        out_emb_diags = self.embedding(tensor_diags.squeeze(1)) # [16, 37, 200]\n",
    "        out_emb_demo =  self.embedding(tensor_demo.squeeze(1))  # [16, 7, 200]\n",
    "        # print(f'out_emb_diags: ', out_emb_diags.size())\n",
    "        # print(f'out_emb_demo: ', out_emb_demo.size())\n",
    "        # lstm for demographic and diagnoses\n",
    "        out_lstm_diags, _ = self.lstm_day(out_emb_diags)    # [16, 37, 256]\n",
    "        out_lstm_demo, _ = self.lstm_day(out_emb_demo)      # [16, 7, 256]\n",
    "        # print(f'out_lstm_diags: ', out_lstm_diags.size())\n",
    "        # print(f'out_lstm_demo: ', out_lstm_demo.size())\n",
    "        # reshape and concat demographics and diags\n",
    "        out_lstm_diags_reshaped = out_lstm_diags.reshape(batch_size, self.max_length_diags * 2 * self.H)\n",
    "        out_lstm_demo_reshaped = out_lstm_demo.reshape(batch_size, self.max_length_demo * 2 * self.H)\n",
    "        # print(f'out_lstm_diags_reshaped', out_lstm_diags_reshaped.size())\n",
    "        # print(f'out_lstm_demo_reshaped', out_lstm_demo_reshaped.size())\n",
    "        full_output = torch.cat([out_lstm_demo_reshaped, out_lstm_diags_reshaped], dim=1)   # [16, 11264]\n",
    "        # print(f'full_output', full_output.size())\n",
    "\n",
    "        for d in range(self.observing_window):\n",
    "            # print('--------------')  \n",
    "            # embedding layer applied to all tensors \n",
    "            out_med_emb = self.embedding(tensor_med[:, d, :].squeeze(1))\n",
    "            out_vitals_emb = self.embedding(tensor_vitals[:, d, :].squeeze(1))\n",
    "            out_labs_emb = self.embedding(tensor_labs[:, d, :].squeeze(1))\n",
    "            # print('out_med_emb', out_med_emb.size())\n",
    "            # print('out_vitals_emb', out_vitals_emb.size())\n",
    "            # print('out_labs_emb', out_labs_emb.size())\n",
    "\n",
    "            # lstm layer applied to embedded tensors\n",
    "            temp = self.lstm_day(out_med_emb)[0]\n",
    "            # print(f'\\nlstm_day(out_med_emb)  {temp.size()} ==> [{batch_size}, {self.max_length_meds * 2 * self.H}]')\n",
    "            temp = temp.reshape(batch_size, self.max_length_meds * 2 * self.H)\n",
    "            output_lstm_med = self.inner_drop(self.fc_med(temp))\n",
    "            # print(f'output_lstm_med {output_lstm_med.size()}')\n",
    "            output_lstm_med = self.inner_drop(self.fc_med(\\\n",
    "                                                self.lstm_day(out_med_emb)[0]\\\n",
    "                                                    .reshape(batch_size, self.max_length_meds * 2 * self.H)))\n",
    "\n",
    "            output_lstm_vitals = self.inner_drop(self.fc_vitals(\\\n",
    "                                                self.lstm_day(out_vitals_emb)[0]\\\n",
    "                                                    .reshape(batch_size, self.max_length_vitals * 2 * self.H)))\n",
    "\n",
    "\n",
    "            output_lstm_labs = self.inner_drop(self.fc_lab(\\\n",
    "                                                self.lstm_day(out_labs_emb)[0]\\\n",
    "                                                    .reshape(batch_size, self.max_length_lab * 2 * self.H)))\n",
    "                         \n",
    "            # concatenate for all * days\n",
    "            full_output = torch.cat((full_output, \\\n",
    "                                        output_lstm_med,\\\n",
    "                                            output_lstm_vitals,\\\n",
    "                                                output_lstm_labs), dim=1) # \n",
    "\n",
    "        # print('--------------')  \n",
    "        # print('full_output size: ', full_output.size(), '\\n')\n",
    "        output = self.fc_adm(full_output)\n",
    "        # print('fc_adm: ', output.size(), '\\n')\n",
    "        output, _ = self.lstm_adm(output)\n",
    "        # print('lstm_adm: ', output.size(), '\\n')\n",
    "        output = self.drop(output)\n",
    "        output = self.fc_2(output)\n",
    "        # print('output after fc_2', output.size())\n",
    "        output = torch.squeeze(output, 1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_length, pred_window=2, observing_window=3):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.max_length = max_length\n",
    "        # self.max_length_diags = 35\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.make_matrices(idx)\n",
    "    \n",
    "    def tokenize(self, text, max_length): \n",
    "        \n",
    "        # max_length = max_length + 2\n",
    "        self.tokenizer.enable_truncation(max_length=max_length)\n",
    "\n",
    "        output = self.tokenizer.encode(text)\n",
    "\n",
    "        # padding and truncation\n",
    "        if len(output.ids) < max_length:\n",
    "            len_missing_token = max_length - len(output.ids)\n",
    "            padding_vec = [self.tokenizer.token_to_id('PAD') for _ in range(len_missing_token)]\n",
    "            token_output = [*output.ids, *padding_vec]\n",
    "        elif len(output.ids) > max_length:\n",
    "            token_output = output.ids[:max_length]\n",
    "        else:\n",
    "            token_output = output.ids\n",
    "        \n",
    "        return token_output\n",
    "\n",
    "    def make_matrices(self, idx):\n",
    "        \n",
    "        hadm_id = self.df.hadm_id.values[idx]\n",
    "        diagnoses_info = self.df.previous_diagnoses.values[idx]\n",
    "        demo_info = self.df.demographics_in_visit.values[idx][0]\n",
    "        lab_info = self.df.lab_tests_in_visit.values[idx]\n",
    "        med_info = self.df.medications_in_visit.values[idx]\n",
    "        vitals_info = self.df.vitals_in_visit.values[idx]\n",
    "        \n",
    "        aki_status = self.df.aki_status_in_visit.values[idx]\n",
    "        days = self.df.days.values[idx]\n",
    "        # print(idx)\n",
    "\n",
    "        lab_info_list = []\n",
    "        med_info_list = []\n",
    "        vitals_info_list = []\n",
    "        labels = []\n",
    "        label_24 = None\n",
    "        label_48 = None\n",
    "\n",
    "        for day in range(days[0], days[0] + self.observing_window + self.pred_window):\n",
    "            # print('day', day)\n",
    "            if day not in days:\n",
    "                labels.append(0)\n",
    "                vitals_info_list.append(self.tokenize('', self.max_length['vitals']))\n",
    "                lab_info_list.append(self.tokenize('', self.max_length['lab_tests']))\n",
    "                med_info_list.append(self.tokenize('', self.max_length['medications']))\n",
    "\n",
    "            else:\n",
    "                i = days.index(day)\n",
    "\n",
    "                if np.isfinite(aki_status[i]):                    \n",
    "                    labels.append(aki_status[i])\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "\n",
    "                # vitals\n",
    "                if (str(vitals_info[i]) == 'nan') or (vitals_info[i] == np.nan):\n",
    "                    vitals_info_list.append(self.tokenize('PAD', self.max_length['vitals']))\n",
    "                else:\n",
    "                    vitals_info_list.append(self.tokenize(vitals_info[i], self.max_length['vitals']))\n",
    "\n",
    "                # lab results\n",
    "                if (str(lab_info[i]) == 'nan') or (lab_info[i] == np.nan):\n",
    "                    lab_info_list.append(self.tokenize('PAD', self.max_length['lab_tests']))\n",
    "                else:\n",
    "                    lab_info_list.append(self.tokenize(lab_info[i], self.max_length['lab_tests']))\n",
    "                \n",
    "                # medications\n",
    "                if (str(med_info[i]) == 'nan') or (med_info[i] == np.nan):\n",
    "                    med_info_list.append(self.tokenize('PAD', self.max_length['medications']))\n",
    "                else:\n",
    "                    med_info_list.append(self.tokenize(med_info[i], self.max_length['medications']))\n",
    "\n",
    "        # diagnoses\n",
    "        if (str(diagnoses_info) == 'nan') or (diagnoses_info == np.nan):\n",
    "            diagnoses_info = self.tokenize('PAD', self.max_length['diagnoses'])\n",
    "        else:\n",
    "            diagnoses_info = self.tokenize(diagnoses_info, self.max_length['diagnoses'])\n",
    "\n",
    "        # demographics\n",
    "        if (str(demo_info) == 'nan') or (demo_info == np.nan):\n",
    "            demo_info = self.tokenize('PAD', self.max_length_diags)\n",
    "        else:\n",
    "            demo_info = self.tokenize(demo_info, self.max_length['demographics'])\n",
    "\n",
    "        # get labels for 48h\n",
    "\n",
    "        # get labels for 24h\n",
    "        label_24 = labels[-self.pred_window]\n",
    "        if sum(labels[-self.pred_window:]) > 0:\n",
    "            label_48 = 1\n",
    "        else:\n",
    "            label_48 = 0\n",
    "        \n",
    "        #make tensors\n",
    "        tensor_demo = torch.tensor(demo_info, dtype=torch.int64)\n",
    "        tensor_diags = torch.tensor(diagnoses_info, dtype=torch.int64)\n",
    "        tensor_vitals = torch.tensor(vitals_info_list, dtype=torch.int64)\n",
    "        tensor_labs = torch.tensor(lab_info_list, dtype=torch.int64)\n",
    "        tensor_meds = torch.tensor(med_info_list, dtype=torch.int64)\n",
    "        tensor_labels = torch.tensor([label_24, label_48], dtype=torch.float64)\n",
    "    \n",
    "        return (tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds), hadm_id, tensor_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds), hadm_id, tensor_labels = next(iter(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>demographics_in_visit</th>\n",
       "      <th>lab_tests_in_visit</th>\n",
       "      <th>medications_in_visit</th>\n",
       "      <th>vitals_in_visit</th>\n",
       "      <th>days_in_visit</th>\n",
       "      <th>aki_status_in_visit</th>\n",
       "      <th>previous_diagnoses</th>\n",
       "      <th>days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16679562</td>\n",
       "      <td>20001395</td>\n",
       "      <td>[hispanic latino m 73, hispanic latino m 73, h...</td>\n",
       "      <td>[hematology blood hematocrit  51.2  %; hematol...</td>\n",
       "      <td>[influenza vaccine quadrivalent  0.5  ml ; bis...</td>\n",
       "      <td>[temp    heartrate  80.0  resprate  16.0  o2sa...</td>\n",
       "      <td>[hispanic latino m 73$temp    heartrate  80.0 ...</td>\n",
       "      <td>[0, 0, 0, 1, 1, 0, 0, 1, 0]</td>\n",
       "      <td></td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id                              demographics_in_visit  \\\n",
       "9    16679562  20001395  [hispanic latino m 73, hispanic latino m 73, h...   \n",
       "\n",
       "                                  lab_tests_in_visit  \\\n",
       "9  [hematology blood hematocrit  51.2  %; hematol...   \n",
       "\n",
       "                                medications_in_visit  \\\n",
       "9  [influenza vaccine quadrivalent  0.5  ml ; bis...   \n",
       "\n",
       "                                     vitals_in_visit  \\\n",
       "9  [temp    heartrate  80.0  resprate  16.0  o2sa...   \n",
       "\n",
       "                                       days_in_visit  \\\n",
       "9  [hispanic latino m 73$temp    heartrate  80.0 ...   \n",
       "\n",
       "           aki_status_in_visit previous_diagnoses                         days  \n",
       "9  [0, 0, 0, 1, 1, 0, 0, 1, 0]                     [0, 1, 2, 3, 4, 5, 6, 7, 8]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/home/svetlana.maslenkova/LSTM/dataframes/pid_train_df_finetuning_6days_aki.pkl', 'rb') as f:\n",
    "    pid_train_df = pickle.load(f)\n",
    "\n",
    "with open('/home/svetlana.maslenkova/LSTM/dataframes/pid_test_df_finetuning_6days_aki.pkl', 'rb') as f:\n",
    "    pid_test_df = pickle.load(f)\n",
    "\n",
    "pid_train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "frac=1\n",
    "train_dataset = MyDataset(pid_train_df.sample(frac=frac), tokenizer=tokenizer, max_length=max_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = MyDataset(pid_test_df.sample(frac=frac), tokenizer=tokenizer, max_length=max_length)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1755"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds), hadm_id, tensor_labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : (27380510, array([1., 1.]))\n",
      "1 : (22132103, array([0., 0.]))\n",
      "2 : (28381869, array([0., 0.]))\n",
      "3 : (20853656, array([1., 1.]))\n",
      "4 : (23652398, array([0., 0.]))\n",
      "5 : (27668515, array([0., 0.]))\n",
      "6 : (22928509, array([1., 1.]))\n",
      "7 : (25748555, array([0., 0.]))\n",
      "8 : (22169742, array([0., 0.]))\n",
      "9 : (25569065, array([1., 1.]))\n",
      "10 : (27512939, array([0., 0.]))\n",
      "11 : (21164517, array([0., 0.]))\n",
      "12 : (24279583, array([1., 1.]))\n",
      "13 : (24635431, array([1., 1.]))\n",
      "14 : (27483857, array([0., 0.]))\n",
      "15 : (29877900, array([1., 1.]))\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(zip(hadm_id.cpu().detach().numpy(), tensor_labels.cpu().detach().numpy())):\n",
    "    print(i,':', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>demographics_in_visit</th>\n",
       "      <th>lab_tests_in_visit</th>\n",
       "      <th>medications_in_visit</th>\n",
       "      <th>vitals_in_visit</th>\n",
       "      <th>days_in_visit</th>\n",
       "      <th>aki_status_in_visit</th>\n",
       "      <th>previous_diagnoses</th>\n",
       "      <th>days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10995547</td>\n",
       "      <td>20003740</td>\n",
       "      <td>[white m 69, white m 69, white m 69, white m 6...</td>\n",
       "      <td>[hematology blood hematocrit  30.9  %; hematol...</td>\n",
       "      <td>[lansoprazole oral disintegrating tab  30  mg ...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[white m 69$nan$hematology blood hematocrit  3...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>D51919 D2639 D30000 DV4986 DE915 DV1582 D9351 ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject_id   hadm_id                              demographics_in_visit  \\\n",
       "28    10995547  20003740  [white m 69, white m 69, white m 69, white m 6...   \n",
       "\n",
       "                                   lab_tests_in_visit  \\\n",
       "28  [hematology blood hematocrit  30.9  %; hematol...   \n",
       "\n",
       "                                 medications_in_visit  \\\n",
       "28  [lansoprazole oral disintegrating tab  30  mg ...   \n",
       "\n",
       "                                      vitals_in_visit  \\\n",
       "28  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "\n",
       "                                        days_in_visit  \\\n",
       "28  [white m 69$nan$hematology blood hematocrit  3...   \n",
       "\n",
       "                              aki_status_in_visit  \\\n",
       "28  [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                   previous_diagnoses  \\\n",
       "28  D51919 D2639 D30000 DV4986 DE915 DV1582 D9351 ...   \n",
       "\n",
       "                                                 days  \n",
       "28  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_train_df[pid_train_df.hadm_id==20003740]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_dataset = MyDataset(pid_train_df[pid_train_df.hadm_id==20003740], tokenizer=tokenizer, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [0, 0, 0, 0, 1]\n",
      "label_24 0\n",
      "label48 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((tensor([  0, 241,  51, 549,   1,   2,   2]),\n",
       "  tensor([   0, 2636, 1793,  914, 1714, 5619,  745, 9614, 3656, 3040, 5520, 1346,\n",
       "          2301, 6817, 8868, 3321, 5520,  745, 2207, 1337, 8868, 2636, 2435,    1,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2]),\n",
       "  tensor([[0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "           2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "          [0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "           2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "          [0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "           2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "          [0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "           2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "          [0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "           2, 2, 2, 2, 2, 2, 2, 2, 2]]),\n",
       "  tensor([[  0, 104,  79,  ...,   2,   2,   2],\n",
       "          [  0, 104,  79,  ...,   2,   2,   2],\n",
       "          [  0, 104,  79,  ...,   2,   2,   2],\n",
       "          [  0, 104,  79,  ...,   2,   2,   2],\n",
       "          [  0, 104,  79,  ...,   2,   2,   2]]),\n",
       "  tensor([[   0, 1343,  514,  ...,    2,    2,    2],\n",
       "          [   0, 1343,  514,  ...,    2,    2,    2],\n",
       "          [   0, 1343,  514,  ...,    2,    2,    2],\n",
       "          [   0, 1343,  514,  ...,    2,    2,    2],\n",
       "          [   0, 1343,  514,  ...,    2,    2,    2]])),\n",
       " 20003740,\n",
       " tensor([0., 1.], dtype=torch.float64))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score, accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "def evaluate(model, test_loader, device, threshold=None, log_res=True):\n",
    "    print('Evaluation..')\n",
    "    def find_nearest(array, value):\n",
    "        array = np.asarray(array)\n",
    "        idx = (np.abs(array - value)).argmin()\n",
    "        return idx\n",
    "\n",
    "    device = 'cpu'\n",
    "    model = model.to(device)\n",
    "    stacked_labels = torch.tensor([]).to(device)\n",
    "    stacked_probs = torch.tensor([]).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    step = 1\n",
    "    with torch.no_grad():\n",
    "        for (tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds), hadm_id, tensor_labels in test_loader:\n",
    "            print(f'Step {step}/{len(test_loader)}' )\n",
    "            labels = tensor_labels.to(device)\n",
    "            tensor_demo = tensor_demo.to(device)\n",
    "            tensor_diags = tensor_diags.to(device)\n",
    "            tensor_vitals = tensor_vitals.to(device)\n",
    "            tensor_labs = tensor_labs.to(device)\n",
    "            tensor_meds = tensor_meds.to(device)\n",
    "\n",
    "            probs = model(tensor_demo.to(device), tensor_diags.to(device), tensor_meds.to(device), \\\n",
    "                                tensor_vitals.to(device), tensor_labs.to(device))\n",
    "            probs = nn.Sigmoid()(probs)\n",
    "            # output = (probs > threshold).int()\n",
    "\n",
    "            # stacking labels and predictions\n",
    "            stacked_labels = torch.cat([stacked_labels, labels], dim=0, )\n",
    "            # stacked_preds = torch.cat([stacked_preds, output], dim=0, )\n",
    "            stacked_probs = torch.cat([stacked_probs, probs], dim=0, )\n",
    "            step += 1\n",
    "            \n",
    "    # transfer to device\n",
    "    stacked_labels = stacked_labels.cpu().detach().numpy()\n",
    "    stacked_probs = stacked_probs.cpu().detach().numpy()\n",
    "\n",
    "    if threshold==None:\n",
    "        for w in range(stacked_labels.ndim):\n",
    "            pred_window = (w+1)*24\n",
    "\n",
    "            precision, recall, thresholds = precision_recall_curve(stacked_labels.T[w], stacked_probs.T[w])\n",
    "            precision, recall, thresholds = np.round(precision, 2), np.round(recall,2), np.round(thresholds,2)\n",
    "            \n",
    "            # convert to f score\n",
    "            fscore = np.round((2 * precision * recall) / (precision + recall), 2)\n",
    "\n",
    "            # locate the index of the largest f score\n",
    "            ix = np.argmax(np.nan_to_num(fscore))\n",
    "            threshold = np.round(thresholds[ix], 2)\n",
    "            print('--------------- ', str(pred_window)+'h', '--------------- ')\n",
    "            print('Best Threshold=%.2f, F-Score=%.2f' % (threshold, fscore[ix]))\n",
    "\n",
    "            stacked_preds = (stacked_probs.T[w] > threshold).astype(int)\n",
    "            y_true = stacked_labels.T[0]\n",
    "            y_pred = stacked_preds\n",
    "\n",
    "            accuracy = np.round(accuracy_score(y_true, y_pred), 2)\n",
    "            print(f'Accuracy: {accuracy}')\n",
    "\n",
    "            f1_score_ = np.round(f1_score(y_true, y_pred, pos_label=1, average='binary', zero_division=0), 2)\n",
    "            print(f'F1: ', f1_score_)\n",
    "\n",
    "            recall_score_ = np.round(recall_score(y_true, y_pred, pos_label=1, average='binary', zero_division=0), 2)\n",
    "            print(f'Sensitivity: ', recall_score_)\n",
    "\n",
    "            precision_score_ = np.round(precision_score(y_true, y_pred, pos_label=1, average='binary', zero_division=0), 2)\n",
    "            print(f'Precision: ', precision_score_)\n",
    "\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "            specificity =  np.round(tn / (tn + fp), 2)\n",
    "            print(f'Specificity: ', specificity)\n",
    "\n",
    "            pr_auc = np.round(auc(recall, precision), 2) \n",
    "            print(f'PR AUC: ', pr_auc)\n",
    "\n",
    "            roc_auc = np.round(roc_auc_score(y_true, y_pred), 2)\n",
    "            print(f'ROC AUC: ', roc_auc)\n",
    "\n",
    "            precision_list = [0.2, 0.25, 0.33, 0.4, 0.5, 0.6, 0.75]\n",
    "            for p in precision_list:\n",
    "                idx = find_nearest(precision, p)\n",
    "                sensitivity = recall[idx]\n",
    "                print(f'Precision {np.round(precision[idx]*100, 1)}% , Sensitivity {sensitivity} ')\n",
    "\n",
    "            print(f'Confusion matrix:\\n', confusion_matrix(y_true, y_pred))\n",
    "\n",
    "\n",
    "            if log_res:\n",
    "                wandb.log({'test_accuracy'+str(pred_window) :accuracy, 'test_f1_score'+str(pred_window):f1_score_, \\\n",
    "                            'test_recall_score'+str(pred_window):recall_score_, 'test_precision_score'+str(pred_window):precision_score_, \\\n",
    "                                'test_specificity'+str(pred_window):specificity})\n",
    "\n",
    "            # get classification metrics for all samples in the test set\n",
    "            classification_report_res = classification_report(y_true, y_pred, zero_division=0, output_dict=True)\n",
    "            print(classification_report(y_true, y_pred, zero_division=0, output_dict=False))\n",
    "\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = EHR_PRETRAINING(max_length=max_length, vocab_size=vocab_size, device=device, pred_window=2, observing_window=3,  H=128, embedding_size=200, drop=0.6)\n",
    "ft_model = EHR_FINETUNING(pretrained_model, max_length, vocab_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = ft_model(tensor_demo.to(device), tensor_diags.to(device), tensor_meds.to(device), \\\n",
    "                                tensor_vitals.to(device), tensor_labs.to(device))\n",
    "probs = nn.Sigmoid()(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_labels = tensor_labels.cpu().detach().numpy()\n",
    "stacked_probs = probs.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 24h Threshold=0.47, F-Score=0.53\n",
      "Best 48h Threshold=0.48, F-Score=0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svetlana.maslenkova/conda_envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "precision24, recall24, thresholds24 = precision_recall_curve(stacked_labels.T[0], stacked_probs.T[0])\n",
    "precision48, recall48, thresholds48 = precision_recall_curve(stacked_labels.T[1], stacked_probs.T[1])\n",
    "\n",
    "# convert to f score\n",
    "fscore24 = np.round((2 * precision24 * recall24) / (precision24 + recall24), 2)\n",
    "fscore48 = np.round((2 * precision48 * recall48) / (precision48 + recall48), 2)\n",
    "\n",
    "# locate the index of the largest f score\n",
    "ix24 = np.argmax(np.nan_to_num(fscore24))\n",
    "threshold24 = np.round(thresholds24[ix24], 2)\n",
    "print('Best 24h Threshold=%.2f, F-Score=%.2f' % (threshold24, fscore24[ix24]))\n",
    "\n",
    "ix48 = np.argmax(np.nan_to_num(fscore48))\n",
    "threshold48 = np.round(thresholds48[ix48], 2)\n",
    "print('Best 48h Threshold=%.2f, F-Score=%.2f' % (threshold48, fscore48[ix48]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation..\n",
      "Step 1/4\n",
      "Step 2/4\n",
      "Step 3/4\n",
      "Step 4/4\n",
      "---------------  24h --------------- \n",
      "Best Threshold=0.51, F-Score=0.54\n",
      "Accuracy: 0.38\n",
      "F1:  0.54\n",
      "Sensitivity:  0.98\n",
      "Precision:  0.37\n",
      "Specificity:  0.03\n",
      "PR AUC:  0.3658\n",
      "ROC AUC:  0.5012357305856555\n",
      "Precision 27.0% , Sensitivity 0.03 \n",
      "Precision 27.0% , Sensitivity 0.03 \n",
      "Precision 33.0% , Sensitivity 0.09 \n",
      "Precision 40.0% , Sensitivity 0.03 \n",
      "Precision 50.0% , Sensitivity 0.01 \n",
      "Precision 50.0% , Sensitivity 0.01 \n",
      "Precision 50.0% , Sensitivity 0.01 \n",
      "Confusion matrix:\n",
      " [[  30 1076]\n",
      " [  16  633]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.03      0.05      1106\n",
      "         1.0       0.37      0.98      0.54       649\n",
      "\n",
      "    accuracy                           0.38      1755\n",
      "   macro avg       0.51      0.50      0.29      1755\n",
      "weighted avg       0.55      0.38      0.23      1755\n",
      "\n",
      "---------------  48h --------------- \n",
      "Best Threshold=0.51, F-Score=0.54\n",
      "Accuracy: 0.37\n",
      "F1:  0.54\n",
      "Sensitivity:  1.0\n",
      "Precision:  0.37\n",
      "Specificity:  0.0\n",
      "PR AUC:  0.382\n",
      "ROC AUC:  0.4981408314920437\n",
      "Precision 20.0% , Sensitivity 0.0 \n",
      "Precision 29.0% , Sensitivity 0.0 \n",
      "Precision 33.0% , Sensitivity 0.01 \n",
      "Precision 40.0% , Sensitivity 0.38 \n",
      "Precision 50.0% , Sensitivity 0.03 \n",
      "Precision 54.0% , Sensitivity 0.02 \n",
      "Precision 54.0% , Sensitivity 0.02 \n",
      "Confusion matrix:\n",
      " [[   1 1105]\n",
      " [   3  646]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.25      0.00      0.00      1106\n",
      "         1.0       0.37      1.00      0.54       649\n",
      "\n",
      "    accuracy                           0.37      1755\n",
      "   macro avg       0.31      0.50      0.27      1755\n",
      "weighted avg       0.29      0.37      0.20      1755\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svetlana.maslenkova/conda_envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:50: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "evaluate(ft_model, test_loader, device, threshold=None, log_res=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length_day=400, diags='icd', pred_window=2, observing_window=2):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.max_length_day = max_length_day\n",
    "        self.diags = diags\n",
    "\n",
    "        if self.diags == 'titles':\n",
    "            self.max_length_diags = 400\n",
    "        else:\n",
    "            self.max_length_diags = 40\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.make_matrices(idx)\n",
    "    \n",
    "    def tokenize(self, text, max_length):\n",
    "        \n",
    "        try:\n",
    "            output = self.tokenizer.encode(text)\n",
    "        except:\n",
    "            # print(idx, type(text), text, max_length)\n",
    "            output = self.tokenizer.encode(text[0])\n",
    "\n",
    "        # padding and truncation\n",
    "        if len(output.ids) < max_length:\n",
    "            len_missing_token = max_length - len(output.ids)\n",
    "            padding_vec = [self.tokenizer.token_to_id('PAD') for _ in range(len_missing_token)]\n",
    "            token_output = [*output.ids, *padding_vec]\n",
    "        elif len(output.ids) > max_length:\n",
    "            token_output = output.ids[:max_length]\n",
    "        else:\n",
    "            token_output = output.ids\n",
    "        \n",
    "        return token_output\n",
    "\n",
    "    def make_matrices(self, idx):\n",
    "        \n",
    "        hadm_id = self.df.hadm_id.values[idx]\n",
    "        \n",
    "        if self.diags == 'titles':\n",
    "            diagnoses_info = self.df.previous_diags_titles.values[idx][0]\n",
    "        else:\n",
    "            diagnoses_info = self.df.previous_diags_icd.values[idx][0]\n",
    "        \n",
    "        day_info = self.df.days_in_visit.values[idx]\n",
    "        days = self.df.days.values[idx]\n",
    "        # print(hadm_id)\n",
    "        # print(days)\n",
    "        day_info_list = []\n",
    "\n",
    "        for day in range(0, self.observing_window + self.pred_window):\n",
    "            \n",
    "            if day not in days:\n",
    "                day_info_list.append(self.tokenize('', self.max_length_day))\n",
    "            else:\n",
    "                # print('day', day)\n",
    "                i = days.index(day)\n",
    "\n",
    "                if (str(day_info[i]) == 'nan') or (day_info[i] == np.nan):\n",
    "                    day_info_list.append(self.tokenize('PAD', self.max_length_day))\n",
    "                else:\n",
    "                    day_info_list.append(self.tokenize(day_info[i], self.max_length_day))\n",
    "\n",
    "        # diagnoses\n",
    "        if (str(diagnoses_info) == 'nan') or (diagnoses_info == np.nan):\n",
    "            diagnoses_info = self.tokenize('PAD', self.max_length_diags)\n",
    "        else:\n",
    "            diagnoses_info = self.tokenize(diagnoses_info, self.max_length_diags)\n",
    "\n",
    "\n",
    "        #make tensors\n",
    "        tensor_day = torch.tensor(day_info_list[:self.observing_window], dtype=torch.int64)\n",
    "        tensor_diags = torch.tensor(diagnoses_info, dtype=torch.int64)\n",
    "        tensor_labels = torch.tensor([], dtype=torch.int64)\n",
    "\n",
    "        return tensor_day, tensor_diags, tensor_labels, hadm_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHR_PRETRAINING(nn.Module):\n",
    "    def __init__(self, max_length, vocab_size, device, diags='icd', pred_window=2, observing_window=2,  H=128, embedding_size=200, drop=0.1):\n",
    "        super(EHR_PRETRAINING, self).__init__()\n",
    "\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.H = H\n",
    "        self.max_length = max_length\n",
    "        self.drop = drop\n",
    "        \n",
    "        if diags == 'titles':\n",
    "            self.max_length_diags = 400\n",
    "        else:\n",
    "            self.max_length_diags = 40\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device\n",
    "        self.drop = drop\n",
    "\n",
    "        # self.embedding = pretrained_model\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "\n",
    "        self.lstm_day = nn.LSTM(input_size=embedding_size,\n",
    "                            hidden_size=self.H,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "\n",
    "        self.lstm_diags = nn.LSTM(input_size=embedding_size,\n",
    "                            hidden_size=self.H,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "\n",
    "        self.fc_day = nn.Linear(self.max_length * 2 * self.H, 2048)\n",
    "\n",
    "        self.fc_adm = nn.Linear(2048*self.observing_window +  self.max_length_diags * 2 * self.H, 2048)\n",
    "\n",
    "        self.lstm_adm = nn.LSTM(input_size=2048,\n",
    "                            hidden_size=self.H,\n",
    "                            num_layers=2,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "\n",
    "        self.drop = nn.Dropout(p=drop)\n",
    "        self.inner_drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=2*self.H, out_features=2*256)\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor_day, tensor_diagnoses):\n",
    "\n",
    "        batch_size = tensor_day.size()[0]\n",
    "\n",
    "        out_emb_diags = self.embedding(tensor_diagnoses.squeeze(1))\n",
    "        # print('out_emb_diags: ', out_emb_diags.size())\n",
    "        out_lstm_diags, _ = self.lstm_diags(out_emb_diags)\n",
    "        # print('out_lstm_diags: ', out_lstm_diags.size())\n",
    "        full_output = out_lstm_diags.reshape(batch_size, self.max_length_diags * 2 * self.H)\n",
    "        \n",
    "\n",
    "        for d in range(self.observing_window):\n",
    "            # embedding layer applied to all tensors [16,400,200]\n",
    "            out_emb = self.embedding(tensor_day[:, d, :].squeeze(1))\n",
    "            # print('out_emb', out_emb.size())\n",
    "\n",
    "            # lstm layer applied to embedded tensors\n",
    "            output_lstm_day= self.fc_day(\\\n",
    "                                    self.lstm_day(out_emb)[0]\\\n",
    "                                        .reshape(batch_size, self.max_length * 2 * self.H))\n",
    "\n",
    "            # print('output_lstm_day', output_lstm_day.size())                   \n",
    "            # concatenate for all * days\n",
    "            full_output = torch.cat([full_output, output_lstm_day], dim=1) # [16, 768]\n",
    "\n",
    "        # print('full_output size: ', full_output.size(), '\\n')\n",
    "        output = self.fc_adm (full_output)\n",
    "        # print('output after fc_adm size: ', output.size(), '\\n')\n",
    "        output_vector, _ = self.lstm_adm(output)\n",
    "        # the fisrt transformation\n",
    "        output_vector_X = self.drop(output_vector)\n",
    "        projection_X = self.projection(output_vector_X)\n",
    "        # # the second transformation\n",
    "        output_vector_Y = self.drop(output_vector)\n",
    "        projection_Y = self.projection(output_vector_Y)\n",
    "\n",
    "        return output_vector_X,  projection_X, output_vector_Y, projection_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "frac = 0.001\n",
    "\n",
    "train_dataset = MyDataset(pid_train_df.sample(frac=frac), tokenizer=tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 400\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "device = 'cuda'\n",
    "\n",
    "model = EHR_PRETRAINING(max_length, vocab_size, device, diags='icd').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_day, tensor_diags, tensor_labels, hadm_id = next(iter(train_loader))\n",
    "tensor_day, tensor_diags = tensor_day.to(device), tensor_diags.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vector_X, projection_X, output_vector_Y, projection_Y = model(tensor_day, tensor_diags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9420e-02,  3.5314e-05,  1.4487e-02,  ...,  5.6660e-02,\n",
       "          5.6011e-02, -8.8976e-02],\n",
       "        [-1.8303e-02, -3.1647e-03,  1.8133e-02,  ...,  5.2081e-02,\n",
       "          5.9263e-02, -7.2527e-02],\n",
       "        [-2.3190e-02, -7.4278e-03,  1.6173e-02,  ...,  4.4069e-02,\n",
       "          5.2465e-02, -6.9551e-02]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projection_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0265,  0.0046,  0.0117,  ...,  0.0540,  0.0617, -0.0869],\n",
       "        [-0.0202, -0.0055,  0.0105,  ...,  0.0485,  0.0690, -0.0690],\n",
       "        [-0.0131, -0.0027,  0.0106,  ...,  0.0527,  0.0561, -0.0664]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projection_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0078,  0.0000, -0.0143,  ...,  0.1013,  0.0357, -0.1036],\n",
       "        [-0.0169, -0.1032,  0.0000,  ...,  0.0000,  0.0000, -0.1101],\n",
       "        [ 0.0000, -0.1060, -0.0245,  ...,  0.0869,  0.0431, -0.1062],\n",
       "        ...,\n",
       "        [-0.0212, -0.1618, -0.0482,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0285, -0.1446,  0.0000,  ...,  0.0000,  0.0189, -0.0596],\n",
       "        [-0.0335, -0.1426, -0.0418,  ...,  0.0528,  0.0118, -0.0361]],\n",
       "       device='cuda:0', grad_fn=<NativeDropoutBackward0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_vector_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00, -5.8968e-02, -2.2629e-03, -2.3544e-02,  3.3400e-02,\n",
       "         -2.7205e-02, -4.4431e-02,  9.9583e-03, -4.7166e-02,  3.7175e-02,\n",
       "         -8.9546e-03,  2.2536e-03, -5.5320e-03,  3.2667e-02, -5.6383e-04,\n",
       "          3.1778e-02, -1.9815e-02, -3.8474e-02, -1.7781e-02, -1.6577e-02,\n",
       "          4.1587e-02, -7.1100e-03,  1.2259e-02, -2.0063e-02,  1.2659e-02,\n",
       "         -3.0761e-02,  1.8774e-02,  9.1431e-03,  0.0000e+00, -5.2403e-03,\n",
       "          4.2864e-02, -3.6183e-02, -9.6499e-03, -7.6416e-03,  3.5689e-04,\n",
       "          1.1887e-02,  1.6931e-02,  3.1938e-02, -3.6014e-02, -5.8834e-03,\n",
       "          7.8818e-03, -1.7443e-02,  3.1470e-02, -3.6839e-02, -4.1719e-02,\n",
       "         -2.7083e-02, -2.9044e-02, -4.1725e-03, -1.3064e-02,  2.9032e-02,\n",
       "          0.0000e+00, -1.2770e-02,  6.2197e-03,  1.2823e-02,  1.8895e-02,\n",
       "         -1.9691e-04,  1.0021e-02, -2.0564e-02, -1.7412e-02,  9.1874e-03,\n",
       "         -1.2178e-02, -1.7402e-02, -6.7556e-03, -2.0202e-02, -1.2921e-03,\n",
       "          4.1124e-03,  2.7269e-02,  5.1775e-02,  1.2275e-02,  5.1484e-02,\n",
       "          0.0000e+00, -1.5388e-02,  0.0000e+00, -3.8686e-03, -2.2848e-02,\n",
       "          4.0632e-02,  5.0013e-02,  4.0992e-02,  0.0000e+00, -1.1173e-02,\n",
       "         -1.6195e-02,  2.2757e-02, -6.4245e-04, -3.8040e-04,  1.8562e-02,\n",
       "          5.1488e-03,  2.2550e-03, -1.4087e-02,  0.0000e+00, -2.2605e-02,\n",
       "          5.5016e-02,  0.0000e+00, -2.0344e-02,  1.9317e-02,  1.8058e-02,\n",
       "          9.7866e-03,  0.0000e+00,  9.2142e-03,  0.0000e+00, -2.8884e-02,\n",
       "          2.6980e-03,  0.0000e+00, -2.3257e-02, -4.2748e-02, -1.6589e-02,\n",
       "          1.9147e-02, -8.2847e-03, -1.7883e-02, -4.0743e-03, -4.3249e-02,\n",
       "         -2.3542e-02,  0.0000e+00, -3.4612e-02,  1.5968e-02, -1.3055e-02,\n",
       "         -2.1133e-02, -6.5457e-03, -1.8604e-03, -1.8896e-02, -4.8719e-02,\n",
       "         -2.3641e-02,  2.7267e-03, -5.0593e-03, -6.7880e-03, -9.3131e-03,\n",
       "          2.1987e-02, -3.6293e-03,  5.9154e-03, -4.9878e-02,  3.0214e-02,\n",
       "          3.6929e-02,  5.7573e-02,  6.2571e-02,  1.1374e-02,  6.7542e-03,\n",
       "          3.7550e-02,  0.0000e+00, -1.2736e-02,  1.4638e-02, -2.4484e-02,\n",
       "         -9.2365e-03,  0.0000e+00, -6.0872e-02, -2.2609e-02,  0.0000e+00,\n",
       "         -5.6700e-02, -8.4753e-02,  2.6159e-02, -6.5714e-02,  2.4312e-02,\n",
       "         -3.3465e-02, -3.1535e-02,  0.0000e+00,  4.3063e-02, -2.1369e-02,\n",
       "         -3.6520e-02, -1.0432e-02,  6.6419e-03,  1.0913e-02, -3.7909e-02,\n",
       "          0.0000e+00,  7.3361e-02,  1.1766e-02,  6.4118e-02,  4.9485e-02,\n",
       "         -2.9288e-02,  2.8453e-02,  6.8137e-04, -6.4852e-02, -5.3868e-03,\n",
       "         -1.4515e-02, -8.6449e-04,  0.0000e+00, -3.2908e-02, -1.1630e-02,\n",
       "         -3.7688e-02, -1.4684e-05,  6.8408e-02,  2.0096e-02,  0.0000e+00,\n",
       "          1.9176e-04,  0.0000e+00,  6.4743e-02, -2.7015e-02, -2.6453e-02,\n",
       "         -3.7065e-02, -5.7731e-02, -1.4002e-02,  3.7748e-02, -8.6541e-02,\n",
       "          0.0000e+00,  9.3722e-02, -2.9837e-02,  1.4288e-02,  7.2003e-03,\n",
       "         -4.8567e-02,  8.9061e-03,  2.7829e-02,  3.0939e-02,  3.0509e-02,\n",
       "          1.3244e-02,  4.3272e-02,  1.7205e-02,  1.1322e-01,  3.5448e-02,\n",
       "          2.7810e-02, -1.5070e-02,  1.6912e-02,  7.3819e-02,  0.0000e+00,\n",
       "          0.0000e+00, -3.3265e-03, -3.7374e-02,  8.5963e-03, -2.7242e-02,\n",
       "          1.4253e-02, -1.7392e-02, -3.6721e-03,  5.8042e-03, -4.2598e-02,\n",
       "          6.5752e-02, -1.7325e-03,  6.4199e-02, -1.9566e-02, -2.7547e-02,\n",
       "          2.1793e-04, -4.1044e-02,  5.5196e-02, -1.2040e-01,  9.4914e-02,\n",
       "          1.0342e-02, -5.6780e-02,  4.9349e-03,  6.7404e-02, -2.5461e-02,\n",
       "         -5.1175e-02,  0.0000e+00, -2.6770e-02, -5.2941e-02,  5.2722e-02,\n",
       "          6.6383e-02, -9.9272e-03,  3.1467e-02,  5.6454e-02, -4.7739e-02,\n",
       "          8.4813e-04, -1.2924e-02,  1.4637e-02, -4.8877e-02, -3.5161e-02,\n",
       "         -4.9769e-02, -1.2029e-02,  0.0000e+00,  6.3341e-02, -1.5232e-02,\n",
       "         -8.1699e-04],\n",
       "        [ 4.4064e-04, -9.2130e-02,  3.0124e-03,  0.0000e+00,  6.1425e-02,\n",
       "         -5.3217e-02, -6.4354e-02,  3.5823e-02, -7.7740e-02,  4.7216e-02,\n",
       "         -1.1058e-02, -7.7235e-03, -9.2048e-03,  5.6708e-02, -1.4546e-02,\n",
       "          6.2733e-02,  0.0000e+00, -5.7538e-02, -4.5996e-02,  7.0660e-03,\n",
       "          0.0000e+00,  7.2862e-03, -2.7050e-04, -6.5487e-03,  4.2617e-03,\n",
       "         -3.0654e-02, -1.3809e-03, -1.1820e-02, -3.1807e-02, -4.2966e-02,\n",
       "          6.2517e-02,  0.0000e+00,  5.0573e-03,  0.0000e+00, -1.1941e-02,\n",
       "         -3.7415e-03, -5.3566e-03,  3.7806e-02, -4.8215e-02, -7.3804e-03,\n",
       "         -4.4065e-03, -1.6809e-02,  2.9037e-02, -4.6820e-02, -3.2866e-02,\n",
       "         -2.5085e-02, -4.0859e-02, -1.1793e-02, -8.7336e-03,  2.7451e-02,\n",
       "          1.9070e-02, -1.2943e-02,  0.0000e+00,  1.5119e-02,  0.0000e+00,\n",
       "          0.0000e+00,  1.7865e-02, -4.6888e-02, -6.6430e-02,  2.7287e-02,\n",
       "          3.5269e-03, -5.2788e-02, -3.1461e-02, -3.9436e-02, -1.5656e-02,\n",
       "         -1.6611e-02,  3.5170e-02,  5.5966e-02,  2.8230e-02,  6.2533e-02,\n",
       "         -1.1365e-02,  1.3957e-02, -4.6999e-02, -2.3137e-02, -3.9704e-02,\n",
       "          5.1150e-02,  4.6459e-02,  0.0000e+00, -4.0807e-02, -1.0488e-02,\n",
       "         -2.3095e-02,  3.0140e-02,  8.9689e-03, -1.3017e-02,  3.1088e-02,\n",
       "          0.0000e+00, -8.6249e-04, -9.8349e-03, -4.6323e-02, -4.0180e-02,\n",
       "          6.0380e-02, -3.1590e-02, -4.9816e-02,  0.0000e+00,  0.0000e+00,\n",
       "          1.1802e-02, -6.3491e-02,  3.5069e-02,  2.7889e-02, -2.2427e-02,\n",
       "         -2.5658e-02,  2.1491e-02, -2.9830e-02, -9.2090e-02, -1.3759e-02,\n",
       "          6.6272e-03, -1.2863e-02, -3.0267e-02, -4.1890e-03, -6.1272e-02,\n",
       "         -4.9616e-02, -7.7048e-02, -4.7733e-02,  2.1951e-02, -4.3095e-02,\n",
       "         -1.6137e-02, -1.4918e-03, -9.9527e-03, -2.5234e-02, -6.8063e-02,\n",
       "         -3.3273e-02, -1.3708e-02, -1.6323e-02,  2.4763e-02,  2.8278e-03,\n",
       "          3.1862e-02, -1.9376e-02, -4.1219e-03, -6.0251e-02,  3.1480e-02,\n",
       "          2.7580e-02,  3.5136e-02,  1.4083e-02, -1.5264e-03,  0.0000e+00,\n",
       "          4.8154e-02, -4.2267e-02, -9.8175e-03,  3.3475e-02, -5.3819e-02,\n",
       "         -1.4452e-02,  3.5763e-02, -3.9590e-02, -4.0347e-02, -3.8323e-02,\n",
       "          0.0000e+00, -7.2553e-02,  2.9989e-03, -4.7417e-02,  3.4360e-02,\n",
       "         -1.5096e-02, -1.2147e-02,  1.2121e-02,  0.0000e+00, -2.6510e-02,\n",
       "          0.0000e+00, -1.0581e-02,  9.4362e-03,  1.1265e-02, -4.0101e-02,\n",
       "         -5.4065e-02,  4.4502e-02, -1.4341e-03,  4.4571e-02,  5.5283e-02,\n",
       "         -2.8362e-02,  4.0288e-03,  2.1290e-02, -5.1432e-02, -1.0544e-02,\n",
       "          0.0000e+00,  4.8739e-03,  1.4115e-02, -3.0541e-02, -6.6523e-04,\n",
       "         -1.7444e-02, -2.3619e-03,  3.9641e-02,  3.5416e-03,  5.1299e-03,\n",
       "          3.3307e-03,  4.7797e-03,  4.1637e-02, -2.4090e-02, -1.2749e-03,\n",
       "         -4.9010e-02, -6.6634e-02, -2.0718e-02,  2.1042e-02, -6.5954e-02,\n",
       "          1.1406e-02,  7.0133e-02, -6.6580e-03,  2.6751e-03,  1.1513e-04,\n",
       "         -3.9826e-02,  7.9691e-03,  2.6268e-02,  4.3423e-02,  1.3625e-02,\n",
       "          2.0792e-02,  3.4084e-02, -5.6255e-03,  1.2395e-01,  3.2796e-02,\n",
       "          2.6945e-02, -2.2188e-02,  3.4408e-04,  0.0000e+00,  1.7883e-02,\n",
       "          2.7060e-02,  3.4359e-03,  0.0000e+00,  1.7479e-02, -2.9560e-02,\n",
       "          1.3989e-02, -1.1924e-02, -1.7728e-02,  6.6797e-04, -4.3498e-02,\n",
       "          6.4659e-02, -5.4043e-03,  5.5489e-02, -8.7301e-03, -2.5985e-02,\n",
       "         -8.9978e-03, -3.2987e-02,  4.6685e-02, -1.0140e-01,  5.5287e-02,\n",
       "         -6.2042e-03, -4.5178e-02,  8.1054e-03,  0.0000e+00, -2.2831e-02,\n",
       "         -6.1973e-02,  3.0423e-02, -1.9383e-02, -5.9643e-02,  2.6346e-02,\n",
       "          3.6116e-02, -5.5788e-03,  0.0000e+00,  6.8378e-02,  0.0000e+00,\n",
       "          7.7033e-03, -2.1516e-02,  2.2088e-02, -5.8041e-02, -1.2664e-02,\n",
       "         -6.4144e-02, -1.0202e-02, -1.8551e-02,  7.7178e-02, -1.2680e-02,\n",
       "          2.9186e-04],\n",
       "        [ 6.1455e-03, -9.3414e-02,  1.2221e-02, -2.0326e-02,  4.5090e-02,\n",
       "         -6.3530e-02, -6.9775e-02,  3.6477e-02, -9.3958e-02,  4.3044e-02,\n",
       "          0.0000e+00, -1.1724e-02,  8.2962e-03,  6.0571e-02, -2.4123e-02,\n",
       "          6.3787e-02, -2.7891e-02,  0.0000e+00,  0.0000e+00, -1.0191e-02,\n",
       "          7.4909e-02,  3.0743e-02,  1.7528e-02, -1.1546e-02,  0.0000e+00,\n",
       "         -3.0451e-02, -7.8997e-03,  6.1024e-03, -5.6371e-02, -6.5056e-02,\n",
       "          6.6997e-02, -4.5822e-02,  3.5547e-03, -2.1263e-02,  1.6549e-02,\n",
       "         -1.0636e-02, -1.0105e-03,  4.0737e-02, -6.6184e-02,  2.0703e-03,\n",
       "         -1.0078e-04, -2.5422e-02,  1.5302e-02, -4.5955e-02, -4.3085e-02,\n",
       "         -3.2393e-02,  0.0000e+00, -2.8166e-02,  1.5568e-03,  2.0819e-02,\n",
       "          2.3676e-02, -2.2859e-02,  1.4669e-02,  2.8161e-02,  5.3005e-02,\n",
       "          2.6111e-03,  1.8850e-02, -3.2413e-02, -6.2643e-02,  1.5729e-02,\n",
       "          0.0000e+00, -6.0167e-02,  0.0000e+00, -3.6932e-02,  0.0000e+00,\n",
       "         -2.4347e-02,  2.3940e-02,  4.8057e-02,  3.8063e-02,  6.7528e-02,\n",
       "          0.0000e+00,  1.2330e-02, -3.8357e-02, -4.6906e-02, -3.6309e-02,\n",
       "          6.9145e-02,  6.3581e-02,  6.7983e-02, -4.4453e-02, -2.0756e-02,\n",
       "         -1.4444e-02,  3.2204e-02,  1.3272e-02, -2.0116e-02,  2.9395e-02,\n",
       "          1.8913e-02, -1.4405e-02, -1.8209e-02, -2.7499e-02, -2.7207e-02,\n",
       "          0.0000e+00, -2.7047e-02, -4.2639e-02,  3.2684e-02, -1.3974e-02,\n",
       "          0.0000e+00, -6.0795e-02,  4.1504e-02,  2.8342e-02, -3.8076e-02,\n",
       "         -6.5621e-03,  2.2105e-02, -3.5223e-02, -8.1079e-02, -1.0851e-02,\n",
       "          5.3345e-03, -1.7385e-02, -3.4436e-02,  1.7815e-03, -6.2836e-02,\n",
       "         -4.6304e-02, -6.8813e-02, -6.6001e-02,  2.0910e-02, -5.9113e-02,\n",
       "         -3.0928e-02,  7.3562e-03, -1.1442e-02, -7.1365e-03,  0.0000e+00,\n",
       "         -5.1980e-02, -1.8239e-02,  0.0000e+00,  2.3019e-02, -1.2302e-02,\n",
       "          4.2157e-02,  6.9103e-03,  9.5887e-03, -3.9701e-02,  1.5677e-02,\n",
       "          1.4736e-02,  2.2798e-02,  1.7039e-02, -2.0110e-03,  3.2350e-03,\n",
       "          3.5747e-02, -2.1755e-02,  7.5007e-03,  3.6959e-02, -4.3747e-02,\n",
       "         -1.5881e-02,  0.0000e+00, -2.1152e-02, -1.9686e-02, -2.5908e-02,\n",
       "         -2.6816e-02, -5.3684e-02, -1.5583e-02, -3.1960e-02,  2.1907e-02,\n",
       "         -1.4520e-02, -2.0407e-02,  3.9322e-03,  3.5855e-02, -1.1155e-02,\n",
       "         -2.8897e-02,  1.6537e-03, -7.4326e-03,  2.7335e-02, -2.0973e-02,\n",
       "         -2.5152e-02,  2.1292e-02, -1.8112e-02,  3.0187e-02,  2.9818e-02,\n",
       "         -1.0655e-02,  5.6174e-03,  2.1523e-03, -2.4432e-02, -7.6365e-05,\n",
       "         -1.2664e-02, -2.1349e-03, -4.6830e-04, -1.0196e-02, -9.7831e-03,\n",
       "         -1.6236e-02,  2.8258e-03,  3.0447e-02,  1.9476e-02,  3.9170e-03,\n",
       "          2.0757e-03, -8.8798e-03,  1.5716e-02, -1.2843e-02, -9.1155e-03,\n",
       "         -2.3861e-02, -4.5488e-02,  5.2785e-04,  0.0000e+00, -4.0876e-02,\n",
       "          5.2447e-03,  4.4071e-02, -6.2398e-03, -6.9209e-03, -1.8784e-04,\n",
       "         -2.6858e-02,  7.4771e-03,  1.2119e-02,  2.1083e-02,  1.7123e-02,\n",
       "          1.7397e-02,  0.0000e+00, -6.7075e-03,  6.6059e-02,  3.2948e-02,\n",
       "          0.0000e+00, -1.5095e-02, -3.0261e-02,  2.4504e-02,  9.8490e-04,\n",
       "          1.8503e-02,  1.3769e-02, -2.0471e-02,  0.0000e+00, -8.8717e-03,\n",
       "          4.6390e-03,  0.0000e+00,  1.4827e-02,  1.3573e-02, -2.9861e-02,\n",
       "          4.9166e-02,  6.9865e-03,  2.9728e-02, -1.0319e-02, -2.1498e-02,\n",
       "         -1.9350e-02, -2.7636e-02,  2.5999e-02,  0.0000e+00,  4.2229e-02,\n",
       "          0.0000e+00, -3.0271e-02,  3.5048e-03,  4.0062e-02, -1.7760e-02,\n",
       "         -2.7376e-02,  2.6267e-02, -1.0666e-02, -4.4025e-02,  7.6512e-03,\n",
       "          0.0000e+00, -9.5371e-03,  2.5680e-02,  5.1712e-02, -1.9812e-02,\n",
       "          1.1337e-02, -2.6157e-03,  2.0390e-02, -4.6985e-02, -1.5150e-02,\n",
       "         -1.6560e-02, -1.8336e-02, -3.0370e-02,  6.3988e-02, -1.8219e-03,\n",
       "          2.0932e-02]], device='cuda:0', grad_fn=<NativeDropoutBackward0>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_vector_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.9420e-02,  3.5314e-05,  1.4487e-02,  7.0870e-02, -4.1140e-02,\n",
       "         3.0710e-02,  3.6967e-02, -2.2717e-02,  1.1818e-02,  1.8982e-02,\n",
       "         3.9897e-02, -4.5286e-02,  6.3634e-02, -4.4493e-02,  2.0341e-02,\n",
       "        -3.2309e-02,  4.4833e-02, -2.4988e-02, -4.0352e-02, -1.8328e-02,\n",
       "         3.2064e-02, -4.3617e-02,  3.0652e-02,  6.0117e-02,  5.3680e-02,\n",
       "         1.9183e-02, -5.4956e-02, -3.1291e-02,  2.8828e-02,  3.7649e-02,\n",
       "         6.7418e-02, -7.3999e-02,  6.6951e-02, -5.9084e-03, -5.0934e-03,\n",
       "         3.3017e-02,  1.4158e-02,  5.1025e-02, -4.4179e-02, -1.8671e-02,\n",
       "        -2.0749e-02, -2.7205e-02, -6.7455e-03,  2.8118e-02, -7.0825e-02,\n",
       "        -4.5575e-02, -2.9788e-02,  6.6793e-02,  1.0062e-02,  4.6156e-02,\n",
       "         8.0024e-03, -2.9930e-02, -4.2261e-02,  3.9721e-02, -4.9415e-02,\n",
       "         6.7252e-02,  3.9843e-02,  6.6396e-02,  3.3630e-02,  5.6174e-02,\n",
       "         1.4103e-02,  2.3366e-02,  5.0062e-02, -1.2563e-02,  3.3775e-02,\n",
       "         2.9293e-02, -1.4840e-02,  1.2196e-02, -2.3618e-02,  6.4787e-02,\n",
       "        -2.7129e-02, -5.2891e-04,  3.2697e-02,  8.9888e-03, -4.8411e-02,\n",
       "        -4.0301e-02,  8.3836e-03,  4.4895e-02, -3.8005e-02,  2.6541e-02,\n",
       "         1.3767e-02, -1.5012e-02, -2.4832e-02, -2.5116e-02, -1.4261e-02,\n",
       "        -4.5977e-02,  6.2206e-02,  4.4714e-04, -2.5608e-03,  2.3174e-02,\n",
       "        -4.2507e-02, -2.4157e-02,  4.3535e-02,  6.3549e-02, -3.3987e-02,\n",
       "        -5.2770e-03, -2.2745e-02,  6.0769e-02,  7.0884e-02, -2.3166e-02,\n",
       "        -4.6908e-02, -1.4552e-02,  4.2210e-02, -5.7294e-02, -1.6767e-04,\n",
       "        -6.7387e-02, -4.6989e-02, -2.7840e-02,  6.0817e-02, -3.3212e-02,\n",
       "        -1.7197e-02, -2.3410e-02, -5.4915e-02, -8.4972e-03,  1.5650e-02,\n",
       "        -5.9842e-02, -2.6663e-02, -2.0861e-02, -1.9262e-02, -1.3613e-02,\n",
       "        -4.0188e-02, -1.6663e-02,  1.1388e-02,  6.5778e-02,  1.8219e-02,\n",
       "        -3.8775e-02, -1.9091e-02,  5.0545e-02, -3.9126e-02, -2.9643e-02,\n",
       "         2.7305e-02, -6.5941e-02, -6.5563e-02,  6.1860e-02, -4.4986e-02,\n",
       "         7.0581e-02, -5.8372e-02, -1.8907e-02, -2.4302e-02, -4.2508e-02,\n",
       "        -8.6722e-03,  5.0865e-02,  2.9782e-02, -3.3102e-02,  1.3525e-02,\n",
       "         1.2897e-02, -3.8166e-02,  3.2719e-02, -3.9279e-02, -3.2556e-03,\n",
       "         2.0499e-03,  1.1576e-02,  1.8675e-03,  1.1987e-02, -4.9641e-02,\n",
       "         3.3361e-02, -6.3069e-02,  9.4009e-03, -6.8866e-02, -5.8198e-02,\n",
       "        -3.9776e-02,  4.2943e-02,  4.1197e-02, -4.7845e-03,  2.9312e-02,\n",
       "         1.1835e-02,  3.3823e-02,  5.8222e-02,  3.0743e-02,  2.5005e-02,\n",
       "         1.2953e-02,  3.4367e-02,  4.2462e-02,  1.0966e-02,  2.9715e-02,\n",
       "        -2.0706e-03,  5.0906e-02,  3.6810e-02, -3.9674e-04, -2.4790e-02,\n",
       "        -1.3964e-02,  1.9617e-02,  1.7572e-02, -9.8316e-03, -6.9514e-03,\n",
       "        -4.1852e-02,  6.6840e-02,  5.2402e-02, -5.3567e-02,  3.2243e-02,\n",
       "         5.4493e-03,  1.3402e-02,  1.3191e-02, -3.7446e-02,  2.8938e-02,\n",
       "         4.8536e-02, -1.0640e-02,  3.8807e-02, -1.9584e-02, -5.8649e-02,\n",
       "         7.6388e-02, -1.0539e-02,  3.5562e-03,  2.7966e-02,  6.1836e-02,\n",
       "         3.8743e-02,  9.9637e-03, -1.1692e-02,  4.2444e-02,  2.2876e-02,\n",
       "         9.5426e-03,  3.7184e-02,  1.8441e-02, -1.3620e-02,  7.1398e-02,\n",
       "         6.2715e-02, -5.5596e-02,  3.7292e-03,  5.5436e-02,  7.1511e-02,\n",
       "         5.1936e-02,  1.8118e-02,  1.0216e-02, -2.6188e-03, -5.5977e-02,\n",
       "         6.7822e-02, -1.5007e-02,  4.9772e-02, -3.5095e-02,  2.8231e-02,\n",
       "         3.1679e-02, -3.3304e-02,  6.3318e-03,  4.5097e-02, -3.8406e-02,\n",
       "         5.3393e-02,  3.9265e-02,  3.6675e-02, -1.6824e-02,  3.3276e-02,\n",
       "         4.3715e-02,  6.5979e-02, -2.3915e-02,  4.3992e-02,  5.6416e-02,\n",
       "         5.1642e-02,  5.3903e-02,  8.2130e-02, -3.7511e-02,  5.9505e-02,\n",
       "        -2.1018e-02,  4.9635e-02,  8.0442e-03, -5.7635e-02, -6.3684e-02,\n",
       "         5.3274e-02, -2.9713e-02, -3.1739e-02,  3.7711e-02,  8.5730e-02,\n",
       "         1.8243e-02,  7.3133e-02, -4.0105e-02, -7.6750e-02,  5.0638e-02,\n",
       "        -3.0042e-02, -3.8000e-03,  6.4800e-02, -6.9675e-03,  1.7566e-02,\n",
       "        -2.5334e-03, -2.5446e-02, -1.9371e-02, -1.6549e-02,  1.8386e-02,\n",
       "         1.7733e-02,  6.4006e-03,  3.4874e-02, -9.1851e-03,  2.2241e-02,\n",
       "        -1.9204e-02,  3.5915e-02, -1.7376e-02,  5.2687e-02,  7.6317e-04,\n",
       "        -4.2675e-03, -3.7691e-02, -3.3758e-02, -1.8851e-02,  4.4235e-02,\n",
       "        -7.1536e-02, -6.7767e-02,  1.8699e-02,  2.3804e-02, -5.0579e-02,\n",
       "        -3.2303e-02, -2.2879e-02,  4.1943e-02,  1.8353e-02, -1.7024e-02,\n",
       "         3.0840e-02,  1.3552e-02,  3.0941e-02,  4.9165e-02, -3.5758e-02,\n",
       "         9.2912e-03,  6.2952e-02, -1.4416e-02,  2.2725e-02, -5.3758e-03,\n",
       "         4.4829e-03, -5.3239e-02,  2.1057e-02, -4.4257e-02,  3.7696e-03,\n",
       "         1.9451e-02, -5.2371e-03, -4.9853e-02, -1.4258e-02, -7.8400e-03,\n",
       "        -2.9296e-02,  1.1464e-02, -4.8174e-02, -1.4471e-02, -2.3148e-02,\n",
       "        -3.9344e-02,  2.8764e-02,  8.0332e-03,  5.0918e-02,  5.3609e-02,\n",
       "         2.6521e-03, -1.3466e-02,  2.3425e-02,  3.2447e-02, -5.5876e-02,\n",
       "        -2.6707e-02, -2.3369e-02,  2.4858e-02,  1.9747e-02, -5.1010e-02,\n",
       "        -4.0513e-02,  6.0448e-02,  8.6818e-03, -3.3036e-02,  2.1486e-03,\n",
       "         3.0106e-02, -4.5316e-02,  5.1105e-02,  7.9827e-03, -3.2822e-02,\n",
       "        -4.6522e-03, -3.0191e-02, -1.1453e-02, -3.2171e-02,  4.8920e-02,\n",
       "        -1.8742e-02,  8.3419e-03,  4.6581e-02,  6.1256e-02,  6.1757e-02,\n",
       "         1.3367e-03,  5.0897e-03, -6.6356e-02,  2.4477e-02, -7.4274e-02,\n",
       "        -1.4510e-02,  1.7737e-02,  7.4737e-02,  2.6644e-03, -1.8898e-03,\n",
       "         3.8127e-02,  2.3571e-02,  4.6801e-02,  6.2769e-02, -9.7048e-03,\n",
       "        -3.4059e-02, -3.1597e-02, -3.7480e-02, -1.6761e-02,  5.6209e-02,\n",
       "        -7.9137e-03,  4.3422e-02,  2.0455e-03, -5.8437e-03,  5.6977e-03,\n",
       "        -4.0377e-02, -6.3653e-02, -7.8973e-03, -5.0322e-02, -7.6035e-03,\n",
       "        -6.3417e-02, -1.6482e-02, -8.3320e-04,  6.1445e-02,  2.2773e-02,\n",
       "         4.4806e-02,  2.1656e-02, -3.0570e-02, -2.4002e-02, -1.9738e-02,\n",
       "         1.3071e-02,  3.9660e-02, -5.4071e-02,  3.8784e-02,  6.6422e-02,\n",
       "        -4.3372e-02,  2.3122e-02, -3.1780e-02,  1.9964e-02,  3.1933e-02,\n",
       "        -3.7785e-02, -3.2543e-02, -2.0470e-02,  4.2577e-02, -6.4744e-02,\n",
       "         1.8662e-02,  3.0715e-02, -2.2637e-03, -3.4219e-02,  4.7788e-02,\n",
       "         7.5727e-03,  4.8416e-02,  1.0158e-02,  4.7178e-02,  1.1437e-02,\n",
       "        -3.8663e-02, -4.9608e-03, -1.8484e-02,  5.7051e-02,  4.5298e-02,\n",
       "         1.2811e-02,  3.4674e-03, -2.2350e-02, -4.1734e-02, -2.3630e-02,\n",
       "        -3.1838e-02, -2.5594e-02,  1.2429e-03,  1.6890e-02, -6.7645e-02,\n",
       "        -5.6143e-02,  1.6128e-02, -4.9239e-03, -1.1669e-02,  5.1754e-02,\n",
       "         2.3003e-02, -1.2427e-02,  7.3873e-02, -3.2399e-02, -1.6173e-02,\n",
       "         1.4358e-02, -3.7669e-02, -2.2812e-02, -7.4673e-02,  3.4488e-02,\n",
       "         8.5925e-02,  5.7344e-02, -1.4133e-02, -6.9020e-02,  9.9097e-03,\n",
       "        -1.1837e-02, -1.0736e-02,  3.3631e-02,  9.2801e-03,  1.3503e-02,\n",
       "         4.7849e-02, -5.9963e-02, -3.4333e-02,  3.7549e-02,  8.8797e-02,\n",
       "         8.5052e-03, -2.4339e-02, -5.4703e-02, -3.6953e-02, -3.0256e-03,\n",
       "        -5.1415e-03,  8.3988e-03,  1.2748e-02, -5.5780e-02, -6.3745e-03,\n",
       "        -2.8566e-02,  1.9255e-02,  4.6577e-02,  5.1944e-03,  5.8358e-02,\n",
       "         3.9341e-02,  3.4764e-02,  4.2927e-02,  1.1664e-02, -6.7263e-02,\n",
       "         7.9952e-03,  4.5332e-02,  2.4193e-02, -7.4809e-02, -5.1106e-02,\n",
       "         5.0329e-02,  2.4025e-02, -3.0804e-04,  3.4120e-02,  5.8666e-02,\n",
       "        -2.2923e-02,  1.3141e-02, -2.0699e-02, -3.8433e-02,  4.1472e-02,\n",
       "         2.2369e-02, -8.3314e-03, -4.9085e-02,  8.0353e-03,  5.6660e-02,\n",
       "         5.6011e-02, -8.8976e-02], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(projection_X.size())\n",
    "projection_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, batch_size, device, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.register_buffer(\"temperature\", torch.tensor(temperature))\n",
    "        self.register_buffer(\"negatives_mask\", (~torch.eye(batch_size * 2, batch_size * 2, dtype=bool)).float())\n",
    "            \n",
    "    def forward(self, emb_i, emb_j):\n",
    "        \"\"\"\n",
    "        emb_i and emb_j are batches of embeddings, where corresponding indices are pairs\n",
    "        z_i, z_j as per SimCLR paper\n",
    "        \"\"\"\n",
    "        z_i = F.normalize(emb_i, dim=1)\n",
    "        z_j = F.normalize(emb_j, dim=1)\n",
    "\n",
    "        representations = torch.cat([z_i, z_j], dim=0)\n",
    "        \n",
    "        similarity_matrix = F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=2)\n",
    "        \n",
    "        sim_ij = torch.diag(similarity_matrix, self.batch_size)\n",
    "        sim_ji = torch.diag(similarity_matrix, -self.batch_size)\n",
    "        positives = torch.cat([sim_ij, sim_ji], dim=0)\n",
    "        \n",
    "        nominator = torch.exp(positives / self.temperature)\n",
    "        denominator = self.negatives_mask.to(self.device) * torch.exp(similarity_matrix / self.temperature)\n",
    "    \n",
    "        loss_partial = -torch.log(nominator / torch.sum(denominator, dim=1))\n",
    "        loss = torch.sum(loss_partial) / (2 * self.batch_size)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLoss = ContrastiveLoss(batch_size=BATCH_SIZE, device=device, temperature=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([[0.1, 0.01, 0.02, 0.30, 0.01, 0.01], \n",
    "                  [-0.01, -0.9, 0.72, 0.80, 0.91, -0.1],\n",
    "                  [-0.5, 0.3, -0.6, 0.02, -0.2, -0.1]]).to(device)\n",
    "t2 = torch.tensor([[0.1, 0.0, 0.02, 0.30, 0.01, 0.01], \n",
    "                  [-0.01, -0.9, 0.0, 0.80, 0.91, -0.1],\n",
    "                  [-0.5, 0.3, -0.0, 0.02, -0.2, -0.1]]).to(device)\n",
    "t3 = torch.tensor([[-0.9, -0.40, 0.2, 0.02, -0.9, 0.9], \n",
    "                  [-0.9, -0.40, 0.2, 0.02, -0.9, 0.9],\n",
    "                  [-0.9, -0.40, 0.2, 0.02, -0.9, 0.9]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6678e-05, device='cuda:0')\n",
      "tensor(0.0002, device='cuda:0')\n",
      "tensor(15.0038, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(CLoss(t1, t1))\n",
    "print(CLoss(t1, t2))\n",
    "print(CLoss(t1, t3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 256])\n",
      "torch.Size([6, 6])\n"
     ]
    }
   ],
   "source": [
    "z_i = F.normalize(output_vector_X, dim=1)\n",
    "z_j = F.normalize(output_vector_X, dim=1)\n",
    "\n",
    "representations = torch.cat([z_i, z_j], dim=0)\n",
    "print(representations.size())\n",
    "\n",
    "similarity_matrix = F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=2)\n",
    "print(similarity_matrix.size())\n",
    "\n",
    "sim_ij = torch.diag(similarity_matrix, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.8337, 0.7208, 1.0000, 0.8337, 0.7208],\n",
       "        [0.8337, 1.0000, 0.8530, 0.8337, 1.0000, 0.8530],\n",
       "        [0.7208, 0.8530, 1.0000, 0.7208, 0.8530, 1.0000],\n",
       "        [1.0000, 0.8337, 0.7208, 1.0000, 0.8337, 0.7208],\n",
       "        [0.8337, 1.0000, 0.8530, 0.8337, 1.0000, 0.8530],\n",
       "        [0.7208, 0.8530, 1.0000, 0.7208, 0.8530, 1.0000]], device='cuda:0',\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5873, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLoss(projection_X, projection_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4181, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLoss(output_vector_X, output_vector_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3109, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLoss(output_vector_X, output_vector_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_PATH = '/home/svetlanamaslenkova/Documents/AKI_deep/LSTM/dataframes/'\n",
    "\n",
    "with open(DF_PATH + 'pid_train_df_pretraining.pkl', 'rb') as f:\n",
    "    pid_train_df = pickle.load(f)\n",
    "\n",
    "with open(DF_PATH + 'pid_val_df_pretraining.pkl', 'rb') as f:\n",
    "    pid_val_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>demographics_in_visit</th>\n",
       "      <th>lab_tests_in_visit</th>\n",
       "      <th>medications_in_visit</th>\n",
       "      <th>vitals_in_visit</th>\n",
       "      <th>days_in_visit</th>\n",
       "      <th>previous_diags_icd</th>\n",
       "      <th>previous_diags_titles</th>\n",
       "      <th>days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16925328.0</td>\n",
       "      <td>20000024.0</td>\n",
       "      <td>[white f 92 , white f 92 ]</td>\n",
       "      <td>[hematology blood hematocrit  32.1  %; hematol...</td>\n",
       "      <td>[oxycodone (immediate release)  2.5  mg ; gaba...</td>\n",
       "      <td>[temp  98.2  heartrate  53.0  resprate  18.0  ...</td>\n",
       "      <td>[white f 92 $temp  98.2  heartrate  53.0  resp...</td>\n",
       "      <td>[ds32059a dy92000 dm1990 ds3210xa di10 dw010xx...</td>\n",
       "      <td>[unspecified fracture of fifth lumbar vertebra...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19430048.0</td>\n",
       "      <td>20000034.0</td>\n",
       "      <td>[black african american m 74 , black african a...</td>\n",
       "      <td>[hematology blood hematocrit  30.3  %; hematol...</td>\n",
       "      <td>[influenza vaccine quadrivalent  0.5  ml ; sod...</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[black african american m 74 $nan$hematology b...</td>\n",
       "      <td>[dz8719 dn184 de83119 dk861 dz8619 dj439 dr338...</td>\n",
       "      <td>[personal history of other diseases of the dig...</td>\n",
       "      <td>[0.0, 1.0, 2.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18910522.0</td>\n",
       "      <td>20000041.0</td>\n",
       "      <td>[white f 63 , white f 63 , white f 63 , white ...</td>\n",
       "      <td>[hematology blood hematocrit  28.2  %; hematol...</td>\n",
       "      <td>[aspirin  81  mg ; bisacodyl  10  mg ; ezetimi...</td>\n",
       "      <td>[nan, nan, nan, nan]</td>\n",
       "      <td>[white f 63 $nan$hematology blood hematocrit  ...</td>\n",
       "      <td>[dv1271 d27801 d71536 dv8801 d2859 dv1301 dv12...</td>\n",
       "      <td>[personal history of peptic ulcer disease morb...</td>\n",
       "      <td>[0.0, 1.0, 2.0, 3.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11146739.0</td>\n",
       "      <td>20000057.0</td>\n",
       "      <td>[white f 93 , white f 93 , white f 93 , white ...</td>\n",
       "      <td>[nan, hematology blood hematocrit  38.4  %; he...</td>\n",
       "      <td>[timolol maleate 0.5%  1  drop ; levothyroxine...</td>\n",
       "      <td>[temp  99.3  heartrate  81.0  resprate  16.0  ...</td>\n",
       "      <td>[white f 93 $temp  99.3  heartrate  81.0  resp...</td>\n",
       "      <td>[d9597 de8490 de8889 de8889, d9597 de8490 de88...</td>\n",
       "      <td>[knee, leg, ankle, and foot injury home accide...</td>\n",
       "      <td>[-1.0, 0.0, 1.0, 2.0, 3.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14546051.0</td>\n",
       "      <td>20000069.0</td>\n",
       "      <td>[hispanic latino f 32 , hispanic latino f 32 ,...</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[sodium chloride 0.9%  flush  3  ml ; lactated...</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[hispanic latino f 32 $nan$nan$sodium chloride...</td>\n",
       "      <td>[ ,  ,  ]</td>\n",
       "      <td>[ ,  ,  ]</td>\n",
       "      <td>[0.0, 1.0, 2.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14046553.0</td>\n",
       "      <td>20000094.0</td>\n",
       "      <td>[white m 84 , white m 84 ]</td>\n",
       "      <td>[hematology blood hematocrit  42.1  %; hematol...</td>\n",
       "      <td>[influenza vaccine quadrivalent  0.5  ml ; hep...</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[white m 84 $nan$hematology blood hematocrit  ...</td>\n",
       "      <td>[d41401 d25000 d4019 , d41401 d25000 d4019 ]</td>\n",
       "      <td>[coronary atherosclerosis of native coronary a...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13074106.0</td>\n",
       "      <td>20000102.0</td>\n",
       "      <td>[black african american f 18 , black african a...</td>\n",
       "      <td>[nan, hematology blood hematocrit  27.9  %, na...</td>\n",
       "      <td>[0.9% sodium chloride (mini bag plus)  100  ml...</td>\n",
       "      <td>[nan, nan, nan, nan]</td>\n",
       "      <td>[black african american f 18 $nan$nan$0.9% sod...</td>\n",
       "      <td>[ ,  ,  ,  ]</td>\n",
       "      <td>[ ,  ,  ,  ]</td>\n",
       "      <td>[0.0, 1.0, 2.0, 3.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14990224.0</td>\n",
       "      <td>20000147.0</td>\n",
       "      <td>[white m 71 , white m 71 , white m 71 , white ...</td>\n",
       "      <td>[nan, hematology blood hematocrit  38.3  %; he...</td>\n",
       "      <td>[sodium chloride 0.9%  flush  10  ml ; , sodiu...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>[white m 71 $nan$nan$sodium chloride 0.9%  flu...</td>\n",
       "      <td>[ ,  ,  ,  ,  ,  ]</td>\n",
       "      <td>[ ,  ,  ,  ,  ,  ]</td>\n",
       "      <td>[-1.0, 0.0, 1.0, 2.0, 3.0, 4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12868349.0</td>\n",
       "      <td>20000164.0</td>\n",
       "      <td>[unknown f 69 , unknown f 69 , unknown f 69 , ...</td>\n",
       "      <td>[nan, hematology blood hematocrit  36.5  %; he...</td>\n",
       "      <td>[ranitidine  300  mg ; trazodone  50  mg ; inf...</td>\n",
       "      <td>[nan, nan, nan, nan, nan]</td>\n",
       "      <td>[unknown f 69 $nan$nan$ranitidine  300  mg ; t...</td>\n",
       "      <td>[ ,  ,  ,  ,  ]</td>\n",
       "      <td>[ ,  ,  ,  ,  ]</td>\n",
       "      <td>[0.0, 1.0, 2.0, 3.0, 4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>12640657.0</td>\n",
       "      <td>20000235.0</td>\n",
       "      <td>[white m 47 , white m 47 , white m 47 , white ...</td>\n",
       "      <td>[hematology urine wbc  &gt;50  # hpf; hematology ...</td>\n",
       "      <td>[influenza virus vaccine  0.5  ml ; levothyrox...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[white m 47 $nan$hematology urine wbc  &gt;50  # ...</td>\n",
       "      <td>[ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...</td>\n",
       "      <td>[ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...</td>\n",
       "      <td>[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>13758099.0</td>\n",
       "      <td>20000239.0</td>\n",
       "      <td>[black african american m 75 , black african a...</td>\n",
       "      <td>[hematology blood hematocrit  29.9  %; hematol...</td>\n",
       "      <td>[pneumococcal 23-valent polysaccharide vaccine...</td>\n",
       "      <td>[temp  98.1  heartrate  87.0  resprate  21.0  ...</td>\n",
       "      <td>[black african american m 75 $temp  98.1  hear...</td>\n",
       "      <td>[dy929 dn186 di2510 df329 di69354 dt82838a dz7...</td>\n",
       "      <td>[unspecified place or not applicable end stage...</td>\n",
       "      <td>[0.0, 1.0, 2.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>11210583.0</td>\n",
       "      <td>20000254.0</td>\n",
       "      <td>[white f 57 , white f 57 ]</td>\n",
       "      <td>[hematology blood hematocrit  35.6  %; hematol...</td>\n",
       "      <td>[influenza vaccine quadrivalent  0.5  ml ; sod...</td>\n",
       "      <td>[temp  98.5  heartrate  52.0  resprate  16.0  ...</td>\n",
       "      <td>[white f 57 $temp  98.5  heartrate  52.0  resp...</td>\n",
       "      <td>[ ,  ]</td>\n",
       "      <td>[ ,  ]</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>17033324.0</td>\n",
       "      <td>20000293.0</td>\n",
       "      <td>[white m 71 , white m 71 , white m 71 , white ...</td>\n",
       "      <td>[nan, hematology blood hematocrit  37.8  %; he...</td>\n",
       "      <td>[metformin xr (glucophage xr)  500  mg ; pheny...</td>\n",
       "      <td>[nan, nan, nan, nan]</td>\n",
       "      <td>[white m 71 $nan$nan$metformin xr (glucophage ...</td>\n",
       "      <td>[d496 d25060 d70715 d6827 dv5861 d2749 d25070 ...</td>\n",
       "      <td>[chronic airway obstruction, not elsewhere cla...</td>\n",
       "      <td>[-1.0, 0.0, 1.0, 2.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>16179342.0</td>\n",
       "      <td>20000298.0</td>\n",
       "      <td>[black african american f 79 , black african a...</td>\n",
       "      <td>[nan, hematology blood hematocrit  27.7  %; he...</td>\n",
       "      <td>[gabapentin  300  mg ; levothyroxine sodium  7...</td>\n",
       "      <td>[nan, nan, nan, nan]</td>\n",
       "      <td>[black african american f 79 $nan$nan$gabapent...</td>\n",
       "      <td>[ ,  ,  ,  ]</td>\n",
       "      <td>[ ,  ,  ,  ]</td>\n",
       "      <td>[-1.0, 0.0, 1.0, 2.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>13267456.0</td>\n",
       "      <td>20000343.0</td>\n",
       "      <td>[black african american m 59 , black african a...</td>\n",
       "      <td>[hematology blood hematocrit  35.3  %; hematol...</td>\n",
       "      <td>[influenza vaccine quadrivalent  0.5  ml ; sod...</td>\n",
       "      <td>[temp  99.0  heartrate  92.0  resprate  18.0  ...</td>\n",
       "      <td>[black african american m 59 $temp  99.0  hear...</td>\n",
       "      <td>[ ,  ,  ]</td>\n",
       "      <td>[ ,  ,  ]</td>\n",
       "      <td>[0.0, 1.0, 2.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>13559141.0</td>\n",
       "      <td>20000347.0</td>\n",
       "      <td>[white f 39 , white f 39 , white f 39 , white ...</td>\n",
       "      <td>[nan, hematology blood hematocrit  33.0  %; he...</td>\n",
       "      <td>[azathioprine  50  mg ; multivitamins  1  tab ...</td>\n",
       "      <td>[temp  98.8  heartrate  90.0  resprate  18.0  ...</td>\n",
       "      <td>[white f 39 $temp  98.8  heartrate  90.0  resp...</td>\n",
       "      <td>[d99859 d04149 d5569 de8788 d04104 d5990 d3000...</td>\n",
       "      <td>[other postoperative infection other and unspe...</td>\n",
       "      <td>[-1.0, 0.0, 1.0, 2.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>17913090.0</td>\n",
       "      <td>20000351.0</td>\n",
       "      <td>[black african american f 50 , black african a...</td>\n",
       "      <td>[hematology urine wbc  11  # hpf; blood gas bl...</td>\n",
       "      <td>[influenza vaccine quadrivalent  0.5  ml ; hep...</td>\n",
       "      <td>[temp    heartrate  89.0  resprate  18.0  o2sa...</td>\n",
       "      <td>[black african american f 50 $temp    heartrat...</td>\n",
       "      <td>[dz794 de119 di252 df29 dz955 di2510 df29 de11...</td>\n",
       "      <td>[long term (current) use of insulin type 2 dia...</td>\n",
       "      <td>[0.0, 1.0, 2.0, 3.0, 4.0, 5.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>12043836.0</td>\n",
       "      <td>20000374.0</td>\n",
       "      <td>[black african american m 35 , black african a...</td>\n",
       "      <td>[hematology blood hematocrit  28.1  %; hematol...</td>\n",
       "      <td>[sodium chloride 0.9%  flush  3  ml ; acetamin...</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[black african american m 35 $nan$hematology b...</td>\n",
       "      <td>[d99681 d2720 d28521 d5856 dv1581 d40301, d996...</td>\n",
       "      <td>[complications of transplanted kidney pure hyp...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>10315295.0</td>\n",
       "      <td>20000394.0</td>\n",
       "      <td>[white m 83 , white m 83 , white m 83 ]</td>\n",
       "      <td>[nan, hematology blood hematocrit  38.7  %; he...</td>\n",
       "      <td>[aspirin ec  325  mg ; clopidogrel  75  mg ; a...</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[white m 83 $nan$nan$aspirin ec  325  mg ; clo...</td>\n",
       "      <td>[dv4364 d412 d53081 d7242 dv4501 d73390 dv1083...</td>\n",
       "      <td>[hip joint replacement old myocardial infarcti...</td>\n",
       "      <td>[-1.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>16058391.0</td>\n",
       "      <td>20000397.0</td>\n",
       "      <td>[other m 61 , other m 61 , other m 61 , other ...</td>\n",
       "      <td>[hematology blood hematocrit  47.0  %; hematol...</td>\n",
       "      <td>[sodium chloride 0.9%  1000  ml ; influenza va...</td>\n",
       "      <td>[nan, nan, nan, nan, nan]</td>\n",
       "      <td>[other m 61 $nan$hematology blood hematocrit  ...</td>\n",
       "      <td>[ ,  ,  ,  ,  ]</td>\n",
       "      <td>[ ,  ,  ,  ,  ]</td>\n",
       "      <td>[0.0, 1.0, 2.0, 3.0, 4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>15325429.0</td>\n",
       "      <td>20000400.0</td>\n",
       "      <td>[white f 53 ]</td>\n",
       "      <td>[hematology blood hematocrit  30.1  %; hematol...</td>\n",
       "      <td>[sodium chloride 0.9%  flush  3  ml ; lactated...</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[white f 53 $nan$hematology blood hematocrit  ...</td>\n",
       "      <td>[d30000 d6270 d311 d2851]</td>\n",
       "      <td>[anxiety state, unspecified premenopausal meno...</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>13193917.0</td>\n",
       "      <td>20000452.0</td>\n",
       "      <td>[white m 42 , white m 42 , white m 42 ]</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[sodium chloride 0.9%  flush  3  ml ; d5 1 2ns...</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[white m 42 $nan$nan$sodium chloride 0.9%  flu...</td>\n",
       "      <td>[ ,  ,  ]</td>\n",
       "      <td>[ ,  ,  ]</td>\n",
       "      <td>[0.0, 1.0, 2.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>16207995.0</td>\n",
       "      <td>20000471.0</td>\n",
       "      <td>[white f 56 , white f 56 , white f 56 , white ...</td>\n",
       "      <td>[nan, hematology blood hematocrit  23.2  %; he...</td>\n",
       "      <td>[furosemide  20  mg ; levothyroxine sodium  10...</td>\n",
       "      <td>[nan, nan, nan, nan, nan]</td>\n",
       "      <td>[white f 56 $nan$nan$furosemide  20  mg ; levo...</td>\n",
       "      <td>[d28800 dv1251 dv1255 d28522 d25000 dv4983 dv5...</td>\n",
       "      <td>[neutropenia, unspecified personal history of ...</td>\n",
       "      <td>[-1.0, 0.0, 1.0, 2.0, 3.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>19588958.0</td>\n",
       "      <td>20000502.0</td>\n",
       "      <td>[white m 49 , white m 49 , white m 49 , white ...</td>\n",
       "      <td>[nan, hematology urine glucose  neg  mg dl; he...</td>\n",
       "      <td>[docusate sodium  100  mg ; heparin  5000  uni...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>[white m 49 $nan$nan$docusate sodium  100  mg ...</td>\n",
       "      <td>[ ,  ,  ,  ,  ,  ]</td>\n",
       "      <td>[ ,  ,  ,  ,  ,  ]</td>\n",
       "      <td>[-1.0, 0.0, 1.0, 2.0, 3.0, 4.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>13801793.0</td>\n",
       "      <td>20000535.0</td>\n",
       "      <td>[white m 59 ]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[rivaroxaban  20  mg ; ]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[white m 59 $nan$nan$rivaroxaban  20  mg ; ]</td>\n",
       "      <td>[dz87891 dd62 dg610 dq231 dg40802 dk2270 dj459...</td>\n",
       "      <td>[personal history of nicotine dependence acute...</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>12300663.0</td>\n",
       "      <td>20000588.0</td>\n",
       "      <td>[white m 71 ]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[epinephrine (epipen)  0.3  mg ; diphenhydrami...</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[white m 71 $nan$nan$epinephrine (epipen)  0.3...</td>\n",
       "      <td>[dg8918 dg4733 dr110 dr1013 dc772 di10 dg893 d...</td>\n",
       "      <td>[other acute postprocedural pain obstructive s...</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>16479007.0</td>\n",
       "      <td>20000600.0</td>\n",
       "      <td>[white m 59 ]</td>\n",
       "      <td>[hematology blood hematocrit  36.2  %; hematol...</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[temp  97.7  heartrate  69.0  resprate  18.0  ...</td>\n",
       "      <td>[white m 59 $temp  97.7  heartrate  69.0  resp...</td>\n",
       "      <td>[de871 dz590 dr45851 dg43909 df1320 dz87820 dy...</td>\n",
       "      <td>[hypo-osmolality and hyponatremia homelessness...</td>\n",
       "      <td>[-1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>10687335.0</td>\n",
       "      <td>20000602.0</td>\n",
       "      <td>[white m 45 ]</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[sodium chloride 0.9%  flush  3  ml ; lr  1000...</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[white m 45 $nan$nan$sodium chloride 0.9%  flu...</td>\n",
       "      <td>[ ]</td>\n",
       "      <td>[ ]</td>\n",
       "      <td>[0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>12726398.0</td>\n",
       "      <td>20000629.0</td>\n",
       "      <td>[white f 60 , white f 60 , white f 60 , white ...</td>\n",
       "      <td>[nan, hematology blood hematocrit  31.6  %; he...</td>\n",
       "      <td>[furosemide  40  mg ; spironolactone  100  mg ...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[white f 60 $nan$nan$furosemide  40  mg ; spir...</td>\n",
       "      <td>[d56210 d5715 dv4577 d6961 d07070 dv1042 d5722...</td>\n",
       "      <td>[diverticulosis of colon (without mention of h...</td>\n",
       "      <td>[-1.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>10957643.0</td>\n",
       "      <td>20000691.0</td>\n",
       "      <td>[white m 72 , white m 72 ]</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[hydrochlorothiazide  12.5  mg ; omeprazole  2...</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[white m 72 $nan$nan$hydrochlorothiazide  12.5...</td>\n",
       "      <td>[ ,  ]</td>\n",
       "      <td>[ ,  ]</td>\n",
       "      <td>[-1.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject_id     hadm_id  \\\n",
       "0    16925328.0  20000024.0   \n",
       "2    19430048.0  20000034.0   \n",
       "5    18910522.0  20000041.0   \n",
       "9    11146739.0  20000057.0   \n",
       "14   14546051.0  20000069.0   \n",
       "17   14046553.0  20000094.0   \n",
       "19   13074106.0  20000102.0   \n",
       "23   14990224.0  20000147.0   \n",
       "29   12868349.0  20000164.0   \n",
       "34   12640657.0  20000235.0   \n",
       "55   13758099.0  20000239.0   \n",
       "58   11210583.0  20000254.0   \n",
       "60   17033324.0  20000293.0   \n",
       "64   16179342.0  20000298.0   \n",
       "68   13267456.0  20000343.0   \n",
       "71   13559141.0  20000347.0   \n",
       "75   17913090.0  20000351.0   \n",
       "81   12043836.0  20000374.0   \n",
       "83   10315295.0  20000394.0   \n",
       "86   16058391.0  20000397.0   \n",
       "91   15325429.0  20000400.0   \n",
       "92   13193917.0  20000452.0   \n",
       "95   16207995.0  20000471.0   \n",
       "100  19588958.0  20000502.0   \n",
       "106  13801793.0  20000535.0   \n",
       "107  12300663.0  20000588.0   \n",
       "108  16479007.0  20000600.0   \n",
       "109  10687335.0  20000602.0   \n",
       "110  12726398.0  20000629.0   \n",
       "121  10957643.0  20000691.0   \n",
       "\n",
       "                                 demographics_in_visit  \\\n",
       "0                           [white f 92 , white f 92 ]   \n",
       "2    [black african american m 74 , black african a...   \n",
       "5    [white f 63 , white f 63 , white f 63 , white ...   \n",
       "9    [white f 93 , white f 93 , white f 93 , white ...   \n",
       "14   [hispanic latino f 32 , hispanic latino f 32 ,...   \n",
       "17                          [white m 84 , white m 84 ]   \n",
       "19   [black african american f 18 , black african a...   \n",
       "23   [white m 71 , white m 71 , white m 71 , white ...   \n",
       "29   [unknown f 69 , unknown f 69 , unknown f 69 , ...   \n",
       "34   [white m 47 , white m 47 , white m 47 , white ...   \n",
       "55   [black african american m 75 , black african a...   \n",
       "58                          [white f 57 , white f 57 ]   \n",
       "60   [white m 71 , white m 71 , white m 71 , white ...   \n",
       "64   [black african american f 79 , black african a...   \n",
       "68   [black african american m 59 , black african a...   \n",
       "71   [white f 39 , white f 39 , white f 39 , white ...   \n",
       "75   [black african american f 50 , black african a...   \n",
       "81   [black african american m 35 , black african a...   \n",
       "83             [white m 83 , white m 83 , white m 83 ]   \n",
       "86   [other m 61 , other m 61 , other m 61 , other ...   \n",
       "91                                       [white f 53 ]   \n",
       "92             [white m 42 , white m 42 , white m 42 ]   \n",
       "95   [white f 56 , white f 56 , white f 56 , white ...   \n",
       "100  [white m 49 , white m 49 , white m 49 , white ...   \n",
       "106                                      [white m 59 ]   \n",
       "107                                      [white m 71 ]   \n",
       "108                                      [white m 59 ]   \n",
       "109                                      [white m 45 ]   \n",
       "110  [white f 60 , white f 60 , white f 60 , white ...   \n",
       "121                         [white m 72 , white m 72 ]   \n",
       "\n",
       "                                    lab_tests_in_visit  \\\n",
       "0    [hematology blood hematocrit  32.1  %; hematol...   \n",
       "2    [hematology blood hematocrit  30.3  %; hematol...   \n",
       "5    [hematology blood hematocrit  28.2  %; hematol...   \n",
       "9    [nan, hematology blood hematocrit  38.4  %; he...   \n",
       "14                                     [nan, nan, nan]   \n",
       "17   [hematology blood hematocrit  42.1  %; hematol...   \n",
       "19   [nan, hematology blood hematocrit  27.9  %, na...   \n",
       "23   [nan, hematology blood hematocrit  38.3  %; he...   \n",
       "29   [nan, hematology blood hematocrit  36.5  %; he...   \n",
       "34   [hematology urine wbc  >50  # hpf; hematology ...   \n",
       "55   [hematology blood hematocrit  29.9  %; hematol...   \n",
       "58   [hematology blood hematocrit  35.6  %; hematol...   \n",
       "60   [nan, hematology blood hematocrit  37.8  %; he...   \n",
       "64   [nan, hematology blood hematocrit  27.7  %; he...   \n",
       "68   [hematology blood hematocrit  35.3  %; hematol...   \n",
       "71   [nan, hematology blood hematocrit  33.0  %; he...   \n",
       "75   [hematology urine wbc  11  # hpf; blood gas bl...   \n",
       "81   [hematology blood hematocrit  28.1  %; hematol...   \n",
       "83   [nan, hematology blood hematocrit  38.7  %; he...   \n",
       "86   [hematology blood hematocrit  47.0  %; hematol...   \n",
       "91   [hematology blood hematocrit  30.1  %; hematol...   \n",
       "92                                     [nan, nan, nan]   \n",
       "95   [nan, hematology blood hematocrit  23.2  %; he...   \n",
       "100  [nan, hematology urine glucose  neg  mg dl; he...   \n",
       "106                                              [nan]   \n",
       "107                                              [nan]   \n",
       "108  [hematology blood hematocrit  36.2  %; hematol...   \n",
       "109                                              [nan]   \n",
       "110  [nan, hematology blood hematocrit  31.6  %; he...   \n",
       "121                                         [nan, nan]   \n",
       "\n",
       "                                  medications_in_visit  \\\n",
       "0    [oxycodone (immediate release)  2.5  mg ; gaba...   \n",
       "2    [influenza vaccine quadrivalent  0.5  ml ; sod...   \n",
       "5    [aspirin  81  mg ; bisacodyl  10  mg ; ezetimi...   \n",
       "9    [timolol maleate 0.5%  1  drop ; levothyroxine...   \n",
       "14   [sodium chloride 0.9%  flush  3  ml ; lactated...   \n",
       "17   [influenza vaccine quadrivalent  0.5  ml ; hep...   \n",
       "19   [0.9% sodium chloride (mini bag plus)  100  ml...   \n",
       "23   [sodium chloride 0.9%  flush  10  ml ; , sodiu...   \n",
       "29   [ranitidine  300  mg ; trazodone  50  mg ; inf...   \n",
       "34   [influenza virus vaccine  0.5  ml ; levothyrox...   \n",
       "55   [pneumococcal 23-valent polysaccharide vaccine...   \n",
       "58   [influenza vaccine quadrivalent  0.5  ml ; sod...   \n",
       "60   [metformin xr (glucophage xr)  500  mg ; pheny...   \n",
       "64   [gabapentin  300  mg ; levothyroxine sodium  7...   \n",
       "68   [influenza vaccine quadrivalent  0.5  ml ; sod...   \n",
       "71   [azathioprine  50  mg ; multivitamins  1  tab ...   \n",
       "75   [influenza vaccine quadrivalent  0.5  ml ; hep...   \n",
       "81   [sodium chloride 0.9%  flush  3  ml ; acetamin...   \n",
       "83   [aspirin ec  325  mg ; clopidogrel  75  mg ; a...   \n",
       "86   [sodium chloride 0.9%  1000  ml ; influenza va...   \n",
       "91   [sodium chloride 0.9%  flush  3  ml ; lactated...   \n",
       "92   [sodium chloride 0.9%  flush  3  ml ; d5 1 2ns...   \n",
       "95   [furosemide  20  mg ; levothyroxine sodium  10...   \n",
       "100  [docusate sodium  100  mg ; heparin  5000  uni...   \n",
       "106                           [rivaroxaban  20  mg ; ]   \n",
       "107  [epinephrine (epipen)  0.3  mg ; diphenhydrami...   \n",
       "108                                              [nan]   \n",
       "109  [sodium chloride 0.9%  flush  3  ml ; lr  1000...   \n",
       "110  [furosemide  40  mg ; spironolactone  100  mg ...   \n",
       "121  [hydrochlorothiazide  12.5  mg ; omeprazole  2...   \n",
       "\n",
       "                                       vitals_in_visit  \\\n",
       "0    [temp  98.2  heartrate  53.0  resprate  18.0  ...   \n",
       "2                                      [nan, nan, nan]   \n",
       "5                                 [nan, nan, nan, nan]   \n",
       "9    [temp  99.3  heartrate  81.0  resprate  16.0  ...   \n",
       "14                                     [nan, nan, nan]   \n",
       "17                                          [nan, nan]   \n",
       "19                                [nan, nan, nan, nan]   \n",
       "23                      [nan, nan, nan, nan, nan, nan]   \n",
       "29                           [nan, nan, nan, nan, nan]   \n",
       "34   [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "55   [temp  98.1  heartrate  87.0  resprate  21.0  ...   \n",
       "58   [temp  98.5  heartrate  52.0  resprate  16.0  ...   \n",
       "60                                [nan, nan, nan, nan]   \n",
       "64                                [nan, nan, nan, nan]   \n",
       "68   [temp  99.0  heartrate  92.0  resprate  18.0  ...   \n",
       "71   [temp  98.8  heartrate  90.0  resprate  18.0  ...   \n",
       "75   [temp    heartrate  89.0  resprate  18.0  o2sa...   \n",
       "81                                          [nan, nan]   \n",
       "83                                     [nan, nan, nan]   \n",
       "86                           [nan, nan, nan, nan, nan]   \n",
       "91                                               [nan]   \n",
       "92                                     [nan, nan, nan]   \n",
       "95                           [nan, nan, nan, nan, nan]   \n",
       "100                     [nan, nan, nan, nan, nan, nan]   \n",
       "106                                              [nan]   \n",
       "107                                              [nan]   \n",
       "108  [temp  97.7  heartrate  69.0  resprate  18.0  ...   \n",
       "109                                              [nan]   \n",
       "110  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "121                                         [nan, nan]   \n",
       "\n",
       "                                         days_in_visit  \\\n",
       "0    [white f 92 $temp  98.2  heartrate  53.0  resp...   \n",
       "2    [black african american m 74 $nan$hematology b...   \n",
       "5    [white f 63 $nan$hematology blood hematocrit  ...   \n",
       "9    [white f 93 $temp  99.3  heartrate  81.0  resp...   \n",
       "14   [hispanic latino f 32 $nan$nan$sodium chloride...   \n",
       "17   [white m 84 $nan$hematology blood hematocrit  ...   \n",
       "19   [black african american f 18 $nan$nan$0.9% sod...   \n",
       "23   [white m 71 $nan$nan$sodium chloride 0.9%  flu...   \n",
       "29   [unknown f 69 $nan$nan$ranitidine  300  mg ; t...   \n",
       "34   [white m 47 $nan$hematology urine wbc  >50  # ...   \n",
       "55   [black african american m 75 $temp  98.1  hear...   \n",
       "58   [white f 57 $temp  98.5  heartrate  52.0  resp...   \n",
       "60   [white m 71 $nan$nan$metformin xr (glucophage ...   \n",
       "64   [black african american f 79 $nan$nan$gabapent...   \n",
       "68   [black african american m 59 $temp  99.0  hear...   \n",
       "71   [white f 39 $temp  98.8  heartrate  90.0  resp...   \n",
       "75   [black african american f 50 $temp    heartrat...   \n",
       "81   [black african american m 35 $nan$hematology b...   \n",
       "83   [white m 83 $nan$nan$aspirin ec  325  mg ; clo...   \n",
       "86   [other m 61 $nan$hematology blood hematocrit  ...   \n",
       "91   [white f 53 $nan$hematology blood hematocrit  ...   \n",
       "92   [white m 42 $nan$nan$sodium chloride 0.9%  flu...   \n",
       "95   [white f 56 $nan$nan$furosemide  20  mg ; levo...   \n",
       "100  [white m 49 $nan$nan$docusate sodium  100  mg ...   \n",
       "106       [white m 59 $nan$nan$rivaroxaban  20  mg ; ]   \n",
       "107  [white m 71 $nan$nan$epinephrine (epipen)  0.3...   \n",
       "108  [white m 59 $temp  97.7  heartrate  69.0  resp...   \n",
       "109  [white m 45 $nan$nan$sodium chloride 0.9%  flu...   \n",
       "110  [white f 60 $nan$nan$furosemide  40  mg ; spir...   \n",
       "121  [white m 72 $nan$nan$hydrochlorothiazide  12.5...   \n",
       "\n",
       "                                    previous_diags_icd  \\\n",
       "0    [ds32059a dy92000 dm1990 ds3210xa di10 dw010xx...   \n",
       "2    [dz8719 dn184 de83119 dk861 dz8619 dj439 dr338...   \n",
       "5    [dv1271 d27801 d71536 dv8801 d2859 dv1301 dv12...   \n",
       "9    [d9597 de8490 de8889 de8889, d9597 de8490 de88...   \n",
       "14                                           [ ,  ,  ]   \n",
       "17        [d41401 d25000 d4019 , d41401 d25000 d4019 ]   \n",
       "19                                        [ ,  ,  ,  ]   \n",
       "23                                  [ ,  ,  ,  ,  ,  ]   \n",
       "29                                     [ ,  ,  ,  ,  ]   \n",
       "34   [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...   \n",
       "55   [dy929 dn186 di2510 df329 di69354 dt82838a dz7...   \n",
       "58                                              [ ,  ]   \n",
       "60   [d496 d25060 d70715 d6827 dv5861 d2749 d25070 ...   \n",
       "64                                        [ ,  ,  ,  ]   \n",
       "68                                           [ ,  ,  ]   \n",
       "71   [d99859 d04149 d5569 de8788 d04104 d5990 d3000...   \n",
       "75   [dz794 de119 di252 df29 dz955 di2510 df29 de11...   \n",
       "81   [d99681 d2720 d28521 d5856 dv1581 d40301, d996...   \n",
       "83   [dv4364 d412 d53081 d7242 dv4501 d73390 dv1083...   \n",
       "86                                     [ ,  ,  ,  ,  ]   \n",
       "91                           [d30000 d6270 d311 d2851]   \n",
       "92                                           [ ,  ,  ]   \n",
       "95   [d28800 dv1251 dv1255 d28522 d25000 dv4983 dv5...   \n",
       "100                                 [ ,  ,  ,  ,  ,  ]   \n",
       "106  [dz87891 dd62 dg610 dq231 dg40802 dk2270 dj459...   \n",
       "107  [dg8918 dg4733 dr110 dr1013 dc772 di10 dg893 d...   \n",
       "108  [de871 dz590 dr45851 dg43909 df1320 dz87820 dy...   \n",
       "109                                                [ ]   \n",
       "110  [d56210 d5715 dv4577 d6961 d07070 dv1042 d5722...   \n",
       "121                                             [ ,  ]   \n",
       "\n",
       "                                 previous_diags_titles  \\\n",
       "0    [unspecified fracture of fifth lumbar vertebra...   \n",
       "2    [personal history of other diseases of the dig...   \n",
       "5    [personal history of peptic ulcer disease morb...   \n",
       "9    [knee, leg, ankle, and foot injury home accide...   \n",
       "14                                           [ ,  ,  ]   \n",
       "17   [coronary atherosclerosis of native coronary a...   \n",
       "19                                        [ ,  ,  ,  ]   \n",
       "23                                  [ ,  ,  ,  ,  ,  ]   \n",
       "29                                     [ ,  ,  ,  ,  ]   \n",
       "34   [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...   \n",
       "55   [unspecified place or not applicable end stage...   \n",
       "58                                              [ ,  ]   \n",
       "60   [chronic airway obstruction, not elsewhere cla...   \n",
       "64                                        [ ,  ,  ,  ]   \n",
       "68                                           [ ,  ,  ]   \n",
       "71   [other postoperative infection other and unspe...   \n",
       "75   [long term (current) use of insulin type 2 dia...   \n",
       "81   [complications of transplanted kidney pure hyp...   \n",
       "83   [hip joint replacement old myocardial infarcti...   \n",
       "86                                     [ ,  ,  ,  ,  ]   \n",
       "91   [anxiety state, unspecified premenopausal meno...   \n",
       "92                                           [ ,  ,  ]   \n",
       "95   [neutropenia, unspecified personal history of ...   \n",
       "100                                 [ ,  ,  ,  ,  ,  ]   \n",
       "106  [personal history of nicotine dependence acute...   \n",
       "107  [other acute postprocedural pain obstructive s...   \n",
       "108  [hypo-osmolality and hyponatremia homelessness...   \n",
       "109                                                [ ]   \n",
       "110  [diverticulosis of colon (without mention of h...   \n",
       "121                                             [ ,  ]   \n",
       "\n",
       "                                                  days  \n",
       "0                                           [0.0, 1.0]  \n",
       "2                                      [0.0, 1.0, 2.0]  \n",
       "5                                 [0.0, 1.0, 2.0, 3.0]  \n",
       "9                           [-1.0, 0.0, 1.0, 2.0, 3.0]  \n",
       "14                                     [0.0, 1.0, 2.0]  \n",
       "17                                          [0.0, 1.0]  \n",
       "19                                [0.0, 1.0, 2.0, 3.0]  \n",
       "23                     [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0]  \n",
       "29                           [0.0, 1.0, 2.0, 3.0, 4.0]  \n",
       "34   [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...  \n",
       "55                                     [0.0, 1.0, 2.0]  \n",
       "58                                          [0.0, 1.0]  \n",
       "60                               [-1.0, 0.0, 1.0, 2.0]  \n",
       "64                               [-1.0, 0.0, 1.0, 2.0]  \n",
       "68                                     [0.0, 1.0, 2.0]  \n",
       "71                               [-1.0, 0.0, 1.0, 2.0]  \n",
       "75                      [0.0, 1.0, 2.0, 3.0, 4.0, 5.0]  \n",
       "81                                          [0.0, 1.0]  \n",
       "83                                    [-1.0, 0.0, 1.0]  \n",
       "86                           [0.0, 1.0, 2.0, 3.0, 4.0]  \n",
       "91                                               [0.0]  \n",
       "92                                     [0.0, 1.0, 2.0]  \n",
       "95                          [-1.0, 0.0, 1.0, 2.0, 3.0]  \n",
       "100                    [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0]  \n",
       "106                                              [0.0]  \n",
       "107                                              [0.0]  \n",
       "108                                             [-1.0]  \n",
       "109                                              [0.0]  \n",
       "110  [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0,...  \n",
       "121                                        [-1.0, 0.0]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_train_df.head(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb5fdc613a097f32ece1e000bc60f17f3e33b8b9e39d8b8753abf3e45200e5d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
