{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tokenizers import  Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Punctuation, Whitespace\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers import pre_tokenizers, normalizers\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "import glob\n",
    "from os.path import exists\n",
    "import os\n",
    "\n",
    "import pickle5 as pickle\n",
    "import wandb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(load_path, model, optimizer, device):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, batch_size, device, temperature=0.5):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.register_buffer(\"temperature\", torch.tensor(temperature))\n",
    "        self.register_buffer(\"negatives_mask\", (~torch.eye(batch_size * 2, batch_size * 2, dtype=bool)).float())\n",
    "            \n",
    "    def forward(self, emb_i, emb_j):\n",
    "        \"\"\"\n",
    "        emb_i and emb_j are batches of embeddings, where corresponding indices are pairs\n",
    "        z_i, z_j as per SimCLR paper\n",
    "        \"\"\"\n",
    "        z_i = F.normalize(emb_i, dim=1)\n",
    "        z_j = F.normalize(emb_j, dim=1)\n",
    "\n",
    "        representations = torch.cat([z_i, z_j], dim=0)\n",
    "        \n",
    "        similarity_matrix = F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=2)\n",
    "        \n",
    "        sim_ij = torch.diag(similarity_matrix, self.batch_size)\n",
    "        sim_ji = torch.diag(similarity_matrix, -self.batch_size)\n",
    "        positives = torch.cat([sim_ij, sim_ji], dim=0)\n",
    "        \n",
    "        nominator = torch.exp(positives / self.temperature)\n",
    "        denominator = self.negatives_mask.to(self.device) * torch.exp(similarity_matrix / self.temperature)\n",
    "    \n",
    "        loss_partial = -torch.log(nominator / torch.sum(denominator, dim=1))\n",
    "        loss = torch.sum(loss_partial) / (2 * self.batch_size)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_length, pred_window=2, observing_window=3):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.max_length = max_length\n",
    "        # self.max_length_diags = 35\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.make_matrices(idx)\n",
    "    \n",
    "    def tokenize(self, text, max_length): \n",
    "        \n",
    "        # max_length = max_length + 2\n",
    "        self.tokenizer.enable_truncation(max_length=max_length)\n",
    "\n",
    "        output = self.tokenizer.encode(text)\n",
    "\n",
    "        # padding and truncation\n",
    "        if len(output.ids) < max_length:\n",
    "            len_missing_token = max_length - len(output.ids)\n",
    "            padding_vec = [self.tokenizer.token_to_id('PAD') for _ in range(len_missing_token)]\n",
    "            token_output = [*output.ids, *padding_vec]\n",
    "        elif len(output.ids) > max_length:\n",
    "            token_output = output.ids[:max_length]\n",
    "        else:\n",
    "            token_output = output.ids\n",
    "        \n",
    "        return token_output\n",
    "\n",
    "    def make_matrices(self, idx):\n",
    "        \n",
    "        hadm_id = self.df.hadm_id.values[idx]\n",
    "        diagnoses_info = self.df.previous_diagnoses.values[idx]\n",
    "        demo_info = self.df.demographics_in_visit.values[idx][0]\n",
    "        lab_info = self.df.lab_tests_in_visit.values[idx]\n",
    "        med_info = self.df.medications_in_visit.values[idx]\n",
    "        vitals_info = self.df.vitals_in_visit.values[idx]\n",
    "        \n",
    "        # aki_status = self.df.aki_status_in_visit.values[idx]\n",
    "        days = self.df.days.values[idx]\n",
    "        # print(idx)\n",
    "\n",
    "        lab_info_list = []\n",
    "        med_info_list = []\n",
    "        vitals_info_list = []\n",
    "        label = None\n",
    "\n",
    "        for day in range(days[0], days[0] + self.observing_window + self.pred_window):\n",
    "            # print('day', day)\n",
    "            if day not in days:\n",
    "                vitals_info_list.append(self.tokenize('', self.max_length['vitals']))\n",
    "                lab_info_list.append(self.tokenize('', self.max_length['lab_tests']))\n",
    "                med_info_list.append(self.tokenize('', self.max_length['medications']))\n",
    "\n",
    "            else:\n",
    "                i = days.index(day)\n",
    "                \n",
    "                # vitals\n",
    "                if (str(vitals_info[i]) == 'nan') or (vitals_info[i] == np.nan):\n",
    "                    vitals_info_list.append(self.tokenize('PAD', self.max_length['vitals']))\n",
    "                else:\n",
    "                    vitals_info_list.append(self.tokenize(vitals_info[i], self.max_length['vitals']))\n",
    "\n",
    "                # lab results\n",
    "                if (str(lab_info[i]) == 'nan') or (lab_info[i] == np.nan):\n",
    "                    lab_info_list.append(self.tokenize('PAD', self.max_length['lab_tests']))\n",
    "                else:\n",
    "                    lab_info_list.append(self.tokenize(lab_info[i], self.max_length['lab_tests']))\n",
    "                \n",
    "                # medications\n",
    "                if (str(med_info[i]) == 'nan') or (med_info[i] == np.nan):\n",
    "                    med_info_list.append(self.tokenize('PAD', self.max_length['medications']))\n",
    "                else:\n",
    "                    med_info_list.append(self.tokenize(med_info[i], self.max_length['medications']))\n",
    "\n",
    "        # diagnoses\n",
    "        if (str(diagnoses_info) == 'nan') or (diagnoses_info == np.nan):\n",
    "            diagnoses_info = self.tokenize('PAD', self.max_length['diagnoses'])\n",
    "        else:\n",
    "            diagnoses_info = self.tokenize(diagnoses_info, self.max_length['diagnoses'])\n",
    "\n",
    "        # demographics\n",
    "        if (str(demo_info) == 'nan') or (demo_info == np.nan):\n",
    "            demo_info = self.tokenize('PAD', self.max_length_diags)\n",
    "        else:\n",
    "            demo_info = self.tokenize(demo_info, self.max_length['demographics'])\n",
    "\n",
    "        #make tensors\n",
    "        tensor_demo = torch.tensor(demo_info, dtype=torch.int64)\n",
    "        tensor_diags = torch.tensor(diagnoses_info, dtype=torch.int64)\n",
    "        tensor_vitals = torch.tensor(vitals_info_list, dtype=torch.int64)\n",
    "        tensor_labs = torch.tensor(lab_info_list, dtype=torch.int64)\n",
    "        tensor_meds = torch.tensor(med_info_list, dtype=torch.int64)\n",
    "        # tensor_labels = torch.tensor(label, dtype=torch.float64)\n",
    "    \n",
    "        return tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds, hadm_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHR_PRETRAINING(nn.Module):\n",
    "    def __init__(self, max_length, vocab_size, device, pred_window=2, observing_window=3,  H=128, embedding_size=200, drop=0.6):\n",
    "        super(EHR_PRETRAINING, self).__init__()\n",
    "\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.H = H\n",
    "        self.max_length_demo = max_length['demographics']\n",
    "        self.max_length_diags = max_length['diagnoses']\n",
    "        self.max_length_meds = max_length['medications']\n",
    "        self.max_length_vitals = max_length['vitals']\n",
    "        self.max_length_lab = max_length['lab_tests']\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device\n",
    "        self.drop = drop\n",
    "\n",
    "        # self.embedding = pretrained_model\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "\n",
    "        self.lstm_day = nn.LSTM(input_size=embedding_size,\n",
    "                            hidden_size=self.H,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "\n",
    "        self.fc_med = nn.Linear(self.max_length_meds * 2 * self.H, 2048)\n",
    "        self.fc_vitals = nn.Linear(self.max_length_vitals * 2 * self.H, 2048)\n",
    "        self.fc_lab = nn.Linear(self.max_length_lab * 2 * self.H, 2048)\n",
    "\n",
    "        self.fc_adm = nn.Linear(2 * self.H * (self.max_length_diags + self.max_length_demo) \\\n",
    "                                + self.observing_window * 3 * 2048 , 2048)\n",
    "\n",
    "        self.lstm_adm = nn.LSTM(input_size=2048,\n",
    "                            hidden_size=self.H,\n",
    "                            num_layers=2,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=False)\n",
    "\n",
    "        self.drop = nn.Dropout(p=drop)\n",
    "        self.inner_drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        # self.fc_2 = nn.Linear(self.H*2, 2)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=self.H, out_features=256)\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor_demo, tensor_diags, tensor_med, tensor_vitals, tensor_labs):\n",
    "\n",
    "        batch_size = tensor_demo.size()[0]\n",
    "\n",
    "        # embeddings\n",
    "        out_emb_diags = self.embedding(tensor_diags.squeeze(1)) # [16, 37, 200]\n",
    "        out_emb_demo =  self.embedding(tensor_demo.squeeze(1))  # [16, 7, 200]\n",
    "        print(f'out_emb_diags: ', out_emb_diags.size())\n",
    "        print(f'out_emb_demo: ', out_emb_demo.size())\n",
    "        # lstm for demographic and diagnoses\n",
    "        out_lstm_diags, _ = self.lstm_day(out_emb_diags)    # [16, 37, 256]\n",
    "        out_lstm_demo, _ = self.lstm_day(out_emb_demo)      # [16, 7, 256]\n",
    "        print(f'out_lstm_diags: ', out_lstm_diags.size())\n",
    "        print(f'out_lstm_demo: ', out_lstm_demo.size())\n",
    "        # reshape and concat demographics and diags\n",
    "        out_lstm_diags_reshaped = out_lstm_diags.reshape(batch_size, self.max_length_diags * 2 * self.H)\n",
    "        out_lstm_demo_reshaped = out_lstm_demo.reshape(batch_size, self.max_length_demo * 2 * self.H)\n",
    "        print(f'out_lstm_diags_reshaped', out_lstm_diags_reshaped.size())\n",
    "        print(f'out_lstm_demo_reshaped', out_lstm_demo_reshaped.size())\n",
    "        full_output = torch.cat([out_lstm_demo_reshaped, out_lstm_diags_reshaped], dim=1)   # [16, 11264]\n",
    "        print(f'full_output', full_output.size())\n",
    "\n",
    "        for d in range(self.observing_window):\n",
    "            print('--------------')  \n",
    "            # embedding layer applied to all tensors \n",
    "            out_med_emb = self.embedding(tensor_med[:, d, :].squeeze(1))\n",
    "            out_vitals_emb = self.embedding(tensor_vitals[:, d, :].squeeze(1))\n",
    "            out_labs_emb = self.embedding(tensor_labs[:, d, :].squeeze(1))\n",
    "            print('out_med_emb', out_med_emb.size())\n",
    "            print('out_vitals_emb', out_vitals_emb.size())\n",
    "            print('out_labs_emb', out_labs_emb.size())\n",
    "\n",
    "            # lstm layer applied to embedded tensors\n",
    "            temp = self.lstm_day(out_med_emb)[0]\n",
    "            print(f'\\nlstm_day(out_med_emb)  {temp.size()} ==> [{batch_size}, {self.max_length_meds * 2 * self.H}]')\n",
    "            temp = temp.reshape(batch_size, self.max_length_meds * 2 * self.H)\n",
    "            output_lstm_med = self.inner_drop(self.fc_med(temp))\n",
    "            print(f'output_lstm_med {output_lstm_med.size()}')\n",
    "            # output_lstm_med = self.inner_drop(self.fc_med(\\\n",
    "            #                                     self.lstm_day(out_med_emb)[0]\\\n",
    "            #                                         .reshape(batch_size, self.max_length_meds * 2 * self.H)))\n",
    "\n",
    "            # output_lstm_vitals = self.inner_drop(self.fc_vitals(\\\n",
    "            #                                     self.lstm_day(out_vitals_emb)[0]\\\n",
    "            #                                         .reshape(batch_size, self.max_length_vitals * 2 * self.H)))\n",
    "            temp = self.lstm_day(out_vitals_emb)[0]\n",
    "            print(f'\\nlstm_day(out_vitals_emb)  {temp.size()} ==> [{batch_size}, {self.max_length_vitals * 2 * self.H}]')\n",
    "            temp = temp.reshape(batch_size, self.max_length_vitals * 2 * self.H)\n",
    "            output_lstm_vitals = self.inner_drop(self.fc_vitals(temp))\n",
    "            print(f'output_lstm_vitals {output_lstm_vitals.size()}')\n",
    "\n",
    "            # output_lstm_labs = self.inner_drop(self.fc_lab(\\\n",
    "            #                                     self.lstm_day(out_labs_emb)[0]\\\n",
    "            #                                         .reshape(batch_size, self.max_length_lab * 2 * self.H)))\n",
    "            temp = self.lstm_day(out_labs_emb)[0]\n",
    "            print(f'\\nlstm_day(out_labs_emb)  {temp.size()} ==> [{batch_size}, {self.max_length_lab * 2 * self.H}]')\n",
    "            temp = temp.reshape(batch_size, self.max_length_lab * 2 * self.H)\n",
    "            output_lstm_labs = self.inner_drop(self.fc_lab(temp))\n",
    "            print(f'output_lstm_labs {output_lstm_labs.size()}')   \n",
    "                         \n",
    "            # concatenate for all * days\n",
    "            full_output = torch.cat((full_output, \\\n",
    "                                        output_lstm_med,\\\n",
    "                                            output_lstm_vitals,\\\n",
    "                                                output_lstm_labs), dim=1) # \n",
    "\n",
    "        print('--------------')  \n",
    "        print('full_output size: ', full_output.size(), '\\n')\n",
    "        output = self.fc_adm(full_output)\n",
    "        print('fc_adm: ', output.size(), '\\n')\n",
    "        output_vector, _ = self.lstm_adm(output)\n",
    "        print('lstm_adm: ', output_vector.size(), '\\n')\n",
    "        # the fisrt transformation\n",
    "        output_vector_X = self.drop(output_vector)\n",
    "        projection_X = self.projection(output_vector_X)\n",
    "        # the second transformation\n",
    "        output_vector_Y = self.drop(output_vector)\n",
    "        projection_Y = self.projection(output_vector_Y)\n",
    "\n",
    "\n",
    "        return output_vector_X, projection_X, output_vector_Y, projection_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        batch_size,\n",
    "        file_path,\n",
    "        embedding_size,\n",
    "        device='cpu',\n",
    "        num_epochs=2,\n",
    "        epoch_patience=10,\n",
    "        best_valid_loss = float(\"Inf\"),\n",
    "        dimension=128,\n",
    "        save_model=True,\n",
    "        temperature=0.1):\n",
    "\n",
    "    \n",
    "    train_running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    total_train_steps = len(train_loader)\n",
    "    total_val_steps = len(valid_loader)\n",
    "    stop_training = 0\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_step = 1\n",
    "            print(f'Epoch {epoch+1}/{num_epochs} training..')\n",
    "            criterion = ContrastiveLoss(batch_size=batch_size, device=device, temperature=temperature)\n",
    "\n",
    "            for  tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds, idx in train_loader:\n",
    "                if train_step % 100==0:\n",
    "                    print(f'Step {train_step}/{total_train_steps}')\n",
    "                # print(f'Step {train_step}/{total_train_steps}')\n",
    "\n",
    "                vector_X, projectionX, vector_Y, projectionY = model(tensor_demo.to(device), tensor_diags.to(device),\\\n",
    "                                                                    tensor_meds.to(device), tensor_vitals.to(device),\\\n",
    "                                                                    tensor_labs.to(device))\n",
    "                                        \n",
    "                if train_step >= total_train_steps:\n",
    "                        new_batch_size = projectionX.size()[0]\n",
    "                        criterion = ContrastiveLoss(batch_size=new_batch_size, device=device, temperature=temperature)\n",
    "                \n",
    "                loss = criterion(projectionX.type(torch.float32), projectionY.type(torch.float32))\n",
    "                #   print(loss.item())\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_running_loss += loss.item()\n",
    "                train_step += 1\n",
    "\n",
    "                wandb.log({'step_train_loss': loss.item(), 'global_step': train_step})\n",
    "\n",
    "            epoch_average_train_loss = train_running_loss / len(train_loader)  \n",
    "\n",
    "            model.eval()\n",
    "            val_step = 1\n",
    "            print(f\"Validation started..\")\n",
    "            criterion = ContrastiveLoss(batch_size=batch_size, device=device, temperature=temperature)\n",
    "            with torch.no_grad():\n",
    "                for tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds, idx in valid_loader:\n",
    "                        vector_X, projectionX, vector_Y, projectionY = model(tensor_demo.to(device), tensor_diags.to(device),\\\n",
    "                                                                    tensor_meds.to(device), tensor_vitals.to(device),\\\n",
    "                                                                    tensor_labs.to(device))\n",
    "\n",
    "                        if val_step >= total_val_steps:\n",
    "                            new_batch_size = projectionX.size()[0]\n",
    "                            criterion = ContrastiveLoss(batch_size=new_batch_size, device=device, temperature=temperature)\n",
    "                                            \n",
    "                        loss = criterion(projectionX.type(torch.float32), projectionY.type(torch.float32))\n",
    "\n",
    "                        valid_running_loss += loss.item()\n",
    "                        val_step += 1\n",
    "                        \n",
    "\n",
    "            epoch_average_val_loss = valid_running_loss / len(valid_loader)\n",
    "\n",
    "            train_running_loss = 0.0\n",
    "            valid_running_loss = 0.0\n",
    "\n",
    "            print(f'Train loss {epoch_average_train_loss}, Validation loss {epoch_average_val_loss}')\n",
    "\n",
    "            wandb.log({'epoch_average_train_loss':epoch_average_train_loss, 'epoch_average_val_loss':epoch_average_val_loss, 'epoch':epoch+1})\n",
    "            \n",
    "            # checkpoint\n",
    "            if best_valid_loss > epoch_average_val_loss and save_model:\n",
    "                print(f'Validation loss decreased {best_valid_loss}==>{epoch_average_val_loss}')\n",
    "                best_valid_loss = epoch_average_val_loss\n",
    "                save_checkpoint(file_path + '/model.pt', model, optimizer, best_valid_loss)\n",
    "                wandb.save(file_path + '/model.pt')\n",
    "                stop_training = 0\n",
    "            else:\n",
    "                stop_training +=1\n",
    "            \n",
    "            if stop_training == epoch_patience:\n",
    "                break\n",
    "\n",
    "    print('Finished training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fixed_model = True\n",
    "fixed_model_with_diags = False\n",
    "cont_model = False\n",
    "\n",
    "def main(project_name, num_epochs, pred_window, max_day, PRETRAINED_PATH=None, drop=0.1, \\\n",
    "    temperature=0.5, embedding_size=200, min_frequency=1, BATCH_SIZE=16, small_dataset=True, \\\n",
    "        LR=0.000005, save_model=False, use_gpu=True, saving_folder_name=None, wandb_mode = 'online', \\\n",
    "            run_id=None):\n",
    "    \n",
    "    if use_gpu:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    else:\n",
    "        device='cpu'\n",
    "    print('device: ', device)\n",
    "    \n",
    "    #paths\n",
    "    CURR_PATH = os.getcwd()\n",
    "    PKL_PATH = CURR_PATH+'/pickles/'\n",
    "    DF_PATH = '/home/svetlana.maslenkova/LSTM/dataframes/'\n",
    "    TXT_DIR_TRAIN = CURR_PATH + '/txt_files/train'\n",
    "    destination_folder = '/l/users/svetlana.maslenkova/models' + '/pretraining/fc1_fixed/'\n",
    "\n",
    "\n",
    "    # Training the tokenizer\n",
    "    if exists(CURR_PATH + '/tokenizer.json'):\n",
    "        tokenizer = Tokenizer.from_file(CURR_PATH + '/tokenizer.json')\n",
    "        print(f'Tokenizer is loaded from ==> {CURR_PATH}/tokenizer.json. Vocab size is {tokenizer.get_vocab_size()}')\n",
    "    else:\n",
    "        print('Training tokenizer...')\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"UNK\"))\n",
    "        tokenizer.normalizer = normalizers.Sequence([Lowercase()])\n",
    "        # tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=False), Punctuation( behavior = 'removed')])\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation(behavior = 'isolated')])\n",
    "\n",
    "        trainer = BpeTrainer(special_tokens=[\"<s>\", \"</s>\", \"PAD\", \"UNK\", \"$\"], min_frequency=10)\n",
    "\n",
    "        files = glob.glob('/home/svetlana.maslenkova/LSTM/aki_prediction/txt_files/train'+'/*')\n",
    "        tokenizer.train(files, trainer)\n",
    "        tokenizer.post_processor = BertProcessing(\n",
    "                (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "                (\"<s>\", tokenizer.token_to_id(\"<s>\")), \n",
    "                )\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    # variables for classes\n",
    "    # max_length = {'demographics':5, 'lab_tests':400, 'vitals':200, 'medications':255}\n",
    "    max_length = {'demographics':5+2, 'diagnoses':35+2, 'lab_tests':300+2, 'vitals':31+2, 'medications':256+2}\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    print(f'Vocab size: {vocab_size}')\n",
    "\n",
    "    with open(DF_PATH + 'pid_train_df_pretraining.pkl', 'rb') as f:\n",
    "        pid_train_df = pickle.load(f)\n",
    "\n",
    "    with open(DF_PATH + 'pid_val_df_pretraining.pkl', 'rb') as f:\n",
    "        pid_val_df = pickle.load(f)\n",
    "\n",
    "    if small_dataset: frac=0.0001 \n",
    "    else: frac=1\n",
    "    \n",
    "    pid_train_df_small = pid_train_df.sample(frac=frac)\n",
    "    pid_val_df_small = pid_val_df.sample(frac=frac)\n",
    "\n",
    "    if fixed_model_with_diags:\n",
    "        train_dataset = MyDataset(pid_train_df.sample(frac=frac), tokenizer=tokenizer, max_length_day=400)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        val_dataset = MyDataset(pid_val_df.sample(frac=frac), tokenizer=tokenizer, max_length_day=400)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    elif cont_model:\n",
    "        train_dataset = MyDataset(pid_train_df_small, tokenizer=tokenizer, max_length=max_length)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        val_dataset = MyDataset(pid_val_df_small, tokenizer=tokenizer, max_length=max_length)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    elif new_fixed_model:\n",
    "        train_dataset = MyDataset(pid_train_df.sample(frac=frac), tokenizer=tokenizer, max_length=max_length)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        val_dataset = MyDataset(pid_val_df.sample(frac=frac), tokenizer=tokenizer, max_length=max_length)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print('DATA SHAPES: ')\n",
    "    print('train data shape: ', len(train_loader)*BATCH_SIZE)\n",
    "    print('val data shape: ', len(val_loader)*BATCH_SIZE)\n",
    "\n",
    "    if fixed_model_with_diags:\n",
    "        model = EHR_PRETRAINING(max_length=400, vocab_size=vocab_size, device=device).to(device)\n",
    "    elif cont_model:\n",
    "        model = EHR_model(embedding_size=embedding_size, vocab_size=vocab_size, max_length=max_length, pred_window=pred_window, max_day=max_day, drop=0.1).to(device)\n",
    "    elif new_fixed_model:\n",
    "        model = EHR_PRETRAINING(max_length=max_length, vocab_size=vocab_size, device=device, pred_window=2, observing_window=3,  H=128, embedding_size=200, drop=0.6)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    if PRETRAINED_PATH is not None:\n",
    "        load_checkpoint(PRETRAINED_PATH, model, optimizer, device=device)\n",
    "    \n",
    "    train_params = {'model':model,\n",
    "                    'optimizer':optimizer,\n",
    "                    'train_loader':train_loader,\n",
    "                    'valid_loader':val_loader,\n",
    "                    'batch_size':BATCH_SIZE,\n",
    "                    'embedding_size':embedding_size,\n",
    "                    'file_path':destination_folder,\n",
    "                    'num_epochs':num_epochs,\n",
    "                    'device':device,\n",
    "                    'save_model':save_model,\n",
    "                    'temperature':temperature\n",
    "    }\n",
    "\n",
    "    num_samples = (len(train_loader)+len(val_loader))*BATCH_SIZE // 1000\n",
    "\n",
    "    if saving_folder_name is None:\n",
    "        saving_folder_name = 'CL_WHOLE_FX_ND_' + '_bs' + str(BATCH_SIZE) +'_' + str(num_samples) + 'k' + '_lr'+ str(LR) + '_Adam' + '_temp' + str(temperature) + '_drop' + str(drop)\n",
    "    file_path = destination_folder + saving_folder_name\n",
    "    train_params['file_path'] = file_path\n",
    "\n",
    "    print(f'\\n\\nMODEL PATH: {file_path}')\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "        \n",
    "    run_name = saving_folder_name\n",
    "    print('Run name: ', run_name)\n",
    "    args = {'optimizer':'Adam', 'LR':LR, 'min_frequency':min_frequency, 'dropout':drop, \\\n",
    "        'vocab_size':vocab_size, 'embedding_size':embedding_size, 'pretrained':'FC1', \\\n",
    "            'temperature':temperature, 'batch_size':BATCH_SIZE,  'experiment':'FX_DIAGS_ND', \\\n",
    "                'pretrained':'whole'}\n",
    "\n",
    "    if run_id is None:    \n",
    "        run_id = wandb.util.generate_id()  \n",
    "        resume = 'allow' \n",
    "    else:\n",
    "        resume = 'must'\n",
    "\n",
    "    print('Run id is: ', run_id)\n",
    "    wandb.init(project=project_name, name=run_name, mode=wandb_mode, config=args, id=run_id, resume=resume)\n",
    "    train(**train_params)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n",
      "Training tokenizer...\n",
      "\n",
      "\n",
      "\n",
      "Vocab size: 22569\n",
      "DATA SHAPES: \n",
      "train data shape:  48\n",
      "val data shape:  16\n",
      "\n",
      "\n",
      "MODEL PATH: /l/users/svetlana.maslenkova/models/pretraining/fc1_fixed/test\n",
      "Run name:  test\n",
      "Run id is:  14bnw4hp\n",
      "Epoch 1/1 training..\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[16, 9472]' is invalid for input of size 143360",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2540000/711473782.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_frequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmall_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mLR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_folder_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwandb_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'disabled'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             run_id=None)\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2540000/2615113654.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(project_name, num_epochs, pred_window, max_day, PRETRAINED_PATH, drop, temperature, embedding_size, min_frequency, BATCH_SIZE, small_dataset, LR, save_model, use_gpu, saving_folder_name, wandb_mode, run_id)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Run id is: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwandb_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2540000/1115800015.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, valid_loader, batch_size, file_path, embedding_size, device, num_epochs, epoch_patience, best_valid_loss, dimension, save_model, temperature)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 vector_X, projectionX, vector_Y, projectionY = model(tensor_demo.to(device), tensor_diags.to(device),\\\n\u001b[1;32m     38\u001b[0m                                                                     \u001b[0mtensor_meds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_vitals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                                                                     tensor_labs.to(device))\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtrain_step\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mtotal_train_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda_envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2540000/2208218639.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor_demo, tensor_diags, tensor_med, tensor_vitals, tensor_labs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# reshape and concat demographics and diags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mout_lstm_diags_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_lstm_diags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length_diags\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mout_lstm_demo_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_lstm_demo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length_demo\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[16, 9472]' is invalid for input of size 143360"
     ]
    }
   ],
   "source": [
    "main(project_name='test', num_epochs=1, pred_window=None, max_day=None, PRETRAINED_PATH=None, drop=0.1, \\\n",
    "    temperature=0.1, embedding_size=200, min_frequency=5, BATCH_SIZE=16, small_dataset=True, \\\n",
    "        LR=0.00001, save_model=True, use_gpu=False, saving_folder_name='test', wandb_mode = 'disabled', \\\n",
    "            run_id=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer is loaded from ==> /home/svetlana.maslenkova/LSTM/aki_prediction/pretraining/tokenizer.json. Vocab size is 22569\n"
     ]
    }
   ],
   "source": [
    "#paths\n",
    "CURR_PATH = os.getcwd()\n",
    "PKL_PATH = CURR_PATH+'/pickles/'\n",
    "DF_PATH = '/home/svetlana.maslenkova/LSTM/dataframes/'\n",
    "TXT_DIR_TRAIN = CURR_PATH + '/txt_files/train'\n",
    "destination_folder = '/l/users/svetlana.maslenkova/models' + '/pretraining/fc1_fixed/'\n",
    "\n",
    "# Training the tokenizer\n",
    "if exists(CURR_PATH + '/tokenizer.json'):\n",
    "    tokenizer = Tokenizer.from_file(CURR_PATH + '/tokenizer.json')\n",
    "    print(f'Tokenizer is loaded from ==> {CURR_PATH}/tokenizer.json. Vocab size is {tokenizer.get_vocab_size()}')\n",
    "else:\n",
    "    print('Training tokenizer...')\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"UNK\"))\n",
    "    tokenizer.normalizer = normalizers.Sequence([Lowercase()])\n",
    "    # tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=False), Punctuation( behavior = 'removed')])\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation(behavior = 'isolated')])\n",
    "\n",
    "    trainer = BpeTrainer(special_tokens=[\"<s>\", \"</s>\", \"PAD\", \"UNK\", \"$\"], min_frequency=10)\n",
    "\n",
    "    files = glob.glob('/home/svetlana.maslenkova/LSTM/aki_prediction/txt_files/train'+'/*')\n",
    "    tokenizer.train(files, trainer)\n",
    "    tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")), \n",
    "            )\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    tokenizer.save(CURR_PATH + '/tokenizer.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 22569\n"
     ]
    }
   ],
   "source": [
    "device='cpu'\n",
    "max_length = {'demographics':5+2, 'diagnoses':35+2, 'lab_tests':300+2, 'vitals':31+2, 'medications':256+2}\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f'Vocab size: {vocab_size}')\n",
    "\n",
    "small_dataset = True\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DF_PATH + 'pid_train_df_pretraining.pkl', 'rb') as f:\n",
    "    pid_train_df = pickle.load(f)\n",
    "\n",
    "with open(DF_PATH + 'pid_val_df_pretraining.pkl', 'rb') as f:\n",
    "    pid_val_df = pickle.load(f)\n",
    "\n",
    "if small_dataset: frac=0.0001 \n",
    "else: frac=1\n",
    "\n",
    "pid_train_df_small = pid_train_df.sample(frac=frac)\n",
    "pid_val_df_small = pid_val_df.sample(frac=frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_fixed_model\n"
     ]
    }
   ],
   "source": [
    "if fixed_model_with_diags:\n",
    "    train_dataset = MyDataset(pid_train_df.sample(frac=frac), tokenizer=tokenizer, max_length_day=400)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    val_dataset = MyDataset(pid_val_df.sample(frac=frac), tokenizer=tokenizer, max_length_day=400)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "elif cont_model:\n",
    "    train_dataset = MyDataset(pid_train_df_small, tokenizer=tokenizer, max_length=max_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    val_dataset = MyDataset(pid_val_df_small, tokenizer=tokenizer, max_length=max_length)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "elif new_fixed_model:\n",
    "    print(f'new_fixed_model')\n",
    "    train_dataset = MyDataset(pid_train_df.sample(frac=frac), tokenizer=tokenizer, max_length=max_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    val_dataset = MyDataset(pid_val_df.sample(frac=frac), tokenizer=tokenizer, max_length=max_length)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "if fixed_model_with_diags:\n",
    "    model = EHR_PRETRAINING(max_length=400, vocab_size=vocab_size, device=device).to(device)\n",
    "elif cont_model:\n",
    "    model = EHR_model(embedding_size=embedding_size, vocab_size=vocab_size, max_length=max_length, pred_window=pred_window, max_day=max_day, drop=0.1).to(device)\n",
    "elif new_fixed_model:\n",
    "    pretrained_model = EHR_PRETRAINING(max_length=max_length, vocab_size=vocab_size, device=device, pred_window=2, observing_window=3,  H=128, embedding_size=200, drop=0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 200\n",
    "num_epochs = 1\n",
    "save_model = False\n",
    "temperature = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "train_params = {'model':model,\n",
    "                'optimizer':optimizer,\n",
    "                'train_loader':train_loader,\n",
    "                'valid_loader':val_loader,\n",
    "                'batch_size':BATCH_SIZE,\n",
    "                'embedding_size':embedding_size,\n",
    "                'file_path':destination_folder,\n",
    "                'num_epochs':num_epochs,\n",
    "                'device':device,\n",
    "                'save_model':save_model,\n",
    "                'temperature':temperature\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 training..\n",
      "out_emb_diags:  torch.Size([36, 37, 200])\n",
      "out_emb_demo:  torch.Size([36, 7, 200])\n",
      "out_lstm_diags:  torch.Size([36, 37, 256])\n",
      "out_lstm_demo:  torch.Size([36, 7, 256])\n",
      "out_lstm_diags_reshaped torch.Size([36, 9472])\n",
      "out_lstm_demo_reshaped torch.Size([36, 1792])\n",
      "full_output torch.Size([36, 11264])\n",
      "out_med_emb torch.Size([36, 258, 200])\n",
      "out_vitals_emb torch.Size([36, 33, 200])\n",
      "out_labs_emb torch.Size([36, 302, 200])\n",
      "output_lstm_med torch.Size([36, 2048])\n",
      "output_lstm_vitals torch.Size([36, 2048])\n",
      "output_lstm_labs torch.Size([36, 2048])\n",
      "--------------\n",
      "out_med_emb torch.Size([36, 258, 200])\n",
      "out_vitals_emb torch.Size([36, 33, 200])\n",
      "out_labs_emb torch.Size([36, 302, 200])\n",
      "output_lstm_med torch.Size([36, 2048])\n",
      "output_lstm_vitals torch.Size([36, 2048])\n",
      "output_lstm_labs torch.Size([36, 2048])\n",
      "--------------\n",
      "out_med_emb torch.Size([36, 258, 200])\n",
      "out_vitals_emb torch.Size([36, 33, 200])\n",
      "out_labs_emb torch.Size([36, 302, 200])\n",
      "output_lstm_med torch.Size([36, 2048])\n",
      "output_lstm_vitals torch.Size([36, 2048])\n",
      "output_lstm_labs torch.Size([36, 2048])\n",
      "--------------\n",
      "full_output size:  torch.Size([36, 29696]) \n",
      "\n",
      "fc_adm:  torch.Size([36, 2048]) \n",
      "\n",
      "output_vector:  torch.Size([36, 128]) \n",
      "\n",
      "Validation started..\n",
      "out_emb_diags:  torch.Size([2, 37, 200])\n",
      "out_emb_demo:  torch.Size([2, 7, 200])\n",
      "out_lstm_diags:  torch.Size([2, 37, 256])\n",
      "out_lstm_demo:  torch.Size([2, 7, 256])\n",
      "out_lstm_diags_reshaped torch.Size([2, 9472])\n",
      "out_lstm_demo_reshaped torch.Size([2, 1792])\n",
      "full_output torch.Size([2, 11264])\n",
      "out_med_emb torch.Size([2, 258, 200])\n",
      "out_vitals_emb torch.Size([2, 33, 200])\n",
      "out_labs_emb torch.Size([2, 302, 200])\n",
      "output_lstm_med torch.Size([2, 2048])\n",
      "output_lstm_vitals torch.Size([2, 2048])\n",
      "output_lstm_labs torch.Size([2, 2048])\n",
      "--------------\n",
      "out_med_emb torch.Size([2, 258, 200])\n",
      "out_vitals_emb torch.Size([2, 33, 200])\n",
      "out_labs_emb torch.Size([2, 302, 200])\n",
      "output_lstm_med torch.Size([2, 2048])\n",
      "output_lstm_vitals torch.Size([2, 2048])\n",
      "output_lstm_labs torch.Size([2, 2048])\n",
      "--------------\n",
      "out_med_emb torch.Size([2, 258, 200])\n",
      "out_vitals_emb torch.Size([2, 33, 200])\n",
      "out_labs_emb torch.Size([2, 302, 200])\n",
      "output_lstm_med torch.Size([2, 2048])\n",
      "output_lstm_vitals torch.Size([2, 2048])\n",
      "output_lstm_labs torch.Size([2, 2048])\n",
      "--------------\n",
      "full_output size:  torch.Size([2, 29696]) \n",
      "\n",
      "fc_adm:  torch.Size([2, 2048]) \n",
      "\n",
      "output_vector:  torch.Size([2, 128]) \n",
      "\n",
      "Train loss 4.495946884155273, Validation loss 1.0028973817825317\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "train(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_emb_diags:  torch.Size([36, 37, 200])\n",
      "out_emb_demo:  torch.Size([36, 7, 200])\n",
      "out_lstm_diags:  torch.Size([36, 37, 256])\n",
      "out_lstm_demo:  torch.Size([36, 7, 256])\n",
      "out_lstm_diags_reshaped torch.Size([36, 9472])\n",
      "out_lstm_demo_reshaped torch.Size([36, 1792])\n",
      "full_output torch.Size([36, 11264])\n",
      "--------------\n",
      "out_med_emb torch.Size([36, 258, 200])\n",
      "out_vitals_emb torch.Size([36, 33, 200])\n",
      "out_labs_emb torch.Size([36, 302, 200])\n",
      "\n",
      "lstm_day(out_med_emb)  torch.Size([36, 258, 256]) ==> [36, 66048]\n",
      "output_lstm_med torch.Size([36, 2048])\n",
      "\n",
      "lstm_day(out_vitals_emb)  torch.Size([36, 33, 256]) ==> [36, 8448]\n",
      "output_lstm_vitals torch.Size([36, 2048])\n",
      "\n",
      "lstm_day(out_labs_emb)  torch.Size([36, 302, 256]) ==> [36, 77312]\n",
      "output_lstm_labs torch.Size([36, 2048])\n",
      "--------------\n",
      "out_med_emb torch.Size([36, 258, 200])\n",
      "out_vitals_emb torch.Size([36, 33, 200])\n",
      "out_labs_emb torch.Size([36, 302, 200])\n",
      "\n",
      "lstm_day(out_med_emb)  torch.Size([36, 258, 256]) ==> [36, 66048]\n",
      "output_lstm_med torch.Size([36, 2048])\n",
      "\n",
      "lstm_day(out_vitals_emb)  torch.Size([36, 33, 256]) ==> [36, 8448]\n",
      "output_lstm_vitals torch.Size([36, 2048])\n",
      "\n",
      "lstm_day(out_labs_emb)  torch.Size([36, 302, 256]) ==> [36, 77312]\n",
      "output_lstm_labs torch.Size([36, 2048])\n",
      "--------------\n",
      "out_med_emb torch.Size([36, 258, 200])\n",
      "out_vitals_emb torch.Size([36, 33, 200])\n",
      "out_labs_emb torch.Size([36, 302, 200])\n",
      "\n",
      "lstm_day(out_med_emb)  torch.Size([36, 258, 256]) ==> [36, 66048]\n",
      "output_lstm_med torch.Size([36, 2048])\n",
      "\n",
      "lstm_day(out_vitals_emb)  torch.Size([36, 33, 256]) ==> [36, 8448]\n",
      "output_lstm_vitals torch.Size([36, 2048])\n",
      "\n",
      "lstm_day(out_labs_emb)  torch.Size([36, 302, 256]) ==> [36, 77312]\n",
      "output_lstm_labs torch.Size([36, 2048])\n",
      "--------------\n",
      "full_output size:  torch.Size([36, 29696]) \n",
      "\n",
      "fc_adm:  torch.Size([36, 2048]) \n",
      "\n",
      "output_vector:  torch.Size([36, 128]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds, idx = next(iter(train_loader))\n",
    "\n",
    "vector_X, projectionX, vector_Y, projectionY = model(tensor_demo.to(device), tensor_diags.to(device),\\\n",
    "                                                    tensor_meds.to(device), tensor_vitals.to(device),\\\n",
    "                                                    tensor_labs.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = {'demographics':5+2, 'diagnoses':35+2, 'lab_tests':300+2, 'vitals':31+2, 'medications':256+2}\n",
    "tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds, idx = next(iter(train_loader))\n",
    "model = EHR_PRETRAINING(max_length=max_length, vocab_size=vocab_size, device=device, pred_window=2, observing_window=3,  H=128, embedding_size=200, drop=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 35])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_diags.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "CURR_PATH = os.getcwd()\n",
    "PKL_PATH = CURR_PATH+'/pickles/'\n",
    "DF_PATH = CURR_PATH +'/dataframes/'\n",
    "TXT_DIR_TRAIN = CURR_PATH + '/txt_files/train'\n",
    "destination_folder = '/l/users/svetlana.maslenkova/models' + '/pretraining/fc1_fixed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer.from_file('/home/svetlana.maslenkova/LSTM/aki_prediction/tokenizer.json')\n",
    "# print(f' Vocab size is {tokenizer.get_vocab_size()}')\n",
    "\n",
    "with open('/home/svetlana.maslenkova/LSTM/dataframes/pid_train_df_pretraining.pkl', 'rb') as f:\n",
    "    pid_train_df = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>demographics_in_visit</th>\n",
       "      <th>lab_tests_in_visit</th>\n",
       "      <th>medications_in_visit</th>\n",
       "      <th>vitals_in_visit</th>\n",
       "      <th>days_in_visit</th>\n",
       "      <th>previous_diagnoses</th>\n",
       "      <th>days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10467237</td>\n",
       "      <td>20000019</td>\n",
       "      <td>[hispanic latino f 76  , hispanic latino f 76 ...</td>\n",
       "      <td>[hematology blood hematocrit  26.5  %; hematol...</td>\n",
       "      <td>[pneumococcal 23-valent polysaccharide vaccine...</td>\n",
       "      <td>[temp  98.0  heartrate  65.0  resprate  16.0  ...</td>\n",
       "      <td>[hispanic latino f 76  $temp  98.0  heartrate ...</td>\n",
       "      <td></td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id                              demographics_in_visit  \\\n",
       "0    10467237  20000019  [hispanic latino f 76  , hispanic latino f 76 ...   \n",
       "\n",
       "                                  lab_tests_in_visit  \\\n",
       "0  [hematology blood hematocrit  26.5  %; hematol...   \n",
       "\n",
       "                                medications_in_visit  \\\n",
       "0  [pneumococcal 23-valent polysaccharide vaccine...   \n",
       "\n",
       "                                     vitals_in_visit  \\\n",
       "0  [temp  98.0  heartrate  65.0  resprate  16.0  ...   \n",
       "\n",
       "                                       days_in_visit previous_diagnoses  \\\n",
       "0  [hispanic latino f 76  $temp  98.0  heartrate ...                      \n",
       "\n",
       "        days  \n",
       "0  [0, 1, 2]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "LIST_HADMS = []\n",
    "list_diags = []\n",
    "# for _, row in pid_val_df.iterrows():\n",
    "#     # try:\n",
    "#     #     list_diags.append(row.previous_diagnoses[0].replace('PAD ', '').replace('PAD', ''))\n",
    "#     #     LIST_HADMS.append(row.hadm_id)\n",
    "#     # except:\n",
    "#     #     print(row.previous_diagnoses[0])\n",
    "#     #     print(np.isnan(row.previous_diagnoses[0]))\n",
    "#     #     break\n",
    "#     try:\n",
    "#         if isinstance(row.previous_diagnoses[0], float):\n",
    "#             list_diags.append('')      \n",
    "#         else:\n",
    "#             list_diags.append(row.previous_diagnoses[0].replace('PAD ', '').replace('PAD', ''))  \n",
    "#         LIST_HADMS.append(row.hadm_id) \n",
    "#     except:\n",
    "#         print(row.previous_diagnoses[0])\n",
    "#         print(type(row.previous_diagnoses[0]))\n",
    "#         raise\n",
    "#         break\n",
    "\n",
    "#     i+=1\n",
    "#     # if i>0:break\n",
    "\n",
    "pid_train_df['previous_diags'] = [val.replace('PAD ', '').replace('PAD', '') if not isinstance(val, float) else '' for val in pid_train_df['previous_diags']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer...\n",
      "\n",
      "\n",
      "\n",
      "Vocab size is 22569\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import Digits, Punctuation, Whitespace\n",
    "from tokenizers.normalizers import Lowercase, Replace\n",
    "from tokenizers import pre_tokenizers, normalizers\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "print('Training tokenizer...')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"UNK\"))\n",
    "tokenizer.normalizer = normalizers.Sequence([Lowercase()])\n",
    "# tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=False), Punctuation( behavior = 'removed')])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation(behavior = 'isolated')])\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=[\"<s>\", \"</s>\", \"PAD\", \"UNK\", \"$\"], min_frequency=10)\n",
    "\n",
    "files = glob.glob('/home/svetlana.maslenkova/LSTM/aki_prediction/txt_files/train'+'/*')\n",
    "tokenizer.train(files, trainer)\n",
    "tokenizer.post_processor = BertProcessing(\n",
    "        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "        (\"<s>\", tokenizer.token_to_id(\"<s>\")), \n",
    "        )\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(f'Vocab size is {tokenizer.get_vocab_size()}')\n",
    "tokenizer.save('/home/svetlana.maslenkova/LSTM' + '/tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22569\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(vocab_size)\n",
    "device='cpu'\n",
    "frac=1\n",
    "BATCH_SIZE=1024\n",
    "LR=0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(pid_train_df.sample(frac=frac), tokenizer=tokenizer, max_length=max_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EHR_PRETRAINING(max_length=400, vocab_size=vocab_size, device=device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([25608940, 26676014, 25229043, 22517940, 20539214, 28675100, 21815284,\n",
      "        22431587, 24259871, 29837158, 23507274, 27676611, 21343164, 20376368,\n",
      "        28143288, 27172224])\n"
     ]
    }
   ],
   "source": [
    "tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds, idx = next(iter(train_loader))\n",
    "print(idx)\n",
    "# for idx, (tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds, idx) in enumerate(train_dataset):\n",
    "#     print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp 98 . 8 heartrate 96 . 0 resprate 18 . 0 o2sat 97 . 0 sbp 100 . 0 dbp 59 . 0 rhythm pain 8'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "tokenizer.decode(tensor_vitals[i][0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0,0,0,0,1,0])\n",
    "idx = np.where(a==1)[0][0]\n",
    "a[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens_meds = []\n",
    "tokenizer.enable_truncation(500)\n",
    "for tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds, idx in train_loader:\n",
    "    for sample in tensor_meds.cpu().detach():\n",
    "        for day in sample:\n",
    "            # print(day)\n",
    "            idx = np.where(day==1)[0][0]\n",
    "            # print(len(day[:idx]))\n",
    "            lens_meds.append(len(day[:idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Num of tensors')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(lens, density=False, bins=150)\n",
    "plt.ylim(0, 200000)\n",
    "plt.grid()\n",
    "plt.title('Distribution of number of tokens in laboratory results tensors')\n",
    "plt.xlabel('Num of tokens')\n",
    "plt.ylabel('Num of tensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Num of tensors')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "print(np.max(lens_meds))\n",
    "plt.hist(lens_meds, density=False, bins=150)\n",
    "plt.ylim(0, 60000)\n",
    "plt.grid()\n",
    "plt.title('Distribution of number of tokens in medications tensors')\n",
    "plt.xlabel('Num of tokens')\n",
    "plt.ylabel('Num of tensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 478 565  17  26 487 960  17  19 489 280  17  19 488 117  17  19 485\n",
      " 988  17  19 483 326  17  19 475 479   1   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2\n",
      "   2   2   2   2] ===> temp 97 . 7 heartrate 53 . 0 resprate 18 . 0 o2sat 10 . 0 sbp 129 . 0 dbp 99 . 0 rhythm pain\n",
      "[0 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] ===> \n",
      "[0 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] ===> \n",
      "[0 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] ===> \n",
      "[0 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] ===> \n"
     ]
    }
   ],
   "source": [
    "for p in tensor_vitals[i]:\n",
    "    print(p.cpu().detach().numpy(), \"===>\", tokenizer.decode(p.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day = pid_train_df[pid_train_df.hadm_id==idx[i].item()].previous_diagnoses.values[0]\n",
    "day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de119 db20 de785 di10 dr338'"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[0, 4277, 849, 3387, 828, 3507, 1]\n"
     ]
    }
   ],
   "source": [
    "print(len(output.ids))\n",
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <s>\n",
      "4277 d42843\n",
      "849 d2449\n",
      "3387 d5939\n",
      "828 d4280\n",
      "3507 d4239\n",
      "1 </s>\n"
     ]
    }
   ],
   "source": [
    "for id_ in output.ids:\n",
    "    print(id_, tokenizer.id_to_token(id_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(tensor_vitals[i].cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1091699/3265823591.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_vitals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_to_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "for id_ in tensor_vitals[i].cpu().detach().numpy():\n",
    "    print(id_, tokenizer.id_to_token(id_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7'"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.id_to_token(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Functions\n",
    "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(load_path, model, optimizer, device):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_metrics(load_path):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']\n",
    "\n",
    "def calculate_class_weights(data_loader):\n",
    "    labels = np.array([])\n",
    "    for tensor_day, tensor_diags, tensor_labels, idx in data_loader:\n",
    "        labels = np.concatenate([labels, tensor_labels], axis=0) if labels.size else tensor_labels\n",
    "    n_pos = np.sum(labels==1, axis=0)\n",
    "    n_neg = np.sum(labels==0, axis=0)\n",
    "    pos_weight = np.round(n_neg / n_pos, 2)\n",
    "    \n",
    "    return pos_weight\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHR_FINETUNING(nn.Module):\n",
    "    def __init__(self, pretrained_model, max_length, vocab_size, device, pred_window=2, observing_window=3,  H=128, embedding_size=200, drop=0.6):\n",
    "        super(EHR_FINETUNING, self).__init__()\n",
    "\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.H = H\n",
    "        self.max_length_demo = max_length['demographics']\n",
    "        self.max_length_diags = max_length['diagnoses']\n",
    "        self.max_length_meds = max_length['medications']\n",
    "        self.max_length_vitals = max_length['vitals']\n",
    "        self.max_length_lab = max_length['lab_tests']\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device\n",
    "        self.drop = drop\n",
    "\n",
    "        # self.embedding = pretrained_model\n",
    "        self.embedding = pretrained_model.embedding\n",
    "\n",
    "        self.lstm_day = pretrained_model.lstm_day\n",
    "\n",
    "        self.fc_med = pretrained_model.fc_med\n",
    "        self.fc_vitals = pretrained_model.fc_vitals\n",
    "        self.fc_lab = pretrained_model.fc_lab\n",
    "\n",
    "        self.fc_adm = pretrained_model.fc_adm\n",
    "\n",
    "        self.lstm_adm = pretrained_model.lstm_adm\n",
    "\n",
    "        self.drop = nn.Dropout(p=drop)\n",
    "        self.inner_drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc_2 = nn.Linear(pretrained_model.lstm_adm.hidden_size, 2)\n",
    "\n",
    "    def forward(self, tensor_demo, tensor_diags, tensor_med, tensor_vitals, tensor_labs):\n",
    "\n",
    "        batch_size = tensor_demo.size()[0]\n",
    "\n",
    "        # embeddings\n",
    "        out_emb_diags = self.embedding(tensor_diags.squeeze(1)) # [16, 37, 200]\n",
    "        out_emb_demo =  self.embedding(tensor_demo.squeeze(1))  # [16, 7, 200]\n",
    "        # print(f'out_emb_diags: ', out_emb_diags.size())\n",
    "        # print(f'out_emb_demo: ', out_emb_demo.size())\n",
    "        # lstm for demographic and diagnoses\n",
    "        out_lstm_diags, _ = self.lstm_day(out_emb_diags)    # [16, 37, 256]\n",
    "        out_lstm_demo, _ = self.lstm_day(out_emb_demo)      # [16, 7, 256]\n",
    "        # print(f'out_lstm_diags: ', out_lstm_diags.size())\n",
    "        # print(f'out_lstm_demo: ', out_lstm_demo.size())\n",
    "        # reshape and concat demographics and diags\n",
    "        out_lstm_diags_reshaped = out_lstm_diags.reshape(batch_size, self.max_length_diags * 2 * self.H)\n",
    "        out_lstm_demo_reshaped = out_lstm_demo.reshape(batch_size, self.max_length_demo * 2 * self.H)\n",
    "        # print(f'out_lstm_diags_reshaped', out_lstm_diags_reshaped.size())\n",
    "        # print(f'out_lstm_demo_reshaped', out_lstm_demo_reshaped.size())\n",
    "        full_output = torch.cat([out_lstm_demo_reshaped, out_lstm_diags_reshaped], dim=1)   # [16, 11264]\n",
    "        # print(f'full_output', full_output.size())\n",
    "\n",
    "        for d in range(self.observing_window):\n",
    "            # print('--------------')  \n",
    "            # embedding layer applied to all tensors \n",
    "            out_med_emb = self.embedding(tensor_med[:, d, :].squeeze(1))\n",
    "            out_vitals_emb = self.embedding(tensor_vitals[:, d, :].squeeze(1))\n",
    "            out_labs_emb = self.embedding(tensor_labs[:, d, :].squeeze(1))\n",
    "            # print('out_med_emb', out_med_emb.size())\n",
    "            # print('out_vitals_emb', out_vitals_emb.size())\n",
    "            # print('out_labs_emb', out_labs_emb.size())\n",
    "\n",
    "            # lstm layer applied to embedded tensors\n",
    "            temp = self.lstm_day(out_med_emb)[0]\n",
    "            # print(f'\\nlstm_day(out_med_emb)  {temp.size()} ==> [{batch_size}, {self.max_length_meds * 2 * self.H}]')\n",
    "            temp = temp.reshape(batch_size, self.max_length_meds * 2 * self.H)\n",
    "            output_lstm_med = self.inner_drop(self.fc_med(temp))\n",
    "            # print(f'output_lstm_med {output_lstm_med.size()}')\n",
    "            output_lstm_med = self.inner_drop(self.fc_med(\\\n",
    "                                                self.lstm_day(out_med_emb)[0]\\\n",
    "                                                    .reshape(batch_size, self.max_length_meds * 2 * self.H)))\n",
    "\n",
    "            output_lstm_vitals = self.inner_drop(self.fc_vitals(\\\n",
    "                                                self.lstm_day(out_vitals_emb)[0]\\\n",
    "                                                    .reshape(batch_size, self.max_length_vitals * 2 * self.H)))\n",
    "\n",
    "\n",
    "            output_lstm_labs = self.inner_drop(self.fc_lab(\\\n",
    "                                                self.lstm_day(out_labs_emb)[0]\\\n",
    "                                                    .reshape(batch_size, self.max_length_lab * 2 * self.H)))\n",
    "                         \n",
    "            # concatenate for all * days\n",
    "            full_output = torch.cat((full_output, \\\n",
    "                                        output_lstm_med,\\\n",
    "                                            output_lstm_vitals,\\\n",
    "                                                output_lstm_labs), dim=1) # \n",
    "\n",
    "        # print('--------------')  \n",
    "        # print('full_output size: ', full_output.size(), '\\n')\n",
    "        output = self.fc_adm(full_output)\n",
    "        # print('fc_adm: ', output.size(), '\\n')\n",
    "        output, _ = self.lstm_adm(output)\n",
    "        # print('lstm_adm: ', output.size(), '\\n')\n",
    "        output = self.drop(output)\n",
    "        output = self.fc_2(output)\n",
    "        # print('output after fc_2', output.size())\n",
    "        output = torch.squeeze(output, 1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_length, pred_window=2, observing_window=3):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.observing_window = observing_window\n",
    "        self.pred_window = pred_window\n",
    "        self.max_length = max_length\n",
    "        # self.max_length_diags = 35\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.make_matrices(idx)\n",
    "    \n",
    "    def tokenize(self, text, max_length): \n",
    "        \n",
    "        # max_length = max_length + 2\n",
    "        self.tokenizer.enable_truncation(max_length=max_length)\n",
    "\n",
    "        output = self.tokenizer.encode(text)\n",
    "\n",
    "        # padding and truncation\n",
    "        if len(output.ids) < max_length:\n",
    "            len_missing_token = max_length - len(output.ids)\n",
    "            padding_vec = [self.tokenizer.token_to_id('PAD') for _ in range(len_missing_token)]\n",
    "            token_output = [*output.ids, *padding_vec]\n",
    "        elif len(output.ids) > max_length:\n",
    "            token_output = output.ids[:max_length]\n",
    "        else:\n",
    "            token_output = output.ids\n",
    "        \n",
    "        return token_output\n",
    "\n",
    "    def make_matrices(self, idx):\n",
    "        \n",
    "        hadm_id = self.df.hadm_id.values[idx]\n",
    "        diagnoses_info = self.df.previous_diagnoses.values[idx]\n",
    "        demo_info = self.df.demographics_in_visit.values[idx][0]\n",
    "        lab_info = self.df.lab_tests_in_visit.values[idx]\n",
    "        med_info = self.df.medications_in_visit.values[idx]\n",
    "        vitals_info = self.df.vitals_in_visit.values[idx]\n",
    "        \n",
    "        aki_status = self.df.aki_status_in_visit.values[idx]\n",
    "        days = self.df.days.values[idx]\n",
    "        # print(idx)\n",
    "\n",
    "        lab_info_list = []\n",
    "        med_info_list = []\n",
    "        vitals_info_list = []\n",
    "        labels = []\n",
    "        label_24 = None\n",
    "        label_48 = None\n",
    "\n",
    "        for day in range(days[0], days[0] + self.observing_window + self.pred_window):\n",
    "            # print('day', day)\n",
    "            if day not in days:\n",
    "                labels.append(0)\n",
    "                vitals_info_list.append(self.tokenize('', self.max_length['vitals']))\n",
    "                lab_info_list.append(self.tokenize('', self.max_length['lab_tests']))\n",
    "                med_info_list.append(self.tokenize('', self.max_length['medications']))\n",
    "\n",
    "            else:\n",
    "                i = days.index(day)\n",
    "\n",
    "                if np.isfinite(aki_status[i]):                    \n",
    "                    labels.append(aki_status[i])\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "\n",
    "                # vitals\n",
    "                if (str(vitals_info[i]) == 'nan') or (vitals_info[i] == np.nan):\n",
    "                    vitals_info_list.append(self.tokenize('PAD', self.max_length['vitals']))\n",
    "                else:\n",
    "                    vitals_info_list.append(self.tokenize(vitals_info[i], self.max_length['vitals']))\n",
    "\n",
    "                # lab results\n",
    "                if (str(lab_info[i]) == 'nan') or (lab_info[i] == np.nan):\n",
    "                    lab_info_list.append(self.tokenize('PAD', self.max_length['lab_tests']))\n",
    "                else:\n",
    "                    lab_info_list.append(self.tokenize(lab_info[i], self.max_length['lab_tests']))\n",
    "                \n",
    "                # medications\n",
    "                if (str(med_info[i]) == 'nan') or (med_info[i] == np.nan):\n",
    "                    med_info_list.append(self.tokenize('PAD', self.max_length['medications']))\n",
    "                else:\n",
    "                    med_info_list.append(self.tokenize(med_info[i], self.max_length['medications']))\n",
    "\n",
    "        # diagnoses\n",
    "        if (str(diagnoses_info) == 'nan') or (diagnoses_info == np.nan):\n",
    "            diagnoses_info = self.tokenize('PAD', self.max_length['diagnoses'])\n",
    "        else:\n",
    "            diagnoses_info = self.tokenize(diagnoses_info, self.max_length['diagnoses'])\n",
    "\n",
    "        # demographics\n",
    "        if (str(demo_info) == 'nan') or (demo_info == np.nan):\n",
    "            demo_info = self.tokenize('PAD', self.max_length_diags)\n",
    "        else:\n",
    "            demo_info = self.tokenize(demo_info, self.max_length['demographics'])\n",
    "\n",
    "        # get labels for 48h\n",
    "\n",
    "        # get labels for 24h\n",
    "        label_24 = labels[-self.pred_window]\n",
    "        if sum(labels[-self.pred_window:]) > 0:\n",
    "            label_48 = 1\n",
    "        else:\n",
    "            label_48 = 0\n",
    "        \n",
    "        #make tensors\n",
    "        tensor_demo = torch.tensor(demo_info, dtype=torch.int64)\n",
    "        tensor_diags = torch.tensor(diagnoses_info, dtype=torch.int64)\n",
    "        tensor_vitals = torch.tensor(vitals_info_list, dtype=torch.int64)\n",
    "        tensor_labs = torch.tensor(lab_info_list, dtype=torch.int64)\n",
    "        tensor_meds = torch.tensor(med_info_list, dtype=torch.int64)\n",
    "        tensor_labels = torch.tensor([label_24, label_48], dtype=torch.float64)\n",
    "    \n",
    "        return (tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds), hadm_id, tensor_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds), hadm_id, tensor_labels = next(iter(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>demographics_in_visit</th>\n",
       "      <th>lab_tests_in_visit</th>\n",
       "      <th>medications_in_visit</th>\n",
       "      <th>vitals_in_visit</th>\n",
       "      <th>days_in_visit</th>\n",
       "      <th>aki_status_in_visit</th>\n",
       "      <th>previous_diagnoses</th>\n",
       "      <th>days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16679562</td>\n",
       "      <td>20001395</td>\n",
       "      <td>[hispanic latino m 73, hispanic latino m 73, h...</td>\n",
       "      <td>[hematology blood hematocrit  51.2  %; hematol...</td>\n",
       "      <td>[influenza vaccine quadrivalent  0.5  ml ; bis...</td>\n",
       "      <td>[temp    heartrate  80.0  resprate  16.0  o2sa...</td>\n",
       "      <td>[hispanic latino m 73$temp    heartrate  80.0 ...</td>\n",
       "      <td>[0, 0, 0, 1, 1, 0, 0, 1, 0]</td>\n",
       "      <td></td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id                              demographics_in_visit  \\\n",
       "9    16679562  20001395  [hispanic latino m 73, hispanic latino m 73, h...   \n",
       "\n",
       "                                  lab_tests_in_visit  \\\n",
       "9  [hematology blood hematocrit  51.2  %; hematol...   \n",
       "\n",
       "                                medications_in_visit  \\\n",
       "9  [influenza vaccine quadrivalent  0.5  ml ; bis...   \n",
       "\n",
       "                                     vitals_in_visit  \\\n",
       "9  [temp    heartrate  80.0  resprate  16.0  o2sa...   \n",
       "\n",
       "                                       days_in_visit  \\\n",
       "9  [hispanic latino m 73$temp    heartrate  80.0 ...   \n",
       "\n",
       "           aki_status_in_visit previous_diagnoses                         days  \n",
       "9  [0, 0, 0, 1, 1, 0, 0, 1, 0]                     [0, 1, 2, 3, 4, 5, 6, 7, 8]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/home/svetlana.maslenkova/LSTM/dataframes/pid_train_df_finetuning_6days_aki.pkl', 'rb') as f:\n",
    "    pid_train_df = pickle.load(f)\n",
    "\n",
    "with open('/home/svetlana.maslenkova/LSTM/dataframes/pid_test_df_finetuning_6days_aki.pkl', 'rb') as f:\n",
    "    pid_test_df = pickle.load(f)\n",
    "\n",
    "pid_train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "frac=1\n",
    "train_dataset = MyDataset(pid_train_df.sample(frac=frac), tokenizer=tokenizer, max_length=max_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = MyDataset(pid_test_df.sample(frac=frac), tokenizer=tokenizer, max_length=max_length)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1755"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds), hadm_id, tensor_labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : (27380510, array([1., 1.]))\n",
      "1 : (22132103, array([0., 0.]))\n",
      "2 : (28381869, array([0., 0.]))\n",
      "3 : (20853656, array([1., 1.]))\n",
      "4 : (23652398, array([0., 0.]))\n",
      "5 : (27668515, array([0., 0.]))\n",
      "6 : (22928509, array([1., 1.]))\n",
      "7 : (25748555, array([0., 0.]))\n",
      "8 : (22169742, array([0., 0.]))\n",
      "9 : (25569065, array([1., 1.]))\n",
      "10 : (27512939, array([0., 0.]))\n",
      "11 : (21164517, array([0., 0.]))\n",
      "12 : (24279583, array([1., 1.]))\n",
      "13 : (24635431, array([1., 1.]))\n",
      "14 : (27483857, array([0., 0.]))\n",
      "15 : (29877900, array([1., 1.]))\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(zip(hadm_id.cpu().detach().numpy(), tensor_labels.cpu().detach().numpy())):\n",
    "    print(i,':', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>demographics_in_visit</th>\n",
       "      <th>lab_tests_in_visit</th>\n",
       "      <th>medications_in_visit</th>\n",
       "      <th>vitals_in_visit</th>\n",
       "      <th>days_in_visit</th>\n",
       "      <th>aki_status_in_visit</th>\n",
       "      <th>previous_diagnoses</th>\n",
       "      <th>days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10995547</td>\n",
       "      <td>20003740</td>\n",
       "      <td>[white m 69, white m 69, white m 69, white m 6...</td>\n",
       "      <td>[hematology blood hematocrit  30.9  %; hematol...</td>\n",
       "      <td>[lansoprazole oral disintegrating tab  30  mg ...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[white m 69$nan$hematology blood hematocrit  3...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>D51919 D2639 D30000 DV4986 DE915 DV1582 D9351 ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject_id   hadm_id                              demographics_in_visit  \\\n",
       "28    10995547  20003740  [white m 69, white m 69, white m 69, white m 6...   \n",
       "\n",
       "                                   lab_tests_in_visit  \\\n",
       "28  [hematology blood hematocrit  30.9  %; hematol...   \n",
       "\n",
       "                                 medications_in_visit  \\\n",
       "28  [lansoprazole oral disintegrating tab  30  mg ...   \n",
       "\n",
       "                                      vitals_in_visit  \\\n",
       "28  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "\n",
       "                                        days_in_visit  \\\n",
       "28  [white m 69$nan$hematology blood hematocrit  3...   \n",
       "\n",
       "                              aki_status_in_visit  \\\n",
       "28  [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                   previous_diagnoses  \\\n",
       "28  D51919 D2639 D30000 DV4986 DE915 DV1582 D9351 ...   \n",
       "\n",
       "                                                 days  \n",
       "28  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_train_df[pid_train_df.hadm_id==20003740]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_dataset = MyDataset(pid_train_df[pid_train_df.hadm_id==20003740], tokenizer=tokenizer, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels [0, 0, 0, 0, 1]\n",
      "label_24 0\n",
      "label48 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((tensor([  0, 241,  51, 549,   1,   2,   2]),\n",
       "  tensor([   0, 2636, 1793,  914, 1714, 5619,  745, 9614, 3656, 3040, 5520, 1346,\n",
       "          2301, 6817, 8868, 3321, 5520,  745, 2207, 1337, 8868, 2636, 2435,    1,\n",
       "             2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "             2]),\n",
       "  tensor([[0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "           2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "          [0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "           2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "          [0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "           2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "          [0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "           2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "          [0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "           2, 2, 2, 2, 2, 2, 2, 2, 2]]),\n",
       "  tensor([[  0, 104,  79,  ...,   2,   2,   2],\n",
       "          [  0, 104,  79,  ...,   2,   2,   2],\n",
       "          [  0, 104,  79,  ...,   2,   2,   2],\n",
       "          [  0, 104,  79,  ...,   2,   2,   2],\n",
       "          [  0, 104,  79,  ...,   2,   2,   2]]),\n",
       "  tensor([[   0, 1343,  514,  ...,    2,    2,    2],\n",
       "          [   0, 1343,  514,  ...,    2,    2,    2],\n",
       "          [   0, 1343,  514,  ...,    2,    2,    2],\n",
       "          [   0, 1343,  514,  ...,    2,    2,    2],\n",
       "          [   0, 1343,  514,  ...,    2,    2,    2]])),\n",
       " 20003740,\n",
       " tensor([0., 1.], dtype=torch.float64))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, precision_recall_curve, auc, roc_auc_score, accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "def evaluate(model, test_loader, device, threshold=None, log_res=True):\n",
    "    print('Evaluation..')\n",
    "    def find_nearest(array, value):\n",
    "        array = np.asarray(array)\n",
    "        idx = (np.abs(array - value)).argmin()\n",
    "        return idx\n",
    "\n",
    "    device = 'cpu'\n",
    "    model = model.to(device)\n",
    "    stacked_labels = torch.tensor([]).to(device)\n",
    "    stacked_probs = torch.tensor([]).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    step = 1\n",
    "    with torch.no_grad():\n",
    "        for (tensor_demo, tensor_diags, tensor_vitals, tensor_labs, tensor_meds), hadm_id, tensor_labels in test_loader:\n",
    "            print(f'Step {step}/{len(test_loader)}' )\n",
    "            labels = tensor_labels.to(device)\n",
    "            tensor_demo = tensor_demo.to(device)\n",
    "            tensor_diags = tensor_diags.to(device)\n",
    "            tensor_vitals = tensor_vitals.to(device)\n",
    "            tensor_labs = tensor_labs.to(device)\n",
    "            tensor_meds = tensor_meds.to(device)\n",
    "\n",
    "            probs = model(tensor_demo.to(device), tensor_diags.to(device), tensor_meds.to(device), \\\n",
    "                                tensor_vitals.to(device), tensor_labs.to(device))\n",
    "            probs = nn.Sigmoid()(probs)\n",
    "            # output = (probs > threshold).int()\n",
    "\n",
    "            # stacking labels and predictions\n",
    "            stacked_labels = torch.cat([stacked_labels, labels], dim=0, )\n",
    "            # stacked_preds = torch.cat([stacked_preds, output], dim=0, )\n",
    "            stacked_probs = torch.cat([stacked_probs, probs], dim=0, )\n",
    "            step += 1\n",
    "            \n",
    "    # transfer to device\n",
    "    stacked_labels = stacked_labels.cpu().detach().numpy()\n",
    "    stacked_probs = stacked_probs.cpu().detach().numpy()\n",
    "\n",
    "    if threshold==None:\n",
    "        for w in range(stacked_labels.ndim):\n",
    "            pred_window = (w+1)*24\n",
    "\n",
    "            precision, recall, thresholds = precision_recall_curve(stacked_labels.T[w], stacked_probs.T[w])\n",
    "            precision, recall, thresholds = np.round(precision, 2), np.round(recall,2), np.round(thresholds,2)\n",
    "            \n",
    "            # convert to f score\n",
    "            fscore = np.round((2 * precision * recall) / (precision + recall), 2)\n",
    "\n",
    "            # locate the index of the largest f score\n",
    "            ix = np.argmax(np.nan_to_num(fscore))\n",
    "            threshold = np.round(thresholds[ix], 2)\n",
    "            print('--------------- ', str(pred_window)+'h', '--------------- ')\n",
    "            print('Best Threshold=%.2f, F-Score=%.2f' % (threshold, fscore[ix]))\n",
    "\n",
    "            stacked_preds = (stacked_probs.T[w] > threshold).astype(int)\n",
    "            y_true = stacked_labels.T[0]\n",
    "            y_pred = stacked_preds\n",
    "\n",
    "            accuracy = np.round(accuracy_score(y_true, y_pred), 2)\n",
    "            print(f'Accuracy: {accuracy}')\n",
    "\n",
    "            f1_score_ = np.round(f1_score(y_true, y_pred, pos_label=1, average='binary', zero_division=0), 2)\n",
    "            print(f'F1: ', f1_score_)\n",
    "\n",
    "            recall_score_ = np.round(recall_score(y_true, y_pred, pos_label=1, average='binary', zero_division=0), 2)\n",
    "            print(f'Sensitivity: ', recall_score_)\n",
    "\n",
    "            precision_score_ = np.round(precision_score(y_true, y_pred, pos_label=1, average='binary', zero_division=0), 2)\n",
    "            print(f'Precision: ', precision_score_)\n",
    "\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "            specificity =  np.round(tn / (tn + fp), 2)\n",
    "            print(f'Specificity: ', specificity)\n",
    "\n",
    "            pr_auc = np.round(auc(recall, precision), 2) \n",
    "            print(f'PR AUC: ', pr_auc)\n",
    "\n",
    "            roc_auc = np.round(roc_auc_score(y_true, y_pred), 2)\n",
    "            print(f'ROC AUC: ', roc_auc)\n",
    "\n",
    "            precision_list = [0.2, 0.25, 0.33, 0.4, 0.5, 0.6, 0.75]\n",
    "            for p in precision_list:\n",
    "                idx = find_nearest(precision, p)\n",
    "                sensitivity = recall[idx]\n",
    "                print(f'Precision {np.round(precision[idx]*100, 1)}% , Sensitivity {sensitivity} ')\n",
    "\n",
    "            print(f'Confusion matrix:\\n', confusion_matrix(y_true, y_pred))\n",
    "\n",
    "\n",
    "            if log_res:\n",
    "                wandb.log({'test_accuracy'+str(pred_window) :accuracy, 'test_f1_score'+str(pred_window):f1_score_, \\\n",
    "                            'test_recall_score'+str(pred_window):recall_score_, 'test_precision_score'+str(pred_window):precision_score_, \\\n",
    "                                'test_specificity'+str(pred_window):specificity})\n",
    "\n",
    "            # get classification metrics for all samples in the test set\n",
    "            classification_report_res = classification_report(y_true, y_pred, zero_division=0, output_dict=True)\n",
    "            print(classification_report(y_true, y_pred, zero_division=0, output_dict=False))\n",
    "\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = EHR_PRETRAINING(max_length=max_length, vocab_size=vocab_size, device=device, pred_window=2, observing_window=3,  H=128, embedding_size=200, drop=0.6)\n",
    "ft_model = EHR_FINETUNING(pretrained_model, max_length, vocab_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = ft_model(tensor_demo.to(device), tensor_diags.to(device), tensor_meds.to(device), \\\n",
    "                                tensor_vitals.to(device), tensor_labs.to(device))\n",
    "probs = nn.Sigmoid()(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_labels = tensor_labels.cpu().detach().numpy()\n",
    "stacked_probs = probs.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 24h Threshold=0.47, F-Score=0.53\n",
      "Best 48h Threshold=0.48, F-Score=0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svetlana.maslenkova/conda_envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "precision24, recall24, thresholds24 = precision_recall_curve(stacked_labels.T[0], stacked_probs.T[0])\n",
    "precision48, recall48, thresholds48 = precision_recall_curve(stacked_labels.T[1], stacked_probs.T[1])\n",
    "\n",
    "# convert to f score\n",
    "fscore24 = np.round((2 * precision24 * recall24) / (precision24 + recall24), 2)\n",
    "fscore48 = np.round((2 * precision48 * recall48) / (precision48 + recall48), 2)\n",
    "\n",
    "# locate the index of the largest f score\n",
    "ix24 = np.argmax(np.nan_to_num(fscore24))\n",
    "threshold24 = np.round(thresholds24[ix24], 2)\n",
    "print('Best 24h Threshold=%.2f, F-Score=%.2f' % (threshold24, fscore24[ix24]))\n",
    "\n",
    "ix48 = np.argmax(np.nan_to_num(fscore48))\n",
    "threshold48 = np.round(thresholds48[ix48], 2)\n",
    "print('Best 48h Threshold=%.2f, F-Score=%.2f' % (threshold48, fscore48[ix48]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation..\n",
      "Step 1/4\n",
      "Step 2/4\n",
      "Step 3/4\n",
      "Step 4/4\n",
      "---------------  24h --------------- \n",
      "Best Threshold=0.51, F-Score=0.54\n",
      "Accuracy: 0.38\n",
      "F1:  0.54\n",
      "Sensitivity:  0.98\n",
      "Precision:  0.37\n",
      "Specificity:  0.03\n",
      "PR AUC:  0.3658\n",
      "ROC AUC:  0.5012357305856555\n",
      "Precision 27.0% , Sensitivity 0.03 \n",
      "Precision 27.0% , Sensitivity 0.03 \n",
      "Precision 33.0% , Sensitivity 0.09 \n",
      "Precision 40.0% , Sensitivity 0.03 \n",
      "Precision 50.0% , Sensitivity 0.01 \n",
      "Precision 50.0% , Sensitivity 0.01 \n",
      "Precision 50.0% , Sensitivity 0.01 \n",
      "Confusion matrix:\n",
      " [[  30 1076]\n",
      " [  16  633]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.03      0.05      1106\n",
      "         1.0       0.37      0.98      0.54       649\n",
      "\n",
      "    accuracy                           0.38      1755\n",
      "   macro avg       0.51      0.50      0.29      1755\n",
      "weighted avg       0.55      0.38      0.23      1755\n",
      "\n",
      "---------------  48h --------------- \n",
      "Best Threshold=0.51, F-Score=0.54\n",
      "Accuracy: 0.37\n",
      "F1:  0.54\n",
      "Sensitivity:  1.0\n",
      "Precision:  0.37\n",
      "Specificity:  0.0\n",
      "PR AUC:  0.382\n",
      "ROC AUC:  0.4981408314920437\n",
      "Precision 20.0% , Sensitivity 0.0 \n",
      "Precision 29.0% , Sensitivity 0.0 \n",
      "Precision 33.0% , Sensitivity 0.01 \n",
      "Precision 40.0% , Sensitivity 0.38 \n",
      "Precision 50.0% , Sensitivity 0.03 \n",
      "Precision 54.0% , Sensitivity 0.02 \n",
      "Precision 54.0% , Sensitivity 0.02 \n",
      "Confusion matrix:\n",
      " [[   1 1105]\n",
      " [   3  646]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.25      0.00      0.00      1106\n",
      "         1.0       0.37      1.00      0.54       649\n",
      "\n",
      "    accuracy                           0.37      1755\n",
      "   macro avg       0.31      0.50      0.27      1755\n",
      "weighted avg       0.29      0.37      0.20      1755\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svetlana.maslenkova/conda_envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:50: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "evaluate(ft_model, test_loader, device, threshold=None, log_res=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f41d6bb73514239eeae9d84db1aabfbecf43feae35a358a827a9ce792198f6fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
